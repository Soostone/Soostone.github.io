{"0": {
    "doc": "Getting Started",
    "title": "Getting Started",
    "content": "# Getting Started ## Installation The **Napkin** app is available in 2 forms: a [native installer](#native-installer) and a [dockerized version](#docker-version). ### Native installer The Napkin native installer is a self extracting and self contained SHELL archive that does not require a docker or nix environment. It is located in this [repository](https://soostone-napkin-public.s3.us-east-1.amazonaws.com/index.html){:target=\"_blank\" rel=\"noopener\"} and has distributions for the Mac OS Big Sur and Linux operating systems. To use the native installer: 1) Find the distribution corresponding to your operating system 2) Copy the installation snippet 3) Execute it locally As an example, this is the command to install Napkin on MacOS Big Sur: ```sh curl -s https://soostone-napkin-public.s3.us-east-1.amazonaws.com/x86_64-MacOS-20.5.0/last-build/napkin-native-installer.sh \\ | bash -s -- ``` {: .copiable } The installation bundle is large as it is written in Haskell, a compiled coding language and contains all compiled dependencies. If you need access to the compiler (Glasgow Haskell Compiler `ghci`), simply supply the file derivations for the app you are interested in: ```sh curl -s https://soostone-napkin-public.s3.us-east-1.amazonaws.com/x86_64-MacOS-20.5.0/last-build/napkin-native-installer.sh \\ | bash -s -- -e '*-ghc-*' ``` {: .copiable } The ghc derivation contains version **ghci-8.10.4**. Paths to Napkin are imported through the `$PATH` variable. When installing Napkin, remember to use a new shell session so you get the updated `$PATH` variable. You can also use the `exec $SHELL` command. #### Uninstallation The Napkin native installer contains an uninstaller as well. Type `uninstall-` and you can use the 'tab' key to auto-complete the uninstallation script. ```sh $ uninstall- $ uninstall-b82gj7f0igpw23sapafxifsrr0f52771-napkin-0.3.8.sh ``` ### Docker version The native installer is not available on [Windows](https://en.wikipedia.org/wiki/Microsoft_Windows) yet. There are tutorials on the Internet for setting up [Nix](https://nixos.wiki/wiki/Nix) with [WSL](https://en.wikipedia.org/wiki/Windows_Subsystem_for_Linux) on Windows. ## Key Features Napkin has a robust set of capabilities, with more to come! See our roadmap for more information and to let us know which feature(s) you would like to see added to Napkin the most! One key feature to highlight here is Napkin's capability to detect unused [CTE](https://en.wikipedia.org/wiki/Hierarchical_and_recursive_queries_in_SQL#Common_table_expression) columns. While some table columns are introduced in the SQL pipeline for debugging and development purposes, these can get stale or be forgotten about over time. And more importantly, unused fields are still contributing to pipeline cost and time-to-run. This feature is available via Napkin's Command Line Interface (CLI). For reference, to see the list of available Napkin commands, type `--help`. ```sh $ napkin --help ``` {: .copiable } ``` Usage: napkin [-v|--verbose] COMMAND Cli tool to work with Napkin ... find-unused-cte-columns Parse SQL file and find unused CTE columns ... ``` Detailed help for `find-unused-cte-columns` command: ```sh napkin find-unused-cte-columns --help ``` {: .copiable } ``` Usage: napkin find-unused-cte-columns (-b|--backend ARG) (-f|--sql-file ARG) Parse SQL file and find unused CTE columns Available options: -b,--backend ARG choose kind of SQL - BigQueryBackend or PostgresqlBackend -f,--sql-file ARG path to sql file ``` Let's work through an example. First, let's check the following query for unused columns in an intermediate CTE table within a .sql file: {% include codeCaption.html label=\"query.sql\" %} ```sql WITH CTE AS (SELECT f, g FROM DbTable) SELECT f FROM CTE ``` {: .copiable } {% include codeCaption.html label=\"shell\" %} ```sh napkin find-unused-cte-columns -b PostgresqlBackend -f query.sql {\"CTE\":[\"g\"]} ``` {: .copiable } To deal with a set of SQL files - generate a spec file, sort of a project file: {% include codeCaption.html label=\"shell\" %} ```sh cp query.sql query2.sql mkdir sql mv query.sql query2.sql sql napkin generate-spec -d sql # Generated spec file at: specs/spec.yaml cat specs/spec.yaml | grep source # source: query.sql source: query2.sql mv sql specs # Tiny workaround - might not be needed in the future napkin optimize ``` {: .copiable } ``` Table: [query] query, CTE \"CTE\" has unused columns [\"g\"] Table: [query2] query2, CTE \"CTE\" has unused columns [\"g\"] ``` ## [Continue to the Tutorial section...](/tutorial) ",
    "url": "https://soostone.github.io/getting-started/",
    "relUrl": "/getting-started/"
  },"1": {
    "doc": "Google Site Search",
    "title": "Google Site Search",
    "content": "# Google Search {% include googleSearch.html %} ",
    "url": "https://soostone.github.io/google-search/",
    "relUrl": "/google-search/"
  },"2": {
    "doc": "Haddock",
    "title": "Haddock",
    "content": "# Haddock ## Docs for versions * [Version 0.3.8]({{site.haddock_url}}/app-version/index.html) ## Docs for branches * [last build]({{site.haddock_url}}/last-build/index.html) * [master]({{site.haddock_url}}/branch/master/index.html) * [Full list of branches with haddoc]({{site.haddock_url}}/branch/index.html) ",
    "url": "https://soostone.github.io/haddock/",
    "relUrl": "/haddock/"
  },"3": {
    "doc": "About",
    "title": "About",
    "content": "# What is Napkin Napkin is a command line application that executes data pipelines of all sizes, backed by a feature-rich Haskell library offering programmatic freedom. It's lightweight, offers a quick start for new projects and yet scales to massive data pipelines with powerful meta-programming possibilities. Napkin has a broad vision in making life easier for data scientists and engineers, encapsulating a large portion of the data engineering landscape. It therefore bundles several key features together: 1. A consumer-grade Command Line Interface (CLI) that acts as the single point of entry for all typical workflows of data engineering and pipeline curation. The `napkin` app can refresh entire data pipelines, re-create individual tables, validate/typecheck pipelines in seconds, export dependency graphs and more. 1. A multi-backend (w.g. BigQuery and Redshift) database runtime environment that provides for all key capabilities in executing a modern data pipeline, including interacting the database (see what's there, query tables, create/recreate/update tables, etc.), performing runtime unit-tests/assertions, logging, timing and interacting with the outside world. 1. A built-in DAG orchestrator that can automatically detect all the debendency relationships in a data pipeline (e.g. 30+ tables) and perform the pipeline updates in the correct order. Data pipelines are called \"Spec\"s in napkin and ship with all batteries included: Ability to rewrite table destinations into different schemas/datasets for different environments (e.g. devel vs. prod), mass-prefixing/renaming tables, setting different \"Refresh Strategies\" for each table (e.g. update daily vs. only update when missing), a wide range of data unit-tests (e.g. table must be unique by columns X+Y) that are automatically performed each time the table is updated. 1. For the power user, a SQL wrapper DSL in Haskell that stays as close as possible to SQL, without any intermediarey object or relational mappings. This DSL looks almost like regular SQL, but allows sophisticated programmatic manipulation and composition of SQL queries and statements. Napkin can parse regular SQL into this internal DSL, perform any desired manipulations and render it back out as regular SQL. 1. A sophisticated SQL meta-programming environment that accelerates modern data engineering efforts. Napkin users can interweave several options for crafting SQL as they see fit, even in the same file. These options include: - Writing plain SQL files without any low-grade templating noise. Napkin will still auto-detect all depedencies and make the pipeline \"just work\". - Using lifthweight variable substitutions in `.sql` files via Mustache templates. - Using sophisticated `#{sexp} ... #{/sexp}` splices directly in `.sql` files to write Haskell code that dynamically generates SQL fragments on the fly. - Expressing entire queries directly using napkin's Haskell DSL, often used for dynamic generation of SQL code based on complex logic. For example, prediction trees can be rendered into SQL this way, sometimes generated 100K LOC SQL files from a single model. # Napkin's Philosophy Napkin was created to capitalize on an opportunity we noticed back in 2015 to (massively) accelerate our team's data engineering capabilities and yet make the resulting codebases way more sustainable/maintainable. At the time, we were drowning in the complexity of custom Hadoop MapReduce programs, Spark programs and repositories of ad-hoc SQL scripts targeted on Redshift/Hive/etc at the time. We created napkin because we sorely needed something more practical and reliable for our own work. Over time, the opportunities we saw got crystallized into a set of philosophies we can articulate about what napkin is trying to achieve and whether it may be the huge catalyst for your team that it has been for us: ## Base as much data compute as possible on SQL Despite its age and missed opportunities, SQL code is declarative, functional and highly expressive. It's easy to construct even for non-engineer data scientists/analysts and tends to offer good \"equational reasoning\". It's constraied just the right amount that business logic does not go \"off the hook\" like it can in typical programming languages like Python, R, Scala, etc. Once written and tested, SQL tends to produce reliable results. Over the years, we have found almost all data engineering efforts outside of SQL to be error-prone, hard to grow and expensive (e.g. needs data engineers) to maintain over time. If you can imagine how a table should be structured and express that table as a query in SQL, you can use napkin to engineer a pipeline. ## Do as much compute as possible on modern analytics DBs like BigQuery/Redshift/Snowflake Napkin aims to be a data engineering superpower even for very small teams. This is accomplished in large part by leaning on the amazing compute capabilities of modern analytics databases like BigQuery. Napkin's creation goes back to our realization that if we could express even a very complex computation in SQL on these databases, no matter how convoluted, they would get the work done in astonishingly litte time for minimal cost. In our work, we have produced numerous 200,000+ LOC SQL queries using napkin's metaprogramming capabilities that run within minutes on databases like Amazon Redshift and Google's BigQuery. Fun fact: BigQuery has a ~1M character limit on queries, which we sometimes bypass by breaking complex queries into parts and joining them up / unioning them later. Even this transformation can be done automatically for you by napkin in certain cases! ## Abstract and reuse complex transformations where possible ## Data pipelines should be declarative and managed on Git ## Data pipelines should be regenerative ## Data pipeline dev should be lightweight on bare laptops ## Doctrine of extreme convenience With napkin, we aim to make various data engineering and data science workflows so easy to perform that practitioners change their behavior to lean on them more frequently. We believe that speed and convenience without sacrificing correctness and reliability makes a huge difference in sustaining data ecosystem effectiveness. # Napkin's Benefits Here's our best description of benefits you can expect after you've gotten a hang of napkin: 1. You'll be able to see and manage your entire data pipeline in a simple codebase, in declarative fashion and in source control - just like any modern software project. 1. You'll always be able to \"blow away and fully refresh\" your entire pipeline from raw data at the push of a button - recovering from mistakes will be a breeze. 1. Your data pipeline will entirely rely on the power of your backend database, whatever it may be. The likes of BigQuery for large datasets or Postgres (or even Sqlite) when you can get away with it on small data. You won't rely on error prone Python pandas code, your own custom data processing application and similar constructs that are hard to grow/maintain and ensure correctness over time. 1. Your data will have actual unit tests that will confirm correctness with each update. (Example: Making sure you don't double count sales) 1. You'll benefit from tens of combinators we ship with napkin, such as incrementally updating large tables, column-to-row transformations, union-in same-structured tables into one, etc. As we improve napkin, you'll get all that for free. 1. You'll be able to implement your own clever SQL meta-programming to express logic that'd be too tedius to do in plain SQL. Yet the result will still have all the benefits of declarative SQL running on modern analytics databases, instead of your custom Python/R/Scala scripting machine. You'll be able to create your own mini programs that produce 10-table \"purchasing funnel\" computations that connect just the right way based on configuration parameters supplied. # Napkin's Future Napkin is utilized heavily in commercial projects both at Soostone and at our clients. We improve napkin all the time and have a long backlog of major features we will realize in the future. We would like to be transparent with our roadmap and are looking for ways to best communicate our plans. We're currently maintaining a Trello board with our roadmap where we would love to hear your reactions and feedback. You can access our roadmap board at [Napkin Roadmap](https://trello.com/b/rIbzqkFb/napkin-roadmap) # Next Steps [Continue with **Getting Started**](/getting-started) ",
    "url": "https://soostone.github.io/",
    "relUrl": "/"
  },"4": {
    "doc": "Tips and Tricks",
    "title": "Tips and Tricks",
    "content": "1. TOC {:toc} # Introduction The purpose of this page is to give capture a selection of napkin usage examples that occur very frequently in the day-to-day development and maintenance of a data pipeline. # napkin run ## Running only a single table in a spec It's very common when iteratively working on a given table to update it repeatedly and in isolation. Disable all tables, force-update any table with word \"uplift\" in it: ``` napkin run -s specs/myspec1.yaml -D -f '.*(uplift).*' ``` Notice the pattern is a proper Regular Expression. # napkin validate ## Continuosly validating codebase on every change Keeping a validating screen open is invaluable in rapid iteration. Napkin will notice every change on every file that's touched by a given spec and automatically re-validate the entire project. This will catch obvious structural errors in SQL files, mistakes in templates and any compilation errors in custom Haskell code. ``` napkin validate -s specs/myspec1.yaml --live ``` ",
    "url": "https://soostone.github.io/tips-and-tricks/",
    "relUrl": "/tips-and-tricks/"
  },"5": {
    "doc": "Tutorial",
    "title": "Tutorial",
    "content": "1. TOC {:toc} # Tutorial ## Interacting with Napkin - The Spec Napkin is a data pipeline automation tool, designed to execute a series of .sql queries where the resultant data is utilized to create downstream tables. The interface to specify what .sql files to execute is called a Napkin **Spec**. A Spec is defined via a configuration language called [YAML](https://en.wikipedia.org/wiki/YAML). This follows a common paradigm where source tables are never mutated. Instead, Napkin can be repeatably run to create a series of dependent tables as new data comes in to one or many source tables. The sql files / tables in the Napkin Spec comprise an implicit [DAG](https://en.wikipedia.org/wiki/Directed_acyclic_graph), where edges are references to fields from other tables. A common use case is as follows - Source tables are created or updated manually via a data extract or automatically by a piece of software - .SQL files are written to mutate the data into the desired shape that is fit for use in new, downstream tables (never modifying the source data) - A Napkin Spec is created to execute these .SQL files automatically, repeatedly and in the correct dependency order - As new data comes in, the Napkin Spec can be manually or automatically run to recreate the dependent tables with the new data for ongoing use ## Sales DB demo ### Starting out - Creating your Napkin Spec file While you can write out a Napkin Spec file in .yaml yourself, Napkin also has built-in functionality to set this up for you and can help automatically generate the Spec file. All Napkin needs to do this is at least one SQL file. Let's consider a sales DB with the following tables: ### Schema and Input Data First, we'll need to create a source dataset. As a reminder, our SQL pipeline will read from these tables to create the downstream tables but will never mutate the data directly in the source tables. {% include codeCaption.html label=\"input-schema.sql\" %} ```sql CREATE TABLE product ( id INT PRIMARY KEY, price INT NOT NULL, name TEXT NOT NULL); CREATE TABLE sale( id INT PRIMARY KEY, quantity INT NOT NULL, product_id INT NOT NULL); ``` {: .copiable } {% include codeCaption.html label=\"shell\" %} ```sh $ psql -f input-schema.sql salesDb ``` Then we'll put some data into them. {% include codeCaption.html label=\"input-data.sql\" %} ```sql INSERT INTO product VALUES (1, 2, 'chocolate bar'), (2, 3, 'coke'), (3, 50, 'kale'); INSERT INTO sale VALUES (1, 1000, 1), (2, 2, 3), (3, 300, 2), (4, 10, 3); ``` {: .copiable } {% include codeCaption.html label=\"shell\" %} ```sh psql -f input-data.sql salesDb ``` {: .copiable } ### Setting up The Pipeline Place the .sql files that will create your dependent tables in the /sql sub-folder so the Napkin Spec can see and execute them. Here are a couple examples of queries that can run against the source tables we just created. {% include codeCaption.html label=\"sql/best-seller.sql\" %} ```sql SELECT product_id, sum(quantity) FROM sale GROUP BY product_id ORDER BY sum(quantity) DESC; ``` {: .copiable } {% include codeCaption.html label=\"sql/best-revenue.sql\" %} ```sql SELECT product_id, sum(p.price * s.quantity) FROM sale s INNER JOIN product p ON (s.product_id = p.id) GROUP BY product_id ORDER BY 2 DESC; ``` {: .copiable } Next, let's have Napkin automatically generate the Spec file to execute the queries above. Note that Napkin can figure out the dependency relationship of the SQL files and execute them in the correct order. {% include codeCaption.html label=\"shell\" %} ```sql napkin generate-spec -d sql -o spec.yaml mkdir specs mv sql spec.yaml specs ``` {: .copiable } This is what the head of the spec.yaml might look like: {% include codeCaption.html label=\"spec.yaml\" %} ```yaml sql_folder: sql db_url: postgres:/// haskell_packages: [] backend: Postgres haskell: null tables: ``` For `db_url`, this is an example for configuring Napkin to run against a local PostgreSQL instance: ```yaml db_url: postgresql:/user:123@127.0.0.1/salesDb ``` ``` To avoid keepking passwords in a common configuration file, you can provide the database password through Napkin's `--connectionURL` command line option. See `--help` for more details. ``` {: .info} Once you have the Spec, your source tables and the .sql files to execute, you're all set! The next step is to tell Napkin to execute the pipieline you've created. This is done through the 'run' command as shown below. {% include codeCaption.html label=\"shell\" %} ```sh $ napkin run --spec-file specs/spec.yaml ``` {: .copiable } ``` NOTICE: table \"best-revenue\" does not exist, skipping NOTICE: table \"best-seller\" does not exist, skipping [2021-08-20 17:25:14] Table \"best-revenue\" ran in 0.03s: 3 rows affected [2021-08-20 17:25:14] Table \"best-seller\" ran in 0.04s: 3 rows affected [2021-08-20 17:25:14] Run complete. Total cost: 6 rows affected ``` Once the pipeline has run, you can check the database for the new tables (`best-seller`and `best-revenue`): {% include codeCaption.html label=\"psql salesDb\" %} ```sql SELECT * FROM \"best-seller\"; ``` {: .copiable } ``` product_id | sum ------------+------ 1 | 1000 2 | 300 3 | 12 ``` Now you're all set! You have a working SQL pipeline and as new data comes in to your source tables you can repeatably give napkin the 'run' command to execute the pipeline and recreate the target tables. As an example, let's modify our input data here and rerun the pipeline. {% include codeCaption.html label=\"psql salesDb\" %} ```sql INSERT INTO sale VALUES (5, 999, 3); ``` {: .copiable } {% include codeCaption.html label=\"shell\" %} ```sh napkin run --spec-file specs/spec.yaml ``` {: .copiable } {% include codeCaption.html label=\"psql salesDb\" %} ```sql SELECT * FROM \"best-seller\"; ``` {: .copiable } ``` product_id | sum ------------+------ 3 | 1011 1 | 1000 2 | 300 ``` As expected, the updated target tables reflect the change in source data and the `best-seller` table is updated with the new row with the value 'kale' in it on top. ## Using Templates and Variable Interpolation The next step in mastering Napkin is to utilize templates to parameterize your queries. This is an extremely important feature because it can be used for everything from handling different table or variable names in development vs production datasets or simply reusing the same query for multiple purposes. Template variables can be set in a Spec file or can be overridden globally with command line options. Template variables hold text to be used in substitution for the variable name in a template query. The final query should always be a valid SQL expression. Let's work through an example. There are 2 versions of STDDEV functions in Postgres. Let's compare them by computing the results in dedicated tables, but reusing a single parameterized query. {% include codeCaption.html label=\"sql/stdev.mtpl\" %} {% raw %} ```sql SELECT p.name, {{stddev}}(s.{{column_name}}) FROM {{table_name}} s INNER JOIN product p ON (p.id = s.product_id) GROUP BY p.name; ``` {% endraw %} {: .copiable } Append the following config to spec.yaml: {% include codeCaption.html label=\"spec.yaml\" %} ```yaml - post_hooks: [] pre_hooks: [] update_strategy: - type: always create_action: strategy: default hidden_deps: [] deps: [] source: stddev.sql type: sql_file vars: {\"column_name\": \"quantity\", \"table_name\": \"sale\", \"stddev\": \"stddev_pop\"} target_type: table target: sale_stddev_pop_quantity tags: [] - post_hooks: [] pre_hooks: [] update_strategy: - type: always create_action: strategy: default hidden_deps: [] deps: [] source: stddev.sql type: sql_file vars: {\"column_name\": \"quantity\", \"table_name\": \"sale\", \"stddev\": \"stddev_samp\"} target_type: table target: sale_stddev_sam_quantity tags: [] ``` {: .copiable } {% include codeCaption.html label=\"shell\" %} ```sh napkin run --spec-file specs/spec.yaml ``` {: .copiable } Compare the results: {% include codeCaption.html label=\"psql salesDb\" %} ```sql SELECT s.name, stddev_pop, stddev_samp FROM sale_stddev_pop_quantity p, sale_stddev_sam_quantity s WHERE s.name = p.name; ``` {: .copiable } ``` name | stddev_pop | stddev_samp ---------------+------------------+------------------ coke | 0 | kale | 468.116082469580 | 573.322771220540 chocolate bar | 0 | ``` Here we can see that Napkin was able to run the same sql file but in taking the configuration from the spec.yaml file, actually executed two different valid queries with different results placed into different target tables. ## Chain of queries In the previous example, we ran our join query manually. Let's update our Spec so that it's automatically executed. Append the following config to your spec.yaml: {% include codeCaption.html label=\"spec.yaml\" %} ```yaml - post_hooks: [] pre_hooks: [] update_strategy: - type: always create_action: strategy: default hidden_deps: [] deps: [] source: compare-sale-stddev-quantity.sql type: sql_file vars: {} target_type: table target: compare_sale_stddev_quantity tags: [] ``` Let's truncate our intermediate tables so we're 100% confident that the resultant dataset in the target tables was created with the pipelien run we're about to do. {% include codeCaption.html label=\"psql salesDb\" %} ```sql truncate sale_stddev_pop_quantity; truncate sale_stddev_sam_quantity; ``` {: .copiable } {% include codeCaption.html label=\"shell\" %} ```sh napkin run --spec-file specs/spec.yaml ``` {: .copiable } {% include codeCaption.html label=\"psql salesDb\" %} ```sql SELECT * FROM compare_sale_stddev_quantity; ``` As we can see, the target table correctly shows the desired result from our parameterized queries: ``` name | stddev_pop | stddev_samp ---------------+------------------+------------------ coke | 0 | kale | 468.116082469580 | 573.322771220540 chocolate bar | 0 | ``` ## SQL macro functions ## Backends Napkin supports following backends: * [Postgres](https://www.postgresql.org/) * [BigQuery](https://cloud.google.com/bigquery) * [Redshift](https://aws.amazon.com/redshift) These are the databases we currently support. Let us know if your database of choice is not here and maybe we can add it to a future release! ## BigQuery OAuth authentication. ## Embedded Haskell Lot's of programs have embedded scripting langauge for better flexibility and automating complex business logic, recall - [VBA](https://en.wikipedia.org/wiki/Visual_Basic_for_Applications), [Emacs Lisp](https://en.wikipedia.org/wiki/Emacs_Lisp), [MaxScript](https://en.wikipedia.org/wiki/Autodesk_3ds_Max), etc and Napkin is on the same line with them and Napkin Api is available through [Haskell](https://en.wikipedia.org/wiki/Haskell_(programming_language)). ### Evaluation Haskell through **eval** Napkin eval command can interpret a set of free form Haskell modules - just specify top function you are intereseted in. Module with a lazy string function: ```haskell module Hello where import Prelude (String, (++), Char) pureStr :: String pureStr = \"Hello World!!!\" ``` ```sh $ napkin eval -f pureStr -m Hello -r . -i Pure str Hello World!!! ``` See help for calling IO action. Access to plain Haskell interpeter is a test environment for code to be used in spec for describing tables. ## Misc ### Unused column detection Some table columns are introduced for debugging and development purpose, but as time goes an engineer can stop using them and completely forget, meanwhile such columns contribute into cost of running queries. Let's check following query for unused columns in an intermediate CTE table: {% include codeCaption.html label=\"sql/query.sql\" %} ```sql with CTE as (select f, g from DbTable) select f from CTE ``` {: .copiable } {% include codeCaption.html label=\"shell\" %} ```sh napkin optimize ``` {: .copiable } ``` Table: [query] query, CTE \"CTE\" has unused columns [\"g\"] ``` ",
    "url": "https://soostone.github.io/tutorial/",
    "relUrl": "/tutorial/"
  },"6": {
    "doc": "User Manual",
    "title": "User Manual",
    "content": "# User Manual ",
    "url": "https://soostone.github.io/user-manual/",
    "relUrl": "/user-manual/"
  }
}
