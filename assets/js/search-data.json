{"0": {
    "doc": "Cachix versions",
    "title": "Cachix versions",
    "content": ". | 0.5.12 macOS / Linux (currently in development) . | 0.5.11 macOS / Linux (released: 2022-01-20) . | 0.5.10 macOS / Linux (released: 2021-12-29) . | 0.5.9 macOS / Linux (released: 2021-12-14) . | 0.5.8 macOS / Linux (released: 2021-12-08) . | . ",
    "url": "https://docs.napkin.run/cachix/versions/",
    "relUrl": "/cachix/versions/"
  },"1": {
    "doc": "Changelog",
    "title": "Changelog",
    "content": ". ",
    "url": "https://docs.napkin.run/changelog/",
    "relUrl": "/changelog/"
  },"2": {
    "doc": "Changelog",
    "title": "0.5.12 (currently in development)",
    "content": ". | Materialized views are now supported for BigQuery, Postgres, and Redshift. | TimescaleDB: added Haskell DSL combinators for Timescale-specific aggregates in Napkin.Untyped.Ops.Timescale. | TimescaleDB: continuous aggregates are now supported through materialized view options. | YAML Specs: table_options are now part of create_action arguments. | Fix: Mustache interpolation allows to nest section and use variables from outer context. | napkin templates no longer exists, list of all embedded templates are displayed on init --help screen. | Added a possibility to define tables by invoking external commands from spec.yaml file. | . ",
    "url": "https://docs.napkin.run/changelog/#0512-currently-in-development",
    "relUrl": "/changelog/#0512-currently-in-development"
  },"3": {
    "doc": "Changelog",
    "title": "0.5.11 (released 2022-01-20)",
    "content": ". | CLI commands are combined into groups for visual clarity. | Revamped CLI options for partial spec runs. | Napkin will display original table names (in addition to processed table names) in the execution plan summary. | Napkin is now able to conditionally render mustache template sections. | Section is not rendered if condition variable is not defined or ‘false’ or ‘empty list’. | Section is rendered multiple times for ‘non empty list’, binding list element as a variable set. | Section is rendered once for a non empty value, binding it as a variable set. | Napkin produces an error when variable mentioned in section name is not defined. | . | Decoupled SQL dialect from backend: . | SQL dialect can be selected on per-spec or per-table basis using parser_dialect option. | Available parsers are: napkin.bigquery, napkin.postgres, napkin.sqlite, napkin.ansi2011, generic.bigquery, generic.postgres, generic.sqlite, generic.ansi2011, postgres, raw. | When using dialect other than napkin.* Napkin will have limited capabilities (e.g. query optimization will not be available). | Dependency detection and renaming will not affect queries parsed with raw. Napkin will not attempt to parse or modify them. | postgres dialect can be used when some Postgres-specific features are used (e.g. JSON operator such as -&gt;&gt;). | . | Fix: Napkin would not detect dependencies in some subqueries. | Napkin is able to perform strict validations (in run, ‘validate’, dump and optimize commands). The supported validations are: . | Mustache variable, mentioned in section name is not defined. | Part of nested mustache variable path, mentioned in section name in not an object. | Complex (object, array) mustache value rendered directly. | . | Suppress reporting 0 rows affected for SQLite | REPL: Fix: helper macros did not work properly due to regression. | REPL: Spec and Hook programs can return values to the REPL environment for debugging purposes. | Internal: LocalFile Polysemy language got split into LocalFile and Template with umbrella LoadQuery language. | Query statistics is not printed as “unknown” if there are no pieces of information available (typical for Sqlite). | Source location (used in error reporting) is now able to print start of inline query for better context. | Post hooks in YAML will have implicit argument table with target table name. | BigQuery: Add support for STRUCTs. | Fix: renamed target table name was passed to programs instead of raw one. | SQLite version has been updated to 3.36.0 and now supports math and JSON functions. | Napkin now prints embedded Sqlite version as part of napkin version CLI command. | ‘Simple’ logging format now has table name in the context. | Added --use-spec-names to dump command. | Internal change: query transformers (e.g. table renamers) are not baked into programs – they are applied on runtime. | Fix: regression for assert_expression syntax in YAML specs. | . ",
    "url": "https://docs.napkin.run/changelog/#0511-released-2022-01-20",
    "relUrl": "/changelog/#0511-released-2022-01-20"
  },"4": {
    "doc": "Changelog",
    "title": "0.5.10 (released 2021-12-29)",
    "content": ". | &lt;nixpkgs&gt; path in docker now points to the github commit instead of intermediate file. | Is is now possible to specify timeout for google BigQuery in the backend_options. | Napkin will report an error when sql file used with incremental_by_time strategy does not consume cutoff variable. | Fix: SQLite remove extra parents from SQL INSERT INTO SELECT Statement. | Change PostgreSQL parser dialect from ANSI to Postgres. | Fix: Napkin was using incorrect pipeline name for backends (was always “rs”). | Fix: Change CAST operator rendering to be ANSI complainant. | Docker image now uses napkin user by default, it has nicer bash and zsh prompts. | Docker image working directory changed from /project to /home/napkin/project. | Dev-container settings (produced by napkin init) now specify zsh as a default shell. | . | CI builds is DAG now (overall pipeline time reduced). | Meta arguments handling in programs is more consistent: . | Reader MetaArguments is now Input MetaArguments in SpecProgram and HookProgram. | Input SpecMetaArgs is now Input MetaArguments in SpecPreprocessor. | Input MetaArguments helpers are now located in Napkin.Run.Effects.MetaArguments. | . | incrementalByTime now accepts incremental_reset as string or bool (--arg incremental_reset=true will work again). | napkin auth uses now subcommands to show (napkin auth show) and reset (napkin auth reset). | napkin auth show displays output in human-friendly format. | Fix: namespaceAllTables will no longer rename CTEs and break queries. | Fix: renaming tables will no longer affect aliases. | Fix: table aliases were not rendered correctly in JOIN queries (Postgres, Redshift). | Extensions to table_namespace and table_prefix preprocessors: . | added scope parameter that can be either all, managed (default) or unmanaged to control which tables are renamed, | added only and except parameters for fine-grained control on which tables are renamed, | table_namespace has extra on_existing parameter that can be overwrite (default) or keep_original, which allows to keep original namespace if it has been explicitly provided in the spec. | . | Fix: tables now can be moved between schemas (Postgres, Redshift). | Fix: checkTableExists does not assume that default schema is public (Postgres, Redshift). | . ",
    "url": "https://docs.napkin.run/changelog/#0510-released-2021-12-29",
    "relUrl": "/changelog/#0510-released-2021-12-29"
  },"5": {
    "doc": "Changelog",
    "title": "0.5.9 (released 2021-12-14)",
    "content": ". | Napkin now support different log formats though --log-format CLI option. | Added Napkin static binary (does not fully support haskell interpretation) for easier installation. | Renamed live validation option from ‘-l’ (–live) to ‘-i’ (–interactive) to avoid collision with –log-level. | Fix: Interactive validation now doesn’t complain on absent folders. | Added S3 bucket monitoring page. | Consistent log-level setting via command line options: one can use either -v or --log-level (-l). Options can be applied to all commands now. | Error reporting is more consistent. | Improved CLI UI (napkin run -p) look and feel. | Fix: CLI UI did not display query statistics properly when spec execution has been terminated by the user. | Fix: CLI UI did not terminate spec execution. | Napkin will print the information on execution plan (managed tables to be updated, unmanaged tables used as an input, managed tables used as an input, but not scheduled for updated) as well as ETA. | Skipped tables will no longer block dependent tables execution. | Fix: Haskell spec has now access to meta arguments. | Number of concurrent DB operations can be set with backend_options in YAML specs. Use concurrent_queries for Big Query or connection_pool for Postgres and Redshift. The setting defaults to 100. | #253 Support SQLite Builtin functions. | . ",
    "url": "https://docs.napkin.run/changelog/#059-released-2021-12-14",
    "relUrl": "/changelog/#059-released-2021-12-14"
  },"6": {
    "doc": "Changelog",
    "title": "0.5.8 (released 2021-12-08)",
    "content": ". | Improved napkin init. | Docker tags are now consistent. | create_action syntax has changed and it is now consistent for built-in and custom programs. | added sql_query and long_to_wide built-in spec programs. | incremental combinators are now built-in programs. | update_strategy now explicitly defaults to always, empty list will not fall back to always. | it’s possible to call custom programs from yaml without arguments bu providing string (symbol name) instead of object. | deps and hidden_deps are now attribute of table (was part of create_action previously). | . | . ",
    "url": "https://docs.napkin.run/changelog/#058-released-2021-12-08",
    "relUrl": "/changelog/#058-released-2021-12-08"
  },"7": {
    "doc": "Fundamentals",
    "title": "Napkin fundamentals tutorial",
    "content": "This is the first of a series of tutorials to get up and running with Napkin development. We do this through the example of building a simple Napkin data pipeline. The goal is that by the end of this tutorial, you’ll have a good idea of: . | How to start a Napkin project | How to modify, verify, and execute Napkin projects | How to automatically test data | How Napkin helps you build a real-life application quickly | . With that, let’s get started! . | Getting started | Get example dataset | Bootstrapping Napkin project | Setting DB connection | The first queries | Best markets | Discovering dependencies | DRY: Don’t Repeat Yourself | Garbage in, garbage out: validating data | Napkin remembers | Saving our work | Deploying to the production with PostgreSQL | Summary | . ",
    "url": "https://docs.napkin.run/fundamentals/#napkin-fundamentals-tutorial",
    "relUrl": "/fundamentals/#napkin-fundamentals-tutorial"
  },"8": {
    "doc": "Fundamentals",
    "title": "Getting started",
    "content": "There are a few preliminaries for this tutorial: we assume you’re running Linux or macOS. You need to install Napkin. If you are running Linux, you also need to install SQLite for this tutorial. Windows users can use Docker or Devcontainer setup that will bring all necessary tools. With the preliminaries out of the way, we are ready to proceed with bootstrapping our first Napkin data pipeline. In a hurry? Clone a completed project . In addition to the instructions, we provide a completed project as a Git repository. Clone the repo, open it in VSCode and select “Reopen in Container” to get ready to use environment with Sqlite and Postgres databases ready . ",
    "url": "https://docs.napkin.run/fundamentals/#getting-started",
    "relUrl": "/fundamentals/#getting-started"
  },"9": {
    "doc": "Fundamentals",
    "title": "Get example dataset",
    "content": "In this tutorial, we will use Chinook database that is commonly used as an example dataset. Chinook database represents a digital media store database with tables for artists, albums, media tracks, invoices, and customers. For sake of simplicity, we will use SQLite backend, so no additional setup for the database is required. In a production-grade environment, one can use any of the supported backends, such as Postgres or BigQuery. We will demonstrate Napkin’s features with simple queries to give the reader the sense of the benefits of using Napkin for more complex data pipelines. Let’s download the database file from GitHub with the following command. shell . curl -L -o chinook-sql.db https://github.com/lerocha/chinook-database/raw/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite . To verify the Chinook database and SQLite work correctly run: . shell . sqlite3 chinook-sql.db \"SELECT * FROM Artist\" . ",
    "url": "https://docs.napkin.run/fundamentals/#get-example-dataset",
    "relUrl": "/fundamentals/#get-example-dataset"
  },"10": {
    "doc": "Fundamentals",
    "title": "Bootstrapping Napkin project",
    "content": "The most intuitive way of initiating a Napkin project is to use the init CLI command. napkin init creates a new project skeleton with Napkin’s default template. shell . mkdir napkinFundamentalTutorial cd napkinFundamentalTutorial napkin init --project-name chinook-analytics . The corresponding directory structure created by Napkin’s init command for the chinook-analytics project is: . shell . chinook-analytics ├── hie.yaml ├── README.md ├── specs │ └── spec.yaml └── sql └── example.sql . Napkin’s CLI is the point of entry for all typical workflows of data engineering and pipeline curation. napkin --help provides a comprehensive list of available Napkin commands. Commands may take additional parameters. To get help with these parameters, use: napkin command --help. For instance, to get help with the init command, use napkin init --help. Please refer to CLI reference page for further information. The data pipeline we will be working on is specified in specs/spec.yaml file, let’s explore how it looks like: . specs/spec.yaml . # yaml-language-server: $schema=http://napkin-public-storage-bucket.s3-website.us-east-1.amazonaws.com/schema/schema.json # Connect to database: backend: Postgres db_url: postgresql://user@host/database # set your password by providing it using --uri or set PGPASSWORD variable # backend: BigQuery # db_url: bigquery://bigquery.googleapis.com/project_name?dataset=default_dataset_name # Run `napkin auth` to obtain authentication token tables: example: create_action: sql_file: source: example.sql . First, we enable YAML schema support for IDE plugins, then we choose backend and provide DB connection details, finally, we’ll replace the example Napkin managed table. Before moving to the next section, let’s validate our project using Napkin’s validate command: . shell . napkin validate . Let’s keep validating our spec as we move through the tutorial. You can also conveniently run validation in interactive mode, so Napkin will revalidate spec whenever the Spec file or queries are changed. shell . napkin validate --interactive . By default, Napkin will use specs/spec.yaml relative to the current directory for all commands. You can explicitly select the spec with --spec-file argument. Napkin follows a common paradigm where source tables are not mutated by Napkin. Instead, Napkin creates a series of dependent tables, managed tables, based on the SQL files in sql/ folder. Internally, Napkin uses specs/spec.yaml to create a DAG that describes data dependencies between managed and unmanaged tables. Napkin uses this DAG to drive the execution plan. In addition to DAG, Napkin Spec contains back-end connection-related data for connecting to supported target databases. ",
    "url": "https://docs.napkin.run/fundamentals/#bootstrapping-napkin-project",
    "relUrl": "/fundamentals/#bootstrapping-napkin-project"
  },"11": {
    "doc": "Fundamentals",
    "title": "Setting DB connection",
    "content": "Before we can do any operations on the database, we need to configure the database connection. In our case, we need to point Napkin to the SQLite file. We can do this by replacing backend and db_uri in specs/spec.yaml file. specs/spec.yaml . # Connect to Sqlite database: backend: Sqlite db_url: sqlite:chinook-sql.db . Let’s also remove the definition of the example table from our Spec, as well as sql/example.sql it references, as we no longer need them. Now, we can proceed with running our queries. ",
    "url": "https://docs.napkin.run/fundamentals/#setting-db-connection",
    "relUrl": "/fundamentals/#setting-db-connection"
  },"12": {
    "doc": "Fundamentals",
    "title": "The first queries",
    "content": "Let’s start exploring our data set and calculate a summary of sales in each country. First, we need to create a SQL query and store it in sql/sales_by_country.sql file. sql/sales_by_country.sql . SELECT \"BillingCountry\" AS \"Country\", SUM(\"Total\") AS \"Sales\", COUNT(*) AS \"NumInvoices\" FROM \"Invoice\" GROUP BY \"BillingCountry\" . Now we need to link our SQL file to the sales_by_country table we’d like to create. Let’s add the following to the specs/spec.yaml file. specs/spec.yaml . tables: sales_by_country: create_action: sql_file: source: sales_by_country.sql . By convention, Napkin will load SQL files from ../sql relative to the Spec file. This can be overridden with sql_dir: some_other_dir in the Spec file. Let’s validate the spec again with napkin validate, and then we will be ready to run it for the first time: . shell . napkin run . output . [2022-01-12 17:40:28][Info] Determining tables for update... [2022-01-12 17:40:28][Info] Forced tables: none [2022-01-12 17:40:28][Info] Skipped tables: none [2022-01-12 17:40:28][Info] Unmanaged tables that will be used as an input in this run: - Invoice [2022-01-12 17:40:28][Info] Managed tables that will be used as an input in this run, but will not be updated: none [2022-01-12 17:40:28][Info] Managed tables that will be updated in this run: - sales_by_country [2022-01-12 17:40:28][Info] Estimated runtime: 0s [2022-01-12 17:40:28][Info] Running table hooks (Pre) [2022-01-12 17:40:28][Info] Executing table's action [2022-01-12 17:40:28][Info] Table's action complete. [2022-01-12 17:40:28][Info] Running table hooks (Post) [2022-01-12 17:40:28][Info] Table' processing complete. [2022-01-12 17:40:28][Info] TableSpec \"sales_by_country\" server stats: [2022-01-12 17:40:28][Info] Execution completed. Please see below for a summary. [2022-01-12 17:40:28][Info] ---------------------------------------------------------- [2022-01-12 17:40:28][Info] Table \"sales_by_country\" ran in 0.02: [2022-01-12 17:40:28][Info] Run complete. Total cost: . Now the sales_by_country should exist in the database. Let’s double-check: . shell . sqlite3 chinook-sql.db \"SELECT * FROM sales_by_country\" . output . Argentina|37.62|7 Australia|37.62|7 Austria|42.62|7 Belgium|37.62|7 ... ",
    "url": "https://docs.napkin.run/fundamentals/#the-first-queries",
    "relUrl": "/fundamentals/#the-first-queries"
  },"13": {
    "doc": "Fundamentals",
    "title": "Best markets",
    "content": "We have decided to perform a more detailed analysis for ten countries with top sales. We expect to reuse that list over and over again, but also we would like to do this following the DRY principle. Since we have already calculated country sales, we can use sales_by_country table as an input. Let’s add top_10_countries table to our Spec: . sql/top_10_countries.sql . SELECT * FROM sales_by_country ORDER BY \"Sales\" DESC FETCH FIRST 10 ROWS ONLY . specs/spec.yaml . tables: ... top_10_countries: create_action: sql_file: source: top_10_countries.sql . Next, we would like to know the number of customers in our top countries. Let’s add this to spec as well: . sql/top_10_countries_customers.sql . SELECT top_10_countries.\"Country\" AS \"Country\", COUNT(*) AS \"NumCustomers\" FROM top_10_countries JOIN \"Customer\" ON top_10_countries.\"Country\" = \"Customer\".\"Country\" GROUP BY top_10_countries.\"Country\" . specs/spec.yaml . tables: ... top_10_countries_customers: create_action: sql_file: source: top_10_countries_customers.sql . Let’s run the spec again and check if it’s all correct: . shell . sqlite3 chinook-sql.db \"SELECT * FROM top_10_countries_customers\" . output . Brazil|5 Canada|8 Chile|1 Czech Republic|2 France|5 Germany|4 India|2 Portugal|2 USA|13 United Kingdom|3 . ",
    "url": "https://docs.napkin.run/fundamentals/#best-markets",
    "relUrl": "/fundamentals/#best-markets"
  },"14": {
    "doc": "Fundamentals",
    "title": "Discovering dependencies",
    "content": "We have added three tables so far to the Spec and successfully ran them. Napkin did some heavy lifting for us – it analyzed all queries, figured out dependencies, and executed the queries in the correct order. Let’s explore this with dump command which will store all queries (after being processed by Napkin), as well as dependency graph into dump/ folder: . shell . napkin dump . dump ├── 1_sales_by_country.sql ├── 2_top_10_countries.sql ├── 3_top_10_countries_customers.sql ├── dependency_graph.dot ├── dependency_graph.pdf └── MANIFEST.txt . The dependency_graph.pdf file shows what dependencies have been discovered by Napkin. Note that Invoice and Customer tables are displayed differently, as they are not managed by Napkin. ",
    "url": "https://docs.napkin.run/fundamentals/#discovering-dependencies",
    "relUrl": "/fundamentals/#discovering-dependencies"
  },"15": {
    "doc": "Fundamentals",
    "title": "DRY: Don’t Repeat Yourself",
    "content": "Let’s explore our customers. Let’s first aggregate total sales per customer: . sql/customers_total_purchase.sql . SELECT \"Customer\".\"CustomerId\" AS \"CustomerId\", \"Customer\".\"Email\" AS \"Email\", \"Customer\".\"FirstName\" AS \"FirstName\", \"Customer\".\"LastName\" AS \"LastName\", SUM(\"Invoice\".\"Total\") AS \"CustomerSales\" FROM \"Customer\" JOIN \"Invoice\" ON \"Customer\".\"CustomerId\" = \"Invoice\".\"CustomerId\" GROUP BY \"Customer\".\"CustomerId\" . specs/spec.yaml . tables: # ... customers_total_purchase: create_action: sql_file: source: customers_total_purchase.sql . Next, we’d like to find out our top 20 customers: . sql/top_20_customers.sql . SELECT * FROM customers_total_purchase ORDER BY \"CustomerSales\" DESC FETCH FIRST 20 ROWS ONLY . specs/spec.yaml . tables: # ... top_20_customers: create_action: sql_file: source: top_20_customers.sql . Doesn’t that look familiar? Can we do better? The answer is: yes! Let’s start with creating our template query in sql/top_performers.sql file with some placeholders: . sql/top_performers.sql . SELECT * FROM \"{{table}}\" ORDER BY \"{{top_by}}\" DESC FETCH FIRST {{max_limit}} ROWS ONLY . How do we use it? Let’s refactor our table definitions: . specs/spec.yaml . tables: # ... top_10_countries: create_action: sql_file: source: top_performers.sql vars: table: sales_by_country max_limit: 10 top_by: \"Sales\" # ... top_20_customers: create_action: sql_file: source: top_performers.sql vars: table: customers_total_purchase max_limit: 20 top_by: \"CustomerSales\" . Napkin uses Mustache template language to substitute variables specified in triple curly braces with values provided in Spec file. This enables to reuse queries and keep the configuration in the Spec. We can now run our spec again and all new tables will be created for us. If we dump again, the dependency graph will be updated: . ",
    "url": "https://docs.napkin.run/fundamentals/#dry-dont-repeat-yourself",
    "relUrl": "/fundamentals/#dry-dont-repeat-yourself"
  },"16": {
    "doc": "Fundamentals",
    "title": "Garbage in, garbage out: validating data",
    "content": "So far, we were able to verify if the results look good manually. However, as the spec grows and the source data is constantly updated by upstream services, it becomes a tedious and error-prone job. We would better like to automate this process to some extent. Napkin provides a way to run assertions on input data and the results of each table. Let’s invent a few invariants we could expect always be true from the data we have computed: . | sales_by_country: . | Country column should be unique, | should have more than 20 rows; | . | top_10_countries: . | Country column should be unique, | should have exactly 10 rows; | . | top_10_countries_customers: . | Country column should be unique, | should have exactly 10 rows, | NumCustomers column should have all values greater than zero; | . | customers_total_purchase: . | CustomerSales column should have all values greater than zero, | CustomerId column should be unique; | . | top_20_customers: . | should have exactly 20 rows, | CustomerId column should be unique. | . | . Let’s start with checks for sales_by_country table, we will use assert_unique and assert_count assertions. Please refer to User Manual for reference on other hooks. specs/spec.yaml . tables: # ... sales_by_country: # create_action: ... post_hooks: - assert_unique: columns: [\"Country\"] - assert_count: greater_than: 20 . The invariant on customers_total_purchase that NumCustomers column has all values greater than zero is equivalent to requirement that minimum of that column is also greater than zero, and can be implemented with assert_expression hook: . specs/spec.yaml . tables: # ... customers_total_purchase: # create_action: ... post_hooks: # ... - assert_expression: expression: expression: MIN(\"CustomerSales\") &gt; 0 . We can also check invariants on input tables. We recommend using pre_hooks instead, as they will run prior to the table execution. For example, we may expect that all invoices have a non-negative total: . tables: # ... sales_by_country: # create_action: ... # post_hooks: ... pre_hooks: - assert_expression: table: \"Invoice\" expression: expression: MIN(\"Total\") &gt;= 0 . Note that in this case we had to specify the table name manually – table argument defaults to the table being defined for post hooks only. After all checks have been implemented, our spec should look like this: . specs/spec.yaml . # yaml-language-server: $schema=http://napkin-public-storage-bucket.s3-website.us-east-1.amazonaws.com/schema/schema.json app_name: chinook-sqlite # Connect to database: backend: Sqlite db_url: sqlite:chinook.db tables: sales_by_country: create_action: sql_file: source: sales_by_country.sql post_hooks: - assert_unique: columns: [\"Country\"] - assert_count: greater_than: 20 pre_hooks: - assert_expression: table: \"Invoice\" expression: MIN(\"Total\") &gt;=0 top_10_countries: create_action: sql_file: source: top_performers.sql vars: table: sales_by_country max_limit: 10 top_by: \"Sales\" post_hooks: - assert_unique: columns: [\"Country\"] - assert_count: equal: 10 top_10_countries_customers: create_action: sql_file: source: top_10_countries_customers.sql post_hooks: - assert_unique: columns: [\"Country\"] - assert_count: equal: 10 - assert_expression: expression: MIN(\"NumCustomers\") &gt; 0 customers_total_purchase: create_action: sql_file: source: customers_total_purchase.sql post_hooks: - assert_unique: columns: [\"CustomerId\"] - assert_expression: expression: MIN(\"CustomerSales\") &gt;= 0 top_20_customers: create_action: sql_file: source: top_performers.sql vars: table: customers_total_purchase max_limit: 20 top_by: \"CustomerSales\" post_hooks: - assert_unique: columns: [\"CustomerId\"] - assert_count: equal: 20 - assert_expression: expression: MIN(\"CustomerSales\") &gt; 0 . When we run our spec, Napkin will report the status of all checks. By default, when any will fail, the spec execution will be aborted. We can make assertion to result in a warning only by adding on_failure: warn_only attribute. output . [2021-12-29 12:02:44][Info] Execution completed. Please see below for a summary. [2021-12-29 12:02:44][Info] ---------------------------------------------------------- [2021-12-29 12:02:44][Info] Table \"customers_total_purchase\" ran in 0.15: 0 rows affected [2021-12-29 12:02:44][Info] Assertion customers_total_purchase / Post / 1 / assertUniqueBy: customers_total_purchase: OK [2021-12-29 12:02:44][Info] Assertion customers_total_purchase / Post / 2 / assertExpression: customers_total_purchase: OK [2021-12-29 12:02:44][Info] Table \"sales_by_country\" ran in 0.19: 0 rows affected [2021-12-29 12:02:44][Info] Assertion sales_by_country / Pre / 1 / assertExpression: Invoice: OK [2021-12-29 12:02:44][Info] Assertion sales_by_country / Post / 1 / assertUniqueBy: sales_by_country: OK [2021-12-29 12:02:44][Info] Assertion sales_by_country / Post / 2 / assertCountConst: row count of \"sales_by_country\" should be greater than 20: OK [2021-12-29 12:02:44][Info] Table \"top_10_countries\" ran in 0.16: 0 rows affected [2021-12-29 12:02:44][Info] Assertion top_10_countries / Post / 1 / assertUniqueBy: top_10_countries: OK [2021-12-29 12:02:44][Info] Assertion top_10_countries / Post / 2 / assertCountConst: row count of \"top_10_countries\" should be equal to 10: OK [2021-12-29 12:02:44][Info] Table \"top_10_countries_customers\" ran in 0.09: 0 rows affected [2021-12-29 12:02:44][Info] Assertion top_10_countries_customers / Post / 1 / assertUniqueBy: top_10_countries_customers: OK [2021-12-29 12:02:44][Info] Assertion top_10_countries_customers / Post / 2 / assertCountConst: row count of \"top_10_countries_customers\" should be equal to 10: OK [2021-12-29 12:02:44][Info] Assertion top_10_countries_customers / Post / 3 / assertExpression: top_10_countries_customers: OK [2021-12-29 12:02:44][Info] Table \"top_20_customers\" ran in 0.13: 0 rows affected [2021-12-29 12:02:44][Info] Assertion top_20_customers / Post / 1 / assertUniqueBy: top_20_customers: OK [2021-12-29 12:02:44][Info] Assertion top_20_customers / Post / 2 / assertCountConst: row count of \"top_20_customers\" should be equal to 20: OK [2021-12-29 12:02:44][Info] Assertion top_20_customers / Post / 3 / assertExpression: top_20_customers: OK . ",
    "url": "https://docs.napkin.run/fundamentals/#garbage-in-garbage-out-validating-data",
    "relUrl": "/fundamentals/#garbage-in-garbage-out-validating-data"
  },"17": {
    "doc": "Fundamentals",
    "title": "Napkin remembers",
    "content": "Napkin internally keeps track of your application state. You might find it helpful to spend some time with: . shell . napkin history list --spec-file specs/spec.yaml . The data is stored by default in data/ directory in a Sqlite database. Alternatively, it may be stored in Postgres database (see metadata_url field in YAML reference). Data from historical runs is used for planning subsequent executions. As always, napkin history --help will provide a comprehensive list of history parameters. ",
    "url": "https://docs.napkin.run/fundamentals/#napkin-remembers",
    "relUrl": "/fundamentals/#napkin-remembers"
  },"18": {
    "doc": "Fundamentals",
    "title": "Saving our work",
    "content": "In addition to bootstrapping our project, the init command initialized an empty Git repository that we may use to add or push or code to a remote Git repository. ",
    "url": "https://docs.napkin.run/fundamentals/#saving-our-work",
    "relUrl": "/fundamentals/#saving-our-work"
  },"19": {
    "doc": "Fundamentals",
    "title": "Deploying to the production with PostgreSQL",
    "content": "We started this tutorial with the SQLite backend for convenience. In this optional section, we’ll change the backend to PostgreSQL which will let us run our spec in a more realistic, production-ready setup. Setting up the database . Devcontainer . If you are using the devcontainer, all the necessary setup for this tutorial has been already done. First, configure PGHOST, PGUSER, PGPASSWORD, and PGDATABASE to point to the appropriate PostgreSQL server. Setting up PostgreSQL is beyond the scope of this tutorial. For production usage, it’s convenient to separate raw and derived data. We will use PostgreSQL schemas for this. We will create a chinook schema for raw data and a development and a production schemas for derived data. Then, we will load the Chinook database into the chinook schema. For more details please refer to our Multi-environment pipelines in a team setting tutorial. shell . curl -L -O https://github.com/lerocha/chinook-database/raw/master/ChinookDatabase/DataSources/Chinook_PostgreSql.sql psql &lt;&lt;-EOSQL CREATE SCHEMA chinook; CREATE SCHEMA development; CREATE SCHEMA production; EOSQL PGOPTIONS='--search_path=chinook' PGCLIENTENCODING=iso-8859-1 psql -1 -q -f Chinook_PostgreSql.sql . Changing the backend . First, we need to change backend and db_uri in our spec file: . specs/spec-postgres.yaml . # Connect to database: backend: Postgres db_url: postgresql:// . Next, we need to specify table renaming rules, so we don’t need to specify chinook, development, and production schemas explicitly in each query. specs/spec-postgres.yaml . preprocessors: - table_namespace: value: development scope: managed override_with_arg: environment - table_namespace: value: chinook scope: unmanaged . With this configuration, Napkin will assume that all unmanaged tables are located in the chinook schema, and all managed tables will be created in the development schema by default. This can be later overridden with the --arg environment=production CLI argument. shell . napkin run --spec-file specs/spec-postgres.yaml . output . [2022-01-14 14:43:43][Info] Determining tables for update... [2022-01-14 14:43:43][Info] Forced tables: none [2022-01-14 14:43:43][Info] Skipped tables: none [2022-01-14 14:43:43][Info] Unmanaged tables that will be used as an input in this run: - chinook.Customer - chinook.Invoice [2022-01-14 14:43:43][Info] Managed tables that will be used as an input in this run, but will not be updated: none [2022-01-14 14:43:43][Info] Managed tables that will be updated in this run: - customers_total_purchase -&gt; development.customers_total_purchase - sales_by_country -&gt; development.sales_by_country - top_10_countries -&gt; development.top_10_countries - top_10_countries_customers -&gt; development.top_10_countries_customers - top_20_customers -&gt; development.top_20_customers [2022-01-14 14:43:43][Info] Estimated runtime: 0s [2022-01-14 14:43:43][Info] Running table hooks (Pre) [2022-01-14 14:43:43][Info] Executing table's action [2022-01-14 14:43:43][Info] Running table hooks (Pre) [2022-01-14 14:43:43][Info] Executing table's action [2022-01-14 14:43:43][Info] Table's action complete. [2022-01-14 14:43:43][Info] Running table hooks (Post) [2022-01-14 14:43:43][Info] Table' processing complete. [2022-01-14 14:43:43][Info] Table's action complete. [2022-01-14 14:43:43][Info] Running table hooks (Post) [2022-01-14 14:43:43][Info] Table' processing complete. [2022-01-14 14:43:43][Info] TableSpec \"development.sales_by_country\" server stats: 24 rows affected [2022-01-14 14:43:43][Info] TableSpec \"development.customers_total_purchase\" server stats: 59 rows affected [2022-01-14 14:43:43][Info] Running table hooks (Pre) [2022-01-14 14:43:43][Info] Executing table's action [2022-01-14 14:43:43][Info] Running table hooks (Pre) [2022-01-14 14:43:43][Info] Executing table's action [2022-01-14 14:43:43][Info] Table's action complete. [2022-01-14 14:43:43][Info] Running table hooks (Post) [2022-01-14 14:43:43][Info] Table' processing complete. [2022-01-14 14:43:43][Info] Table's action complete. [2022-01-14 14:43:43][Info] Running table hooks (Post) [2022-01-14 14:43:43][Info] Table' processing complete. [2022-01-14 14:43:43][Info] TableSpec \"development.top_10_countries\" server stats: 10 rows affected [2022-01-14 14:43:43][Info] TableSpec \"development.top_20_customers\" server stats: 20 rows affected [2022-01-14 14:43:43][Info] Running table hooks (Pre) [2022-01-14 14:43:43][Info] Executing table's action [2022-01-14 14:43:43][Info] Table's action complete. [2022-01-14 14:43:43][Info] Running table hooks (Post) [2022-01-14 14:43:43][Info] Table' processing complete. [2022-01-14 14:43:43][Info] TableSpec \"development.top_10_countries_customers\" server stats: 10 rows affected [2022-01-14 14:43:43][Info] Execution completed. Please see below for a summary. [2022-01-14 14:43:43][Info] ---------------------------------------------------------- [2022-01-14 14:43:43][Info] Table \"development\".\"customers_total_purchase\" ran in 0.07: 59 rows affected [2022-01-14 14:43:43][Info] Assertion development.customers_total_purchase / Post / 1 / assertUniqueBy: customers_total_purchase: OK [2022-01-14 14:43:43][Info] Assertion development.customers_total_purchase / Post / 2 / assertExpression: customers_total_purchase: OK [2022-01-14 14:43:43][Info] Table \"development\".\"sales_by_country\" ran in 0.06: 24 rows affected [2022-01-14 14:43:43][Info] Assertion development.sales_by_country / Pre / 1 / assertExpression: Invoice: OK [2022-01-14 14:43:43][Info] Assertion development.sales_by_country / Post / 1 / assertUniqueBy: sales_by_country: OK [2022-01-14 14:43:43][Info] Assertion development.sales_by_country / Post / 2 / assertCountConst: row count of \"sales_by_country\" should be greater than 10: OK [2022-01-14 14:43:43][Info] Table \"development\".\"top_10_countries\" ran in 0.06: 10 rows affected [2022-01-14 14:43:43][Info] Assertion development.top_10_countries / Post / 1 / assertUniqueBy: top_10_countries: OK [2022-01-14 14:43:43][Info] Assertion development.top_10_countries / Post / 2 / assertCountConst: row count of \"top_10_countries\" should be equal to 10: OK [2022-01-14 14:43:43][Info] Table \"development\".\"top_10_countries_customers\" ran in 0.04: 10 rows affected [2022-01-14 14:43:43][Info] Assertion development.top_10_countries_customers / Post / 1 / assertUniqueBy: top_10_countries_customers: OK [2022-01-14 14:43:43][Info] Assertion development.top_10_countries_customers / Post / 2 / assertCountConst: row count of \"top_10_countries_customers\" should be equal to 10: OK [2022-01-14 14:43:43][Info] Assertion development.top_10_countries_customers / Post / 3 / assertExpression: top_10_countries_customers: OK [2022-01-14 14:43:43][Info] Table \"development\".\"top_20_customers\" ran in 0.06: 20 rows affected [2022-01-14 14:43:43][Info] Assertion development.top_20_customers / Post / 1 / assertUniqueBy: top_20_customers: OK [2022-01-14 14:43:43][Info] Assertion development.top_20_customers / Post / 2 / assertCountConst: row count of \"top_20_customers\" should be equal to 20: OK [2022-01-14 14:43:43][Info] Assertion development.top_20_customers / Post / 3 / assertExpression: top_20_customers: OK [2022-01-14 14:43:43][Info] Run complete. Total cost: 123 rows affected . We can also perform a dry-run towards the production environment. We may observe in the logs, that the tables will be renamed accordingly to the environment specified. shell . napkin run --spec-file specs/spec-postgres.yaml --arg environment=production --dry-run . output . [2022-01-14 14:46:32][Info] Determining tables for update... [2022-01-14 14:46:32][Warning] Dry run, table specs and hooks will not be executed. [2022-01-14 14:46:32][Info] Forced tables: none [2022-01-14 14:46:32][Info] Skipped tables: none [2022-01-14 14:46:32][Info] Unmanaged tables that will be used as an input in this run: - chinook.Customer - chinook.Invoice [2022-01-14 14:46:32][Info] Managed tables that will be used as an input in this run, but will not be updated: none [2022-01-14 14:46:32][Info] Managed tables that will be updated in this run: - customers_total_purchase -&gt; production.customers_total_purchase - sales_by_country -&gt; production.sales_by_country - top_10_countries -&gt; production.top_10_countries - top_10_countries_customers -&gt; production.top_10_countries_customers - top_20_customers -&gt; production.top_20_customers [2022-01-14 14:46:32][Info] Estimated runtime: 0s [2022-01-14 14:46:32][Warning] Dry run, aborting execution. ",
    "url": "https://docs.napkin.run/fundamentals/#deploying-to-the-production-with-postgresql",
    "relUrl": "/fundamentals/#deploying-to-the-production-with-postgresql"
  },"20": {
    "doc": "Fundamentals",
    "title": "Summary",
    "content": "In this tutorial we: . | bootstrapped a data pipeline – a Spec, | declaratively managed dependencies among Spec components, | used assertions to verify Specs execution meets the given criteria, | execute our Spec, data pipeline, | stored our work, | change the data pipeline backend database. | . Please stay tuned for future Napkin tutorials. ",
    "url": "https://docs.napkin.run/fundamentals/#summary",
    "relUrl": "/fundamentals/#summary"
  },"21": {
    "doc": "Fundamentals",
    "title": "Fundamentals",
    "content": " ",
    "url": "https://docs.napkin.run/fundamentals/",
    "relUrl": "/fundamentals/"
  },"22": {
    "doc": "Getting started",
    "title": "Getting started with Napkin",
    "content": "Napkin is a command-line application that executes data pipelines of all sizes, backed by a feature-rich Haskell library offering programmatic freedom. It’s lightweight, offers a quick start for new projects, and yet scales to massive data pipelines with powerful meta-programming possibilities. This page is an overview of the Napkin documentation and related resources. ",
    "url": "https://docs.napkin.run/getting-started/#getting-started-with-napkin",
    "relUrl": "/getting-started/#getting-started-with-napkin"
  },"23": {
    "doc": "Getting started",
    "title": "Install",
    "content": "Napkin can be installed on Linux and macOS systems in a number of ways: . | Pre-build binary for Linux and macOS operating systems. This is the fastest and easiest way of getting Napkin. Just download and unpack – and you are good to go. | Homebrew on macOS. | Docker image with Napkin and various useful utilities. Installation with this method is easy too – pull the image and extract a wrapper script out of it. | Cachix distribution for NixOS. If you use NixOS, you are probably familiar with Cachix tool. Getting Napkin with Cachix is a one-line command. | VSCode devcontainer actually uses the same docker image under the hood as Docker installation method but enables code completion and other useful IDE features out-of-the-box. This is the preferred way of installing Napkin. | . Windows users can use Napkin with Docker and Devcontainer. ",
    "url": "https://docs.napkin.run/getting-started/#install",
    "relUrl": "/getting-started/#install"
  },"24": {
    "doc": "Getting started",
    "title": "Setup your IDE",
    "content": "Visual Studio Code is the recommended IDE to develop data processing pipelines with Napkin. Please follow our friendly guide to set up for the best experience. ",
    "url": "https://docs.napkin.run/getting-started/#setup-your-ide",
    "relUrl": "/getting-started/#setup-your-ide"
  },"25": {
    "doc": "Getting started",
    "title": "Create a first data pipeline",
    "content": "Our Fundamentals tutorial will guide you through creating the first data pipeline. We will start our pipeline with a few simple SQL queries, then add demonstrate how Napkin can be used not only to orchestrate pipeline execution but also to automatically test output data. ",
    "url": "https://docs.napkin.run/getting-started/#create-a-first-data-pipeline",
    "relUrl": "/getting-started/#create-a-first-data-pipeline"
  },"26": {
    "doc": "Getting started",
    "title": "Further reading",
    "content": ". | User manual | Tips and tricks | Changelog | . ",
    "url": "https://docs.napkin.run/getting-started/#further-reading",
    "relUrl": "/getting-started/#further-reading"
  },"27": {
    "doc": "Getting started",
    "title": "Getting started",
    "content": " ",
    "url": "https://docs.napkin.run/getting-started/",
    "relUrl": "/getting-started/"
  },"28": {
    "doc": "API Reference",
    "title": "API Reference",
    "content": "Napkin currently exposes all internal APIs. However, it is recommended to import just one modules from Napkin (to write custom haskell queries): Napkin.Backends.$BACKEND, where $BACKEND can be one of: . | BigQuery | Postgres | Redshift | Sqlite | . ",
    "url": "https://docs.napkin.run/haddock/",
    "relUrl": "/haddock/"
  },"29": {
    "doc": "API Reference",
    "title": "Haddocs for versions",
    "content": ". | 0.5.12 Currently in development Changelog . | 0.5.11 Release date: 2022-01-20 Changelog . | 0.5.10 Release date: 2021-12-29 Changelog . | 0.5.9 Release date: 2021-12-14 Changelog . | 0.5.8 Release date: 2021-12-08 Changelog . | . ",
    "url": "https://docs.napkin.run/haddock/#haddocs-for-versions",
    "relUrl": "/haddock/#haddocs-for-versions"
  },"30": {
    "doc": "About",
    "title": "What is Napkin",
    "content": "Napkin is a command line application that executes data pipelines of all sizes, backed by a feature-rich Haskell library offering programmatic freedom. It’s lightweight, offers a quick start for new projects and yet scales to massive data pipelines with powerful meta-programming possibilities. Napkin has a broad vision in making life easier for data scientists and engineers, encapsulating a large portion of the data engineering landscape. It therefore bundles several key features together: . | A consumer-grade Command Line Interface (CLI) that acts as the single point of entry for all typical workflows of data engineering and pipeline curation. The napkin app can refresh entire data pipelines, re-create individual tables, validate/typecheck pipelines in seconds, export dependency graphs and more. | A multi-backend (w.g. BigQuery and Redshift) database runtime environment that provides for all key capabilities in executing a modern data pipeline, including interacting the database (see what’s there, query tables, create/recreate/update tables, etc.), performing runtime unit-tests/assertions, logging, timing and interacting with the outside world. | A built-in DAG orchestrator that can automatically detect all the dependency relationships in a data pipeline (e.g. 30+ tables) and perform the pipeline updates in the correct order. Data pipelines are called “Spec”s in napkin and ship with all batteries included: Ability to rewrite table destinations into different schemas/datasets for different environments (e.g. devel vs. prod), mass-prefixing/renaming tables, setting different “Refresh Strategies” for each table (e.g. update daily vs. only update when missing), a wide range of data unit-tests (e.g. table must be unique by columns X+Y) that are automatically performed each time the table is updated. | For the power user, a SQL wrapper DSL in Haskell that stays as close as possible to SQL, without any intermediary object or relational mappings. This DSL looks almost like regular SQL, but allows sophisticated programmatic manipulation and composition of SQL queries and statements. Napkin can parse regular SQL into this internal DSL, perform any desired manipulations and render it back out as regular SQL. | A sophisticated SQL meta-programming environment that accelerates modern data engineering efforts. Napkin users can interweave several options for crafting SQL as they see fit, even in the same file. These options include: . | Writing plain SQL files without any low-grade templating noise. Napkin will still auto-detect all dependencies and make the pipeline “just work”. | Using lightweight variable substitutions in .sql files via Mustache templates. | Using sophisticated {{#sExp}} ... {{/sExp}} splices directly in .sql files to write Haskell code that dynamically generates SQL fragments on the fly. | Expressing entire queries directly using napkin’s Haskell DSL, often used for dynamic generation of SQL code based on complex logic. For example, prediction trees can be rendered into SQL this way, sometimes generated 100K LOC SQL files from a single model. | . | . ",
    "url": "https://docs.napkin.run/#what-is-napkin",
    "relUrl": "/#what-is-napkin"
  },"31": {
    "doc": "About",
    "title": "Napkin’s Philosophy",
    "content": "Napkin was created to capitalize on an opportunity we noticed back in 2015 to (massively) accelerate our team’s data engineering capabilities and yet make the resulting code-bases way more sustainable/maintainable. At the time, we were drowning in the complexity of custom Hadoop MapReduce programs, Spark programs and repositories of ad-hoc SQL scripts targeted on Redshift/Hive/etc at the time. We created napkin because we sorely needed something more practical and reliable for our own work. Over time, the opportunities we saw got crystallized into a set of philosophies we can articulate about what napkin is trying to achieve and whether it may be the huge catalyst for your team that it has been for us. ",
    "url": "https://docs.napkin.run/#napkins-philosophy",
    "relUrl": "/#napkins-philosophy"
  },"32": {
    "doc": "About",
    "title": "Base as much data compute as possible on SQL",
    "content": "Despite its age and missed opportunities, SQL code is declarative, functional and highly expressive. It’s easy to construct even for non-engineer data scientists/analysts and tends to offer good “equational reasoning”. It’s constrained just the right amount that business logic does not go “off the hook” like it can in typical programming languages like Python, R, Scala, etc. Once written and tested, SQL tends to produce reliable results. Over the years, we have found almost all data engineering efforts outside of SQL to be error-prone, hard to grow and expensive (e.g. needs data engineers) to maintain over time. If you can imagine how a table should be structured and express that table as a query in SQL, you can use napkin to engineer a pipeline. ",
    "url": "https://docs.napkin.run/#base-as-much-data-compute-as-possible-on-sql",
    "relUrl": "/#base-as-much-data-compute-as-possible-on-sql"
  },"33": {
    "doc": "About",
    "title": "Do as much compute as possible on modern analytics DBs like BigQuery/Redshift/Snowflake",
    "content": "Napkin aims to be a data engineering superpower even for very small teams. This is accomplished in large part by leaning on the amazing compute capabilities of modern analytics databases like BigQuery. Napkin’s creation goes back to our realization that if we could express even a very complex computation in SQL on these databases, no matter how convoluted, they would get the work done in astonishingly little time for minimal cost. In our work, we have produced numerous 200,000+ LOC SQL queries using napkin’s meta-programming capabilities that run within minutes on databases like Amazon Redshift and Google’s BigQuery. Fun fact: BigQuery has a ~1M character limit on queries, which we sometimes bypass by breaking complex queries into parts and joining them up / unioning them later. Even this transformation can be done automatically for you by napkin in certain cases! . ",
    "url": "https://docs.napkin.run/#do-as-much-compute-as-possible-on-modern-analytics-dbs-like-bigqueryredshiftsnowflake",
    "relUrl": "/#do-as-much-compute-as-possible-on-modern-analytics-dbs-like-bigqueryredshiftsnowflake"
  },"34": {
    "doc": "About",
    "title": "Abstract and reuse complex transformations where possible",
    "content": " ",
    "url": "https://docs.napkin.run/#abstract-and-reuse-complex-transformations-where-possible",
    "relUrl": "/#abstract-and-reuse-complex-transformations-where-possible"
  },"35": {
    "doc": "About",
    "title": "Data pipelines should be declarative and managed on Git",
    "content": " ",
    "url": "https://docs.napkin.run/#data-pipelines-should-be-declarative-and-managed-on-git",
    "relUrl": "/#data-pipelines-should-be-declarative-and-managed-on-git"
  },"36": {
    "doc": "About",
    "title": "Data pipelines should be regenerative",
    "content": " ",
    "url": "https://docs.napkin.run/#data-pipelines-should-be-regenerative",
    "relUrl": "/#data-pipelines-should-be-regenerative"
  },"37": {
    "doc": "About",
    "title": "Data pipeline dev should be lightweight on bare laptops",
    "content": " ",
    "url": "https://docs.napkin.run/#data-pipeline-dev-should-be-lightweight-on-bare-laptops",
    "relUrl": "/#data-pipeline-dev-should-be-lightweight-on-bare-laptops"
  },"38": {
    "doc": "About",
    "title": "Doctrine of extreme convenience",
    "content": "With napkin, we aim to make various data engineering and data science workflows so easy to perform that practitioners change their behavior to lean on them more frequently. We believe that speed and convenience without sacrificing correctness and reliability makes a huge difference in sustaining data ecosystem effectiveness. ",
    "url": "https://docs.napkin.run/#doctrine-of-extreme-convenience",
    "relUrl": "/#doctrine-of-extreme-convenience"
  },"39": {
    "doc": "About",
    "title": "Napkin’s Benefits",
    "content": "Here’s our best description of benefits you can expect after you’ve gotten a hang of napkin: . | You’ll be able to see and manage your entire data pipeline in a simple codebase, in declarative fashion and in source control - just like any modern software project. | You’ll always be able to “blow away and fully refresh” your entire pipeline from raw data at the push of a button - recovering from mistakes will be a breeze. | Your data pipeline will entirely rely on the power of your backend database, whatever it may be. The likes of BigQuery for large datasets or Postgres (or even Sqlite) when you can get away with it on small data. You won’t rely on error prone Python pandas code, your own custom data processing application and similar constructs that are hard to grow/maintain and ensure correctness over time. | Your data will have actual unit tests that will confirm correctness with each update. (Example: Making sure you don’t double count sales) . | You’ll benefit from tens of combinators we ship with napkin, such as incrementally updating large tables, column-to-row transformations, union-in same-structured tables into one, etc. As we improve napkin, you’ll get all that for free. | You’ll be able to implement your own clever SQL meta-programming to express logic that’d be too tedious to do in plain SQL. Yet the result will still have all the benefits of declarative SQL running on modern analytics databases, instead of your custom Python/R/Scala scripting machine. You’ll be able to create your own mini programs that produce 10-table “purchasing funnel” computations that connect just the right way based on configuration parameters supplied. | . ",
    "url": "https://docs.napkin.run/#napkins-benefits",
    "relUrl": "/#napkins-benefits"
  },"40": {
    "doc": "About",
    "title": "Napkin’s Future",
    "content": "Napkin is utilized heavily in commercial projects both at Soostone and at our clients. We improve napkin all the time and have a long backlog of major features we will realize in the future. We would like to be transparent with our roadmap and are looking for ways to best communicate our plans. We’re currently maintaining a Trello board with our roadmap where we would love to hear your reactions and feedback. You can access our roadmap board at Napkin Roadmap . ",
    "url": "https://docs.napkin.run/#napkins-future",
    "relUrl": "/#napkins-future"
  },"41": {
    "doc": "About",
    "title": "Next Steps",
    "content": "Continue with Tutorial . ",
    "url": "https://docs.napkin.run/#next-steps",
    "relUrl": "/#next-steps"
  },"42": {
    "doc": "About",
    "title": "About",
    "content": ". | What is Napkin | Napkin’s Philosophy . | Base as much data compute as possible on SQL | Do as much compute as possible on modern analytics DBs like BigQuery/Redshift/Snowflake | Abstract and reuse complex transformations where possible | Data pipelines should be declarative and managed on Git | Data pipelines should be regenerative | Data pipeline dev should be lightweight on bare laptops | Doctrine of extreme convenience | . | Napkin’s Benefits | Napkin’s Future | Next Steps | . ",
    "url": "https://docs.napkin.run/",
    "relUrl": "/"
  },"43": {
    "doc": "Cachix",
    "title": "Cachix",
    "content": " ",
    "url": "https://docs.napkin.run/install#cachix",
    "relUrl": "/install#cachix"
  },"44": {
    "doc": "Docker",
    "title": "Docker",
    "content": " ",
    "url": "https://docs.napkin.run/install#docker",
    "relUrl": "/install#docker"
  },"45": {
    "doc": "Homebrew",
    "title": "Homebrew",
    "content": " ",
    "url": "https://docs.napkin.run/install#homebrew",
    "relUrl": "/install#homebrew"
  },"46": {
    "doc": "Installation",
    "title": "Installation",
    "content": "There are many ways to install and use Napkin. This page will help to decide which was is easier and better suited for your need. Main installation options are: . | Pre-build binary for Linux and macOS operating systems. This is the fastest and easiest way of getting Napkin. Just download and unpack – and you are good to go. | Homebrew on macOS. | Docker image with Napkin and various useful utilities. Installation with this method is easy too – pull the image and extract a wrapper script out of it. | Cachix distribution for NixOS. If you use NixOS, you are probably familiar with cachix tool. Getting Napkin with cachix is a one-line command. | VSCode devcontainer actually uses the same docker image under the hood as Docker installation method but enables code completion and other useful IDE features out-of-the-box. This is the preferred way of installing Napkin. | . ",
    "url": "https://docs.napkin.run/install/",
    "relUrl": "/install/"
  },"47": {
    "doc": "Installation",
    "title": "Native",
    "content": "The Napkin native binary is a self extracting archive with a binary file that does not require a docker or nix environment to be executed. The latest Linux and macOS versions are continuously updated as being released. Due to the technical limitations, this version of Napkin does not support some advanced features, but it is perfectly fine to use for basic use-cases. Please note, that archive contains not only a binary itself, but also a folder with dynamically loaded libraries for the corresponding operating system, which should be located in the following structure. If you need to move napkin binary to the different location, move the whole napkin directory instead. napkin ├── bin │   └── napkin └── lib ├── libc++.1.0.dylib ├── libc++abi.dylib ├── libcharset.1.dylib ├── libcom_err.3.0.dylib ├── libcrypto.1.1.dylib ├── libffi.8.dylib ├── libgmp.10.dylib ├── libgssapi_krb5.2.2.dylib ├── libiconv-nocharset.dylib ├── libiconv.dylib ├── libk5crypto.3.1.dylib ├── libkrb5.3.3.dylib ├── libkrb5support.1.1.dylib ├── libncursesw.6.dylib ├── libpq.5.dylib ├── libresolv.9.dylib ├── libssl.1.1.dylib └── libz.dylib . If you need a specific version of Napkin, use directory browser in these locations. | for Linux | for macOS | . ",
    "url": "https://docs.napkin.run/install/#native",
    "relUrl": "/install/#native"
  },"48": {
    "doc": "Installation",
    "title": "Homebrew",
    "content": "The Napkin native binary can be also installed on macOS with the help of Homebrew package manager. Napin can be installed by running these commands in the terminal: . shell . brew tap soostone/napkin brew install napkin . Due to the technical limitations, this version of Napkin does not support some advanced features, but it is perfectly fine to use for basic use-cases. ",
    "url": "https://docs.napkin.run/install/#homebrew",
    "relUrl": "/install/#homebrew"
  },"49": {
    "doc": "Installation",
    "title": "Docker",
    "content": "Napkin docker image contains a wrapper script, which is suited for better integration with your host computer while running Napkin (opening browser window, accessing settings, etc.). To install napkin and extract the wrapper script, execute following command: . shell . docker run --rm --pull=always soostone/napkin-exe cat /bin/napkin-docker &gt; ./napkin-docker &amp;&amp; chmod +x ./napkin-docker . Extracted ./napkin-docker script can be used the same way as Napkin native binary: . shell ./napkin-docker version . Napkin version: 0.5.10 Git commit hash: b8c0506dde5ed71b415e884f2b735a923a620813 Built at: 2021-12-23 17:01:34.628557302 UTC . If you need to use different version of Napkin, docker has the following naming theme: . | soostone/napkin-exe:latest - Released latest version (note, that soostone/napkin-exe and soostone/napkin-exe:latest are semantically equivalent). | soostone/napkin-exe:v$VERSION - Napkin from $VERSION version (for example, v0.5.9). | soostone/napkin-exe:v$VERSION-dev - Latest Napkin build for not yet released $VERSION version. | . So this command will download and the v0.5.9 version of Napkin and update the wrapper script in the current folder: . docker run --rm --pull=always soostone/napkin-exe:v0.5.9 cat /bin/napkin-docker &gt; ./napkin-docker &amp;&amp; chmod +x ./napkin-docker . Since docker image contains the wrapper script itself, update procedure is trivial, just pull the newer image and extract the script: . shell . docker run --pull=always --rm soostone/napkin-exe cat /bin/napkin-docker &gt; ./napkin-docker &amp;&amp; chmod +x ./napkin-docker . Docker manual contains all technical details about using Napkin from docker container. ",
    "url": "https://docs.napkin.run/install/#docker",
    "relUrl": "/install/#docker"
  },"50": {
    "doc": "Installation",
    "title": "Cachix",
    "content": "Installation via cachix is intended for NixOS users and consists of the following steps: . | Install cachix tool by following official instructions (step 2) | . shell . nix-env -iA cachix -f https://cachix.org/api/v1/install . | Install latest version of Napkin | . For macOS: . shell . sh &lt;(curl -sL http://napkin-public-storage-bucket.s3-website.us-east-1.amazonaws.com/cachix/darwin/branch/master/index.html) . For Linux: . shell . sh &lt;(curl -sL http://napkin-public-storage-bucket.s3-website.us-east-1.amazonaws.com/cachix/linux/branch/master/index.html) . To install order version of napkin, choose version and OS here and follow installation instructions. ",
    "url": "https://docs.napkin.run/install/#cachix",
    "relUrl": "/install/#cachix"
  },"51": {
    "doc": "Native",
    "title": "Native",
    "content": " ",
    "url": "https://docs.napkin.run/install#native",
    "relUrl": "/install#native"
  },"52": {
    "doc": "Writing Custom Hooks",
    "title": "Writing Custom Hooks with Haskell",
    "content": "Custom hooks have a type of HookProgram backend and they can be exposed to be available through YAML Spec by implementing an additional wrapper for passing arguments from YAML. Custom hooks can be used to implement assertions on data stored in the database (managed or unmanaged tables), as well as, to perform extra operations before or after table spec is executed. In contrast to custom spec programs, hooks are allowed to perform arbitrary IO, such as storing files on a disk or uploading results to cloud storage. Napkin does not include tables accessed by hooks in the dependency graph used to create an execution plan. Let’s implement a custom hook that we will later use as a pre-hook to check source tables are present in the database. First, let’s create a Haskell file haskell/CustomHooks.hs in the project folder. haskell/CustomHooks.hs . module CustomHooks where import Control.Lens ((&amp;), (.~)) import Data.Aeson ((.:)) import Napkin.Backends.Base . Next, we will define our hook program. It will accept a list of tables that need to be checked. It will use mapM_ to iterate over the list provided, checkTableExists will perform a check for table existence, and finally, we’ll assert the result value. We will use the failLater helper to make sure that all checks will be executed. Otherwise, the first failure would short-circuit the remaining checks. haskell/CustomHooks.hs . checkManyTablesExistHook :: [Ref Table] -&gt; HookProgram backend checkManyTablesExistHook = mapM_ $ \\tableName -&gt; failLater $ do checkResult &lt;- checkTableExists tableName assertTrue (\"check if table \" &lt;&gt; refText tableName &lt;&gt; \" exists\") checkResult . Testing hooks . Hooks can be easily tested in a REPL environment. Please refer to REPL tutorial. In this form, we could already use our custom hook from Haskell-based spec: . spec :: Spec b () spec = do defineTable $ tableWithQuery \"sometable\" someQuery &amp; specPreHooks .~ [CustomHooks.checkManyTablesExistHook [\"foo\", \"bar\", \"baz\"]] . However, we can also expose our custom hook to be used from YAML specs. To do this, we need to implement an argument parser. To do this, we implement a simple Aeson parser that will return HookProgram backend. Note that the parser has to be wrapped with HookProgramWithArgParser newtype constructor. checkManyTablesExist :: HookProgramWithArgParser backend checkManyTablesExist = HookProgramWithArgParser $ \\obj -&gt; do tables :: [Ref Table] &lt;- obj .: \"tables\" pure $ checkManyTablesExistHook tables . With that function implemented, we may refer to the hook in YAML-based spec: ... tables: some_table: ... post_hooks: - CustomHooks.checkManyTablesExist: tables: - foo - bar - baz . In some cases, hooks will not accept any arguments. parserlessHook can be used to reduce boilerplate necessary: . haskell/CustomHooks.hs . veryCustomHook :: HookProgramWithArgParser backend veryCustomHook = parserlessHook $ do logInfo \"Runnin veryCustomHook logic\" . This hook can be referenced from YAML as well: ... tables: some_table: ... post_hooks: - CustomHooks.veryCustomHook . ",
    "url": "https://docs.napkin.run/metaprogramming/custom-hooks/#writing-custom-hooks-with-haskell",
    "relUrl": "/metaprogramming/custom-hooks/#writing-custom-hooks-with-haskell"
  },"53": {
    "doc": "Writing Custom Hooks",
    "title": "Writing Custom Hooks",
    "content": " ",
    "url": "https://docs.napkin.run/metaprogramming/custom-hooks/",
    "relUrl": "/metaprogramming/custom-hooks/"
  },"54": {
    "doc": "Haskell API",
    "title": "Haskell API",
    "content": "Coming soon :zzz: . ",
    "url": "https://docs.napkin.run/metaprogramming/haskell/",
    "relUrl": "/metaprogramming/haskell/"
  },"55": {
    "doc": "Metaprogramming",
    "title": "Metaprogramming",
    "content": "Coming soon :zzz: . ",
    "url": "https://docs.napkin.run/metaprogramming/",
    "relUrl": "/metaprogramming/"
  },"56": {
    "doc": "REPL",
    "title": "REPL",
    "content": "napkin repl starts GHCI session that is preconfigured for Spec development. It can be used to run Spec and Hook programs interactively for easier debugging. In addition to configuring GHCI session with appropriate language extensions, search path and packages, Napkin will define helper macros that make running custom spec and hook programs easier. ",
    "url": "https://docs.napkin.run/metaprogramming/repl/",
    "relUrl": "/metaprogramming/repl/"
  },"57": {
    "doc": "REPL",
    "title": "runSpecProgram",
    "content": ":runSpecProgram macro can be used to run any SpecProgram' backend returnValue interactively. Note that programs used to create tables have a type of SpecProgram backend which forces return type to be (). However, for interactive runs, one may return any value. Returned value will be bound to result This macro accepts the target table name and the program as arguments. napkin-repl . import SomeModule :runSpecProgram \"target.table.name\" SomeModule.myProgram . Program can be inlined as well (make sure to use quotes or $ operator) . napkin-repl . :runSpecProgram \"target.table.name\" (logInfo \"I am a spec program\" &gt;&gt; pure \"some return value\") . or . napkin-repl . :runSpecProgram \"target.table.name\" $ logInfo \"I am a spec program\" &gt;&gt; pure \"some return value\" . napkin-repl&gt; :runSpecProgram \"target.table.name\" $ logInfo \"I am a spec program\" &gt;&gt; pure \"some return value\" [2022-01-13 11:46:31][Info] I am a spec program Right \"some return value\" napkin-repl&gt; :type result result :: Either Napkin.Run.Effects.Languages.NapkinError.NapkinEffectError [Char] napkin-repl&gt; result Right \"some return value\" . ",
    "url": "https://docs.napkin.run/metaprogramming/repl/#runspecprogram",
    "relUrl": "/metaprogramming/repl/#runspecprogram"
  },"58": {
    "doc": "REPL",
    "title": "runHookProgram",
    "content": ":runHookProgram macro allows to run arbitrary HookProgram' backend returnValue. Similarly to runSpecProgram, REPL helper allows to return value. napkin-repl . :runHookProgram (logInfo \"I am a hook program\" &gt;&gt; pure 0x1234) . napkin-repl&gt; :runHookProgram (logInfo \"I am a hook program\" &gt;&gt; pure 0x1234) [2022-01-13 11:52:14][Info] I am a hook program Right (Right ([],4660)) napkin-repl&gt; :type result result :: Either Napkin.Run.Effects.Languages.NapkinError.NapkinEffectError (Either (GHC.Base.NonEmpty Napkin.Run.Effects.Languages.Assertion.AssertionEntry) ([Napkin.Run.Effects.Languages.Assertion.AssertionEntry], Integer)) napkin-repl&gt; result Right (Right ([],4660)) . ",
    "url": "https://docs.napkin.run/metaprogramming/repl/#runhookprogram",
    "relUrl": "/metaprogramming/repl/#runhookprogram"
  },"59": {
    "doc": "REPL",
    "title": "runCustomSpec and runCustomHook",
    "content": ":runCustomSpec and :runCustomHook macros allow to run SpecProgramWithArgParser and HookProgramWithArgParser respectively: . | :runCustomSpec accepts SpecProgramWithArgParser backend, target table name, and a Map Text Aeson.Value as arguments. | :runCustomHook accepts HookProgramWithArgParser backend, target table name, and a Map Text Aeson.Value as arguments. | . napkin-repl . :runCustomSpec customSpec \"target.table.name\" [(\"arg\", A.String \"foo\")] . napkin-repl . :runCustomHook customHook [(\"arg\", A.String \"foo\")] . :{ let customHook :: HookProgramWithArgParser b customHook = HookProgramWithArgParser $ \\obj -&gt; do arg &lt;- obj .: \"arg\" pure $ do logInfo $ \"The arg was: \" &lt;&gt; arg :} . napkin-repl&gt; :runCustomHook customHook [(\"arg\" .= \"Hello world!\")] [2022-01-13 12:00:25][Info] The arg was: Hello world! Right (Right ([],())) . ",
    "url": "https://docs.napkin.run/metaprogramming/repl/#runcustomspec-and-runcustomhook",
    "relUrl": "/metaprogramming/repl/#runcustomspec-and-runcustomhook"
  },"60": {
    "doc": "REPL",
    "title": "Example",
    "content": "We can define a HookProgram with corresponding argument parser and test it directly from the REPL. For production use, we’d define them in a Haskell module and import them to REPL or reference in the Spec. :{ let checkManyTablesExistHook :: [Ref Table] -&gt; HookProgram b checkManyTablesExistHook = mapM_ $ \\table -&gt; do exists &lt;- checkTableExists table assertTrue (\"check if table \" &lt;&gt; refText table &lt;&gt; \" exists\") exists customHookWithArgs :: HookProgramWithArgParser b customHookWithArgs = HookProgramWithArgParser $ \\obj -&gt; do tables &lt;- obj .: \"tables\" pure $ checkManyTablesExistHook tables :} :runCustomHook customHookWithArgs [(\"tables\" .= [\"chinook.Artist\", \"does_not.exist\"])] . napkin-repl&gt; :runCustomHook customHookWithArgs [(\"tables\" .= [\"chinook.Artist\", \"does_not.exist\"])] ... some logs omited for brevity ... [2022-01-13 12:10:03][Debug] Executing query: SELECT exists((SELECT 1 AS \"aaac\" FROM \"pg_catalog\".\"pg_class\" AS \"aaaa\" INNER JOIN \"pg_catalog\".\"pg_namespace\" AS \"aaab\" ON (\"aaab\".\"oid\" = \"aaaa\".\"relnamespace\") WHERE ((\"aaab\".\"nspname\" = 'chinook')) and ((\"aaaa\".\"relname\" = 'Artist')))) AS \"check\" [2022-01-13 12:10:03][Debug] table chinook.Artist exists [2022-01-13 12:10:03][Debug] Executing query: SELECT exists((SELECT 1 AS \"aaac\" FROM \"pg_catalog\".\"pg_class\" AS \"aaaa\" INNER JOIN \"pg_catalog\".\"pg_namespace\" AS \"aaab\" ON (\"aaab\".\"oid\" = \"aaaa\".\"relnamespace\") WHERE ((\"aaab\".\"nspname\" = 'does_not')) and ((\"aaaa\".\"relname\" = 'exist')))) AS \"check\" [2022-01-13 12:10:03][Error] table does_not.exist exists Right (Left (AssertionEntry {assertionGroup = AssertionGroup [], assertionMessage = \"table chinook.Artist exists\", assertionStatus = Success, assertionSeverity = FailNow} :| [AssertionEntry {assertionGroup = AssertionGroup [], assertionMessage = \"table does_not.exist exists\", assertionStatus = Failure Nothing, assertionSeverity = FailNow}])) ... ",
    "url": "https://docs.napkin.run/metaprogramming/repl/#example",
    "relUrl": "/metaprogramming/repl/#example"
  },"61": {
    "doc": "Tips and Tricks",
    "title": "Introduction",
    "content": "The purpose of this page is to give capture a selection of napkin usage examples that occur very frequently in the day-to-day development and maintenance of a data pipeline. ",
    "url": "https://docs.napkin.run/tips-and-tricks/#introduction",
    "relUrl": "/tips-and-tricks/#introduction"
  },"62": {
    "doc": "Tips and Tricks",
    "title": "napkin run",
    "content": " ",
    "url": "https://docs.napkin.run/tips-and-tricks/#napkin-run",
    "relUrl": "/tips-and-tricks/#napkin-run"
  },"63": {
    "doc": "Tips and Tricks",
    "title": "Running only a single table in a spec",
    "content": "It’s very common when iteratively working on a given table to update it repeatedly and in isolation. Disable all tables, force-update any table with word “uplift” in it: . shell . napkin run -s specs/myspec1.yaml -D -f '.*(uplift).*' . Notice the pattern is a proper Regular Expression. ",
    "url": "https://docs.napkin.run/tips-and-tricks/#running-only-a-single-table-in-a-spec",
    "relUrl": "/tips-and-tricks/#running-only-a-single-table-in-a-spec"
  },"64": {
    "doc": "Tips and Tricks",
    "title": "napkin validate",
    "content": " ",
    "url": "https://docs.napkin.run/tips-and-tricks/#napkin-validate",
    "relUrl": "/tips-and-tricks/#napkin-validate"
  },"65": {
    "doc": "Tips and Tricks",
    "title": "Continuously validating codebase on every change",
    "content": "Keeping a validating screen open is invaluable in rapid iteration. Napkin will notice every change on every file that’s touched by a given spec and automatically re-validate the entire project. This will catch obvious structural errors in SQL files, mistakes in templates and any compilation errors in custom Haskell code. shell . napkin validate -s specs/myspec1.yaml --interactive . ",
    "url": "https://docs.napkin.run/tips-and-tricks/#continuously-validating-codebase-on-every-change",
    "relUrl": "/tips-and-tricks/#continuously-validating-codebase-on-every-change"
  },"66": {
    "doc": "Tips and Tricks",
    "title": "Tips and Tricks",
    "content": ". | Introduction | napkin run . | Running only a single table in a spec | . | napkin validate . | Continuously validating codebase on every change | . | . ",
    "url": "https://docs.napkin.run/tips-and-tricks/",
    "relUrl": "/tips-and-tricks/"
  },"67": {
    "doc": "CLI reference",
    "title": "Napkin",
    "content": "Napkin CLI application has a number of commands to develop, debug and execute specs . Usage: napkin [(-v|--verbose) | (-l|--log-level LOG_LEVEL)] [--log-format LOG_FORMAT] COMMAND CLI tool to work with Napkin Available options: -v,--verbose Print debug log -l,--log-level LOG_LEVEL Log severity level. Possible values: Debug, Info, Notice, Warning, Error, Critical, Alert, Emergency. (default: Info) --log-format LOG_FORMAT Log line format. Possible values: Simple, Server, Json. (default: Simple) -h,--help Show this help text Init: init Generates Napkin project from a template generate-spec Generates a Napkin spec from a directory of SQL files Execute: run Runs Napkin specs history Set of commands to work with Napkin previous runs auth Authenticates with Google BigQuery Develop: validate Validates YAML spec file dump Performs a dry run and stores queries for inspection repl Drops into Napkin repl optimize Spec/query semantic improvements (e.g. unused columns) Documentation: haddock Opens web page with Napkin haddocks docs Opens web page with Napkin tutorial IDE integration: yaml-schema Stores YAML schema in a file hie-bios Used by Haskell Language Server Version: version Prints Git SHA and version of the build . Napkin has number of global options, which are applicable to almost any command: . | -l, --log-level - Allows to specify log level for the Napkin execution. | -v, --verbose - Enables debug logging, equivalent of providing --log-level Debug. | --log-format - Allows to choose desired format of the log messages. Currently supported: . | Server log format uses Katip’s bracketFormat. | Simple log format is a simplified version of Katip’s bracketFormat. | Json log format uses Katip’s jsonFormat. | . | -h, --help - Displays traditional help message screen for each command. It is usually “safe” to append --help to any Napkin command to know more about options it supports. Napkin will also display the --help screen when it meets the unfamiliar option. | . ",
    "url": "https://docs.napkin.run/user-manual/cli-reference/#napkin",
    "relUrl": "/user-manual/cli-reference/#napkin"
  },"68": {
    "doc": "CLI reference",
    "title": "Init",
    "content": "Napkin project is a folder with a set of various files including a spec file in YAML format having lots of fields. shell . napkin init --help . Usage: napkin init [-t|--template TPL_REF] [-p|--parameter KEY=VALUE] [-n|--project-name PROJECT_NAME] [-f|--force] [-c|--current-dir] [-g|--init-git-repo] Generates Napkin project from a template Available options: -t,--template TPL_REF Template location: [[service='github':]user/repo/template-name service = (gitlab|bitbucket|github), or local path/url or :&lt;embedded template name&gt; (default: :basic) -p,--parameter KEY=VALUE Set custom template parameter -n,--project-name PROJECT_NAME Name of the project to create (default: &quot;napkin-project&quot;) -f,--force Overwrite existing files in case of name collision with files from a template -c,--current-dir Use current directory for the project -g,--init-git-repo Init Git repository in the project folder -h,--help Show this help text Global options: -v,--verbose Print debug log -l,--log-level LOG_LEVEL Log severity level. Possible values: Debug, Info, Notice, Warning, Error, Critical, Alert, Emergency. (default: Info) --log-format LOG_FORMAT Log line format. Possible values: Simple, Server, Json. (default: Simple) Generates Napkin project from a template. Napkin contains following embedded templates: \u001b:basic\u001b You can use custom templates from github, gitlab and bitbucket. Mustache syntax is used for values interpolation. Template variables are resolved with process environment variables and -p parameters override them. | With -n, or --project-name you can specify the name of the project to create. By default, Napkin will try to create a new directory with this name. | -t, --template allows to specify the template (builtin or external) for the Napkin to use. | -p, --parameter option allows to override variables within the template with custom values. | -f, --force flag will force Napkin to override target directory (see --project-name option) if it already exists or files created in the current directory in case of --current-dir was specified. | With -c, --current-dir option, Napkin will use current working directory to write new files into. | With -g, --init-git-repo, Napkin will additionally execute git init command in the directory used to created a new project. | . If you start a project from scratch then init command could save a few seconds by generating minimum working set and setting up a GIT repo. A generated project is based on a hsfiles template. Napkin has a builtin :basic template, but you can define your own templates and supply an url to a repo with them on github e.g. Template file is a list of mustache snippets for every file in a project tree. For example, following command will create napkin project with name napkinFromScratch from :basic template: . shell . napkin init -n napkinFromScratch -t :basic . ",
    "url": "https://docs.napkin.run/user-manual/cli-reference/#init",
    "relUrl": "/user-manual/cli-reference/#init"
  },"69": {
    "doc": "CLI reference",
    "title": "Run",
    "content": "napkin run is used to execute Specs. It provides a handful of options that enable you to adjust how the spec is going to be executed. In a summary, you can: . | perform a partial run that will execute only a subset of defined tables, | perform a dry-run and inspect the execution plan, | pass spec arguments, | enable CLI UI, | override database connection settings. | . Lets review the CLI command: . shell . napkin run --help . Usage: napkin run [-s|--spec-file SPEC_YAML] [--override /json/pointer={&quot;json&quot;: &quot;value&quot;}] [-Q|--metadata-connection-string URI] [-u|--uri URI] [-a|--app-name APPLICATION-NAME] [--credentials-db CREDENTIALS-DB-PATH] [--callback-http-port CALLBACK_HTTP_PORT] [-S|--strict-mustache] [-C|--credentials-file FILE] [--skip-all | --force-all | --only SELECTOR | --force-only SELECTOR] [--force SELECTOR | --force-with-downstream SELECTOR | --enable SELECTOR | --skip SELECTOR] [-p|--show-progress] [-r|--dry-run] [--arg ARG=VALUE | --arg-json JSON | --arg-file FILE] Runs Napkin specs Available options: -s,--spec-file SPEC_YAML Path to the spec yaml file (default: &quot;specs/spec.yaml&quot;) --override /json/pointer={&quot;json&quot;: &quot;value&quot;} Set arbitrary JSON pointer (RFC 6901) in YAML spec with the new value -Q,--metadata-connection-string URI An optional alternate connection string for tracking napkin operations. Currently supports SQLite3 and Postgres backends, e.g. sqlite:/some/file.sqlite3 or postgresql://user@host -u,--uri URI Connection URI to create a database connection postgres:/// # use libpq defaults postgres://user@remotehost/dbname redshift://user@remotehost/dbname bigquery://bigquery.googleapis.com/project?dataset=datasetname sqlite:///home/dev/myRepo/mySqlite.db -a,--app-name APPLICATION-NAME User's project or application name --credentials-db CREDENTIALS-DB-PATH Path to database connection credentials directory --callback-http-port CALLBACK_HTTP_PORT Network port number for the napkin Auth server -S,--strict-mustache Strict mustache validation mode -C,--credentials-file FILE Path to database connection credentials file --skip-all Table selector: Skip all tables, use other options to enable selected tables --force-all Table selector: Force-enable update of all tables, use other options to skip selected tables --only SELECTOR Table selector: Disable all tables except specified --force-only SELECTOR Table selector: Disable all tables except specified which will be forced --force SELECTOR Table selector: Force table update --force-with-downstream SELECTOR Table selector: Force table update, force downstream tables too --enable SELECTOR Table selector: Don't skip or force table, use the update strategy as specified in the Spec file --skip SELECTOR Table selector: Skip table update -p,--show-progress Show a progress bar if the terminal supports it -r,--dry-run Simulate run and report which tables would've been updated --arg ARG=VALUE Argument to be passed to spec --arg-json JSON Spec arguments encoded as JSON object --arg-file FILE Spec arguments stored in JSON file -h,--help Show this help text Global options: -v,--verbose Print debug log -l,--log-level LOG_LEVEL Log severity level. Possible values: Debug, Info, Notice, Warning, Error, Critical, Alert, Emergency. (default: Info) --log-format LOG_FORMAT Log line format. Possible values: Simple, Server, Json. (default: Simple) \u001bTable selectors and SELECTOR types\u001b Table selectors allow performing partial spec runs. This can be useful during spec development. All options are applied in sequence to the execution plan. --skip-all, --force-all, --only, and --force-only options can be specified as a first table selector only. --only, --force-only, --skip, --enable, --force, and --force-with-downstream options accept a SELECTOR, which can be one of: * unprocessed table name, as it was defined in the spec, e.g. \u001b--skip spec:marketing_analytics\u001b, * processed table name after all preprocessors have been applied, e.g. \u001b--skip db:development.john_marketing_analytics\u001b, * table tag, e.g. \u001b--enable tag:marketing\u001b. Table names can be filtered using three matchers: * literal table name, e.g. \u001b--skip spec:marketing_analytics\u001b, * table name pattern - an asterisk can be used as a wildcard at the beginning or at the end of each table name segment, e.g. \u001b--force spec:prod_*.marketing_*\u001b, * table name regex - regex has to be surrounded with /, e.g. \u001b--force spec:/^marketing.*/\u001b, Please refer to the User Manual for further reference. Example: napkin run --only tag:marketing --enable spec:sales_totals --skip spec:marketing_mailing_* . CLI UI . | -p, --show-progress – will enable terminal UI that will display spec execution progress. | . Dry runs . | -r, --dry-run – will perform a dry run of the Spec. Napkin will always print Spec execution plan prior to executing queries, so this can be used to verify if partial run options have been applied correctly: | . shell . napkin run --dry-run --force-only \"spec:artist*\" --skip \"spec:*hashes*\" . [2022-01-13 10:30:00][Info] Determining tables for update... [2022-01-13 10:30:00][Info] Forced tables: none [2022-01-13 10:30:00][Info] Skipped tables: - artists_hashes -&gt; development.unknown_artists_hashes - day_totals -&gt; development.unknown_day_totals - day_totals_2 -&gt; development.unknown_day_totals_2 - first_run_at -&gt; development.unknown_first_run_at - long_to_wide -&gt; development.unknown_long_to_wide - popular_tracks_for_pink_floyd -&gt; development.unknown_popular_tracks_for_pink_floyd - random -&gt; development.unknown_random - total_sales_by_year -&gt; development.unknown_total_sales_by_year - update_daily -&gt; development.unknown_update_daily - update_on_upstream -&gt; development.unknown_update_on_upstream [2022-01-13 10:30:00][Info] Unmanaged tables that will be used as an input in this run: - chinook.Album - chinook.Artist - chinook.Track [2022-01-13 10:30:00][Info] Managed tables that will be used as an input in this run, but will not be updated: none [2022-01-13 10:30:00][Info] Managed tables that will be updated in this run: - artist_album_count -&gt; development.unknown_artist_album_count - artist_album_count_via_mustache -&gt; development.unknown_artist_album_count_via_mustache - artist_album_count_via_mustache_2 -&gt; development.unknown_artist_album_count_via_mustache_2 - artist_track_count -&gt; development.unknown_artist_track_count - artist_track_count_2 -&gt; development.unknown_artist_track_count_2 - artist_track_count_view -&gt; development.unknown_artist_track_count_view [2022-01-13 10:30:00][Info] Estimated runtime: 35s [2022-01-13 10:30:00][Warning] Dry run, aborting execution. Partial spec runs . During Spec development it’s convenient to execute only tables the developer is working on. This allows for shorter feedback loop and can reduce costs as well. Napkin provides a number of command-line options that can be used to cherry-pick tables that will be executed during particular run. However, we don’t recommend using them for production runs. All flags mentioned below will amend the execution plan in their order of appearance. Initial execution plan: . | --skip-all will disable execution of all tables. Typically, it’d be combined with other options that will enable or force some tables. | --force-all will force execution of all tables. This overrides any update strategy that was set in table definitions. | --only will disable execution of all tables and enable tables that fulfill the selector. This brings back any update strategy that was set in table definitions. Synonym of --skip-all --enable SELECTOR. | --force-only will disable execution of all tables and force tables that fulfill the selector. This overrides any update strategy that was set in table definitions. Synonym of --skip-all --force SELECTOR. | . Updating execution plan: . | --force will force tables that fulfill the selector. This overrides any update strategy that was set in table definitions. | --force-with-downstream will force tables that fulfill the selector as well as all tables depend on them. This overrides any update strategy that was set in table definitions. | --enable will void any changes to execution plan that were introduced with previous option for tables that fulfill the selector and will bring back any update strategy that was set in table definitions. | --skip will disable execution of the tables that fulfill the selector. | . Table selectors . --only, --force-only, --force, --force-with-downstream, --enable, and --skip options accept a table selector. Napkin will fail when no tables match a selector, which may indicate user error. | db:PATTERN – will match against preprocessed table names (with all renaming applied). Wildcards or regular expressions may be used to match multiple tables. | spec:PATTERN – will match against unprocessed table names, i.e. the table names as they were specified in the spec file. Wildcards or regular expressions may be used to match multiple tables. If no renaming was done with preprocessors db: and spec: selectors will have the same effect. | tag:TAG – will match table tags. Wildcards or regular expressions cannot be used for tags. | . Where PATTERN can be: . | someproject.somedataset.sometable – an exact match. | someproject.prod_*.*sales* – a wildcard can be used at the beginning and at the end of each name segment, segments are matched separately, so * will never expand beyond name segment: . | production.derived.*marketing would match production.derived.new_marketing but not production.derived_update.marketing which a naive regex would otherwise match. | . | /REGEX/ – a regular expression. | . Example use cases . | --force-only spec:top_artists will disable all tables except top_artists table which will be forced, | --force-only spec:top_* will disable all tables and force all tables that have prefix top_, | --force-only db:*.*.top_* will disable all tables and force all tables that have prefix top_ in all projects and datasets, | --skip-all --force-with-downstream tag:monthly will disable all tables and force tables that have a tag monthly plus all tables that depend on them. | . Meta arguments . | --arg ARG=VALUE allows to provide ARG spec argument with the string value of VALUE. | --arg-json will accept JSON object as an argument that will be interpreted as key-value store. Values don’t need to be strings, they can be any valid JSON object. | --arg-file will read JSON object from a file. | . Multiple options --arg* options can be used at the same time and all data provided will be collected. Other options . | -u, --uri – can be used to override database connection string. This can be useful, if development and production data sets are located on different DB servers. | -C, --credentials-file can be used to provide database credentials manually. Please refer to Connecting to the database page. | --credentials-db – can be used to override location of Napkin’s credentials database. | --callback-http-port – can be used to override Napkin’s HTTP port used for handling BigQuery authorization. | . ",
    "url": "https://docs.napkin.run/user-manual/cli-reference/#run",
    "relUrl": "/user-manual/cli-reference/#run"
  },"70": {
    "doc": "CLI reference",
    "title": "History",
    "content": "Set of napkin history commands allows displaying information from Napkin’s metadata database in various forms. When using Napkin frequently, it might be handy to know: . | When Napkin was executed last time | What are the statuses of the tables | How many records were updated in each table | etc. | . Napkin assigns a GUID identifier to each run to uniquely identify it from other napkin run invocations. Along with GUID of the execution, Napkin stores the date and time when particular run command was issued to Napkin. Lets review the CLI command: . shell . napkin history --help . Usage: napkin history [-s|--spec-file SPEC_YAML] [-a|--app-name APPLICATION-NAME] [-Q|--metadata-connection-string URI] COMMAND Set of commands to work with Napkin previous runs Available options: -s,--spec-file SPEC_YAML Path to the spec yaml file (default: &quot;specs/spec.yaml&quot;) -a,--app-name APPLICATION-NAME User's project or application name -Q,--metadata-connection-string URI An optional alternate connection string for tracking napkin operations. Currently supports SQLite3 and Postgres backends, e.g. sqlite:/some/file.sqlite3 or postgresql://user@host -h,--help Show this help text Available commands: list Lists all previous Napkin runs show Displays Napkin run by ID. Uses latest run if ID wasn't provided gantt Exports Napkin run into a gantt chart. Uses latest run if ID wasn't provided Global options: -v,--verbose Print debug log -l,--log-level LOG_LEVEL Log severity level. Possible values: Debug, Info, Notice, Warning, Error, Critical, Alert, Emergency. (default: Info) --log-format LOG_FORMAT Log line format. Possible values: Simple, Server, Json. (default: Simple) . | -s, --spec-file as usual, allows to specify the path to the spec.yaml file. | --override allows to override any values in the spec.yaml file according to the JSON pointer specification. Can be helpful in cases, where you need to only override several values from spec.yaml for a particular run. | -a, --app-name allows to override app-name parameter from the spec.yaml file. | -Q, --metadata-connection-string allows to override the metadata storage location (see metadata_url parameter in the spec.yaml file) used by Napkin. This might be useful for the situations, when you want to store all metadata information in the central location for further analysis or to ‘port’ Napkin history from a computer, which was earlier used to run Napkin. | . List . shell . napkin history list --help . Usage: napkin history list [LIMIT] Lists all previous Napkin runs Available options: LIMIT Number of records to show (default: 10) -h,--help Show this help text Global options: -s,--spec-file SPEC_YAML Path to the spec yaml file (default: \"specs/spec.yaml\") -a,--app-name APPLICATION-NAME User's project or application name -Q,--metadata-connection-string URI An optional alternate connection string for tracking napkin operations. Currently supports SQLite3 and Postgres backends, e.g. sqlite:/some/file.sqlite3 or postgresql://user@host -v,--verbose Print debug log -l,--log-level LOG_LEVEL Log severity level. Possible values: Debug, Info, Notice, Warning, Error, Critical, Alert, Emergency. (default: Info) --log-format LOG_FORMAT Log line format. Possible values: Simple, Server, Json. (default: Simple) . | LIMIT parameter allows to specify the amount of records for the napkin history list command to display. If there are less invocation of the napkin run in history records, only those records will be displayed. | . For example: . shell . napkin history list . Napkin executed run with ID fd4bf846-da9c-4bb4-a1d0-b3c971e2d993 on 2022-01-03 16:51:50 with 10 tables Napkin executed run with ID 4fd5e08f-c03c-4d6b-b31d-d77ff742f8aa on 2022-01-03 16:51:45 with 17 tables Napkin executed run with ID 17664cb6-7357-45ad-9523-f36daa605a6f on 2022-01-03 16:51:31 with 10 tables Napkin executed run with ID a85cc760-d55e-499e-8c00-a67e4a44f55a on 2022-01-03 16:51:16 with 16 tables Napkin executed run with ID 7a554ca7-61dc-4884-9d06-a4614c9e2827 on 2022-01-03 16:51:00 with 15 tables . Show . shell . napkin history show --help . Usage: napkin history show [GUID] Displays Napkin run by ID. Uses latest run if ID wasn't provided Available options: GUID GUID of the Napkin run -h,--help Show this help text Global options: -s,--spec-file SPEC_YAML Path to the spec yaml file (default: \"specs/spec.yaml\") -a,--app-name APPLICATION-NAME User's project or application name -Q,--metadata-connection-string URI An optional alternate connection string for tracking napkin operations. Currently supports SQLite3 and Postgres backends, e.g. sqlite:/some/file.sqlite3 or postgresql://user@host -v,--verbose Print debug log -l,--log-level LOG_LEVEL Log severity level. Possible values: Debug, Info, Notice, Warning, Error, Critical, Alert, Emergency. (default: Info) --log-format LOG_FORMAT Log line format. Possible values: Simple, Server, Json. (default: Simple) . napkin history show command shows individual table statuses, start and finish execution timestamps, number of affected rows. shell . napkin history show 7a554ca7-61dc-4884-9d06-a4614c9e2827 . Table ns.implicit_aliases_2 started at 2022-01-03 16:51:50 and finished at 2022-01-03 16:51:50 successfully affecting 1 rows Table ns.alias_shadowing started at 2022-01-03 16:51:50 and finished at 2022-01-03 16:51:50 successfully affecting 1 rows Table ns.test_1 started at 2022-01-03 16:51:50 and finished at 2022-01-03 16:51:50 successfully affecting 1 rows Table ns.implicit_aliases started at 2022-01-03 16:51:50 and finished at 2022-01-03 16:51:50 successfully affecting 0 rows Table ns.cte_shadowing started at 2022-01-03 16:51:50 and finished at 2022-01-03 16:51:50 successfully affecting 1 rows Table ns.cte_aliases started at 2022-01-03 16:51:50 and finished at 2022-01-03 16:51:50 successfully affecting 1 rows Table ns.aliases_where_clause started at 2022-01-03 16:51:50 and finished at 2022-01-03 16:51:50 successfully affecting 0 rows Table ns.src_bar started at 2022-01-03 16:51:50 and finished at 2022-01-03 16:51:50 successfully affecting 1 rows Table ns.src_foo started at 2022-01-03 16:51:50 and finished at 2022-01-03 16:51:50 successfully affecting 1 rows Table ns.zzz started at 2022-01-03 16:51:50 and finished at 2022-01-03 16:51:50 successfully affecting 100 rows . | GUID parameter allows you to specify exact Napkin run to obtain information from. To see list of all runs, try napkin history list command. | . If GUID parameter is omitted, napkin history show defaults it to the “last Napkin execution” (the same as in git show). Gantt . napkin history gantt command allows to export metadata about particular Napkin execution into a human-readable form with help of Highcharts HTML library. shell . napkin history gantt --help . Usage: napkin history gantt [GUID] [-o|--output-dir DIR] Exports Napkin run into a gantt chart. Uses latest run if ID wasn't provided Available options: GUID GUID of the Napkin run -o,--output-dir DIR Directory containing OUTPUT files -h,--help Show this help text Global options: -s,--spec-file SPEC_YAML Path to the spec yaml file (default: \"specs/spec.yaml\") -a,--app-name APPLICATION-NAME User's project or application name -Q,--metadata-connection-string URI An optional alternate connection string for tracking napkin operations. Currently supports SQLite3 and Postgres backends, e.g. sqlite:/some/file.sqlite3 or postgresql://user@host -v,--verbose Print debug log -l,--log-level LOG_LEVEL Log severity level. Possible values: Debug, Info, Notice, Warning, Error, Critical, Alert, Emergency. (default: Info) --log-format LOG_FORMAT Log line format. Possible values: Simple, Server, Json. (default: Simple) . | -o, --output-dir parameter points Napkin to the directory, which should be used for exporting HTML file with a Gantt chart. By default, current working directory will be used. | . napkin history gantt command exports metadata of the specified Napkin run to the DIR directory in the form of Gantt chart (HTML page). shell . napkin history gantt 7a554ca7-61dc-4884-9d06-a4614c9e2827 -o data/ . Gantt chart generated: data/gantt_napkin_7a554ca7-61dc-4884-9d06-a4614c9e2827.html . For example: . If GUID parameter is omitted, napkin history gantt defaults it to the “last Napkin execution” (the same as in git show). ",
    "url": "https://docs.napkin.run/user-manual/cli-reference/#history",
    "relUrl": "/user-manual/cli-reference/#history"
  },"71": {
    "doc": "CLI reference",
    "title": "Validate",
    "content": "napkin validate command allows to check the whole spec for: . | Napkin’s ability to correctly parse and process all the sql files. | Napkin’s ability to compile and execute parts of the specs defined in Haskell. | Validity of the built-in and user-defined preprocessors. | Validity of the settings inside spec.yaml file. | Correctness of mustache interpolation instructions. | Absence of dependency loops across table definitions. | . It is a good idea to check for the spec validness before committing the changes to the source code repository (git), so that other team members will not be surprised that napkin run is unable for do it’s job. shell . napkin validate --help . Usage: napkin validate [-s|--spec-file SPEC_YAML] [--override /json/pointer={&quot;json&quot;: &quot;value&quot;}] [-i|--interactive] [-r|--rolling] [-S|--strict-mustache] [--arg ARG=VALUE | --arg-json JSON | --arg-file FILE] Validates YAML spec file Available options: -s,--spec-file SPEC_YAML Path to the spec yaml file (default: &quot;specs/spec.yaml&quot;) --override /json/pointer={&quot;json&quot;: &quot;value&quot;} Set arbitrary JSON pointer (RFC 6901) in YAML spec with the new value -i,--interactive Watches for changed files and constantly revalidates -r,--rolling Does not clear the screen in live validation mode -S,--strict-mustache Strict mustache validation mode --arg ARG=VALUE Argument to be passed to spec --arg-json JSON Spec arguments encoded as JSON object --arg-file FILE Spec arguments stored in JSON file -h,--help Show this help text Global options: -v,--verbose Print debug log -l,--log-level LOG_LEVEL Log severity level. Possible values: Debug, Info, Notice, Warning, Error, Critical, Alert, Emergency. (default: Info) --log-format LOG_FORMAT Log line format. Possible values: Simple, Server, Json. (default: Simple) . | In -i, --interactive mode, Napkin constantly monitors all the files and folders, mentioned in spec.yaml file automatically re-validates the spec on any change. It is convenient to have napkin validate --interactive process opened in a separate window, while you are working on the SQL files or the spec.yaml definition. | By default, --interactive mode clears the screen before each validation cycle. To switch that feature off, you can use -r, --rolling mode, in which Napkin will constantly append validation status and error messages (in any) to the end of the output. | Napkin has number of strict mustache validation checks, which are not enabled by default. Use -S, --strict-mustache flag to enable additional checks (see mustache interpolation for details). | . Without --interactive mode, napkin will print the single OK message to the terminal and exit with 0 exit code. shell . napkin validate -s specs/spec.yaml . output . OK . With --interactive mode, Napkin constantly monitors for the changes in all files mentioned in the spec.yaml and re-validates as soon as it sees a change. ",
    "url": "https://docs.napkin.run/user-manual/cli-reference/#validate",
    "relUrl": "/user-manual/cli-reference/#validate"
  },"72": {
    "doc": "CLI reference",
    "title": "Dump",
    "content": "Besides knowing that your spec is “OK” (by using napkin validate command), Napkin also provides a way to see SQL queries, which Napkin would execute on a real database engine in by using napkin run command. shell . napkin dump --help . Usage: napkin dump [-s|--spec-file SPEC_YAML] [--override /json/pointer={&quot;json&quot;: &quot;value&quot;}] [--arg ARG=VALUE | --arg-json JSON | --arg-file FILE] [-o|--output-dir DIR] [-f|--force] [-i|--generate-inserts] [-e|--exclude-unmanaged-tables] [-S|--strict-mustache] [--use-spec-names] Performs a dry run and stores queries for inspection Available options: -s,--spec-file SPEC_YAML Path to the spec yaml file (default: &quot;specs/spec.yaml&quot;) --override /json/pointer={&quot;json&quot;: &quot;value&quot;} Set arbitrary JSON pointer (RFC 6901) in YAML spec with the new value --arg ARG=VALUE Argument to be passed to spec --arg-json JSON Spec arguments encoded as JSON object --arg-file FILE Spec arguments stored in JSON file -o,--output-dir DIR Directory containing OUTPUT files (default: &quot;dump&quot;) -f,--force Force write to destination directory -i,--generate-inserts Generate inserts for tables instead of simply dumping the query -e,--exclude-unmanaged-tables Show only managed tables in the DOT graph exclusively -S,--strict-mustache Strict mustache validation mode --use-spec-names Use unprocessed table names as filenames -h,--help Show this help text Global options: -v,--verbose Print debug log -l,--log-level LOG_LEVEL Log severity level. Possible values: Debug, Info, Notice, Warning, Error, Critical, Alert, Emergency. (default: Info) --log-format LOG_FORMAT Log line format. Possible values: Simple, Server, Json. (default: Simple) . | -o, --output-dir parameter is used to specify output directory to put SQL files into. Directory will also contain dependency diagram (in form of .dot and .pdf files) between the tables. | -f, --force flag is used to indicate, that it is OK for Napkin to override target directory. | By default, Napkin will add both managed and unmanaged tabled into a dependency diagram. -e, --exclude-unmanaged-tables flag forces Napkin to only include managed tables. | -i, --generate-inserts flag forces Napkin to generate SQL queries in form of INSERT INTO __TABLE_NAME__ SELECT ... instead of default SELECT ... | with --use-spec-names Napkin will use original table names (as declared in spec.yaml) for the file names in dump output. Otherwise, file names are equal to the table names after applying preprocessors. | . Usually napkin dump command will create following directory structure: . shell . napkin dump -o dump . output . dump ├── 01_update_daily.sql ├── 02_update_on_upstream.sql ├── 03_random.sql ├── 04_first_run_at.sql ├── 05_total_sales_by_year.sql ├── 06_long_to_wide.1.sql ├── 06_long_to_wide.2.sql ├── 07_day_totals_2.create_1.sql ├── 07_day_totals_2.update_1.sql ├── 08_day_totals_1.create_1.sql ├── 08_day_totals_1.create_2.sql ├── 08_day_totals_1.update_1.sql ├── 09_day_totals.create_1.sql ├── 09_day_totals.create_2.sql ├── 09_day_totals.update_1.sql ├── 10_artist_hex.sql ├── 11_popular_tracks_for_pink_floyd.sql ├── 12_artist_track_count_view.sql ├── 13_artist_track_count.sql ├── 14_artist_track_count_2.sql ├── 15_artist_album_count_via_mustache_2.sql ├── 16_artist_album_count_via_mustache.sql ├── 17_artist_album_count.sql ├── MANIFEST.txt ├── dependency_graph.dot └── dependency_graph.pdf . Some source sql files, like day_totals.sql will produce multiple output files: . | 09_day_totals.create_1.sql, 09_day_totals.create_2.sql – queries Napkin would execute in order to create day_totals table is it is not exists yet. | 09_day_totals.update_1.sql – query Napkin would execute in order to update already existing day_totals table. | . Along with output SQL files, napkin dump command also generates dependency diagram between the tables in Graphviz format: . graph . digraph { 1 [label=Album ,shape=box ,style=dotted]; 2 [label=Artist ,shape=box ,style=dotted]; 3 [label=InvoiceLine ,shape=box ,style=dotted]; 4 [label=Track ,shape=box ,style=dotted]; 5 [label=artist_album_count ,fillcolor=\"#59a14f\" ,fontcolor=\"#ffffff\" ,style=filled]; 6 [label=artist_album_count_via_mustache ,fillcolor=\"#edc948\" ,fontcolor=\"#000000\" ,style=filled]; 7 [label=artist_album_count_via_mustache_2 ,fillcolor=\"#b07aa1\" ,fontcolor=\"#ffffff\" ,style=filled]; 8 [label=artist_hex ,fillcolor=\"#4e79a7\" ,fontcolor=\"#ffffff\" ,style=filled]; 9 [label=artist_track_count ,fillcolor=\"#f28e2b\" ,fontcolor=\"#000000\" ,style=filled]; 10 [label=artist_track_count_2 ,fillcolor=\"#e15759\" ,fontcolor=\"#ffffff\" ,style=filled]; 11 [label=artist_track_count_view ,fillcolor=\"#76b7b2\" ,fontcolor=\"#000000\" ,style=filled]; 12 [label=popular_tracks_for_pink_floyd ,fillcolor=\"#59a14f\" ,fontcolor=\"#ffffff\" ,style=filled]; 1 -&gt; 5; 1 -&gt; 6; 1 -&gt; 7; 1 -&gt; 9; 1 -&gt; 11; 1 -&gt; 12; 2 -&gt; 5; 2 -&gt; 6; 2 -&gt; 7; 2 -&gt; 8; 2 -&gt; 9; 2 -&gt; 11; 2 -&gt; 12; 3 -&gt; 12; 4 -&gt; 9; 4 -&gt; 11; 4 -&gt; 12; 9 -&gt; 10; } . In PDF format, this diagram (download) looks like that: . ",
    "url": "https://docs.napkin.run/user-manual/cli-reference/#dump",
    "relUrl": "/user-manual/cli-reference/#dump"
  },"73": {
    "doc": "CLI reference",
    "title": "Optimize",
    "content": "The command combines a set of features for discovery and fixing semantic issues with SQL queries or spec itself. Unused CTE column . CTE is a named subquery introduced with WITH keyword. In a complex query comprised of many CTE subqueries a user might forget to remove helper column just after debugging and the column remains in the query and contributes to spec running costs. table_a2.sql . with cte as (select a, b from t) select b from cte . shell . napkin optimize -s specs/spec.yaml . Table: tbl_b2 (table_a2.sql) unused column cte.a . Duplicated CTE . This optimization aims equivalent CTEs defined in the same query. table_a2.sql . with cte1 as (select a from t where b*a &gt; 0), cte2 as (select a from t where a*b &gt; 0) select b from t2 where a1 in cte1 or a2 in cte2 . Table: tbl_b2 (table_a2.sql) duplicated CTE tables: cte1, cte2 . By default optimize just prints discovered issues, though it is possible to ask Napkin to resolve the issues automatically. Automatic refactoring doesn’t preserve original SQL formatting. SQL based optimizer is supported only by generic dialect. shell . napkin optimize --help . Usage: napkin optimize [-s|--spec-file SPEC_YAML] [--mode ARG] [--selector ARG] [-S|--strict-mustache] [--arg ARG=VALUE | --arg-json JSON | --arg-file FILE] Spec/query semantic improvements (e.g. unused columns) Available options: -s,--spec-file SPEC_YAML Path to the spec yaml file (default: &quot;specs/spec.yaml&quot;) --mode ARG Mode for running optimizations. Could be: show-issues, interactive (default: show-issues) --selector ARG Optimization feature selector. By default all optimizations are on: unused-columns, duplicated-queries. To turn on extra feature use &quot;+&quot; prefix to remove feature &quot;-&quot; prefix and to run just specific features don't use any prefix. -S,--strict-mustache Strict mustache validation mode --arg ARG=VALUE Argument to be passed to spec --arg-json JSON Spec arguments encoded as JSON object --arg-file FILE Spec arguments stored in JSON file -h,--help Show this help text Global options: -v,--verbose Print debug log -l,--log-level LOG_LEVEL Log severity level. Possible values: Debug, Info, Notice, Warning, Error, Critical, Alert, Emergency. (default: Info) --log-format LOG_FORMAT Log line format. Possible values: Simple, Server, Json. (default: Simple) . | --mode TODO | --selector TODO | . ",
    "url": "https://docs.napkin.run/user-manual/cli-reference/#optimize",
    "relUrl": "/user-manual/cli-reference/#optimize"
  },"74": {
    "doc": "CLI reference",
    "title": "Haddock",
    "content": "shell . napkin haddock --help . Usage: napkin haddock Opens web page with Napkin haddocks Available options: -h,--help Show this help text Global options: -v,--verbose Print debug log -l,--log-level LOG_LEVEL Log severity level. Possible values: Debug, Info, Notice, Warning, Error, Critical, Alert, Emergency. (default: Info) --log-format LOG_FORMAT Log line format. Possible values: Simple, Server, Json. (default: Simple) . Napkin is written in Haskell and currently exposes all internal APIs for external use (as it allows meta-programming, expressing SQL statements programmatically, etc.). Links to the Napkin Haskell API documentation for each individual version can be found here. You can always “ask” Napkin CLI for the exact link to the API reference for the currently installed version. It will open a browser with an API docs for the exact git commit SHA Napkin was built from. shell . napkin haddock . output . Opening \"http://napkin-public-storage-bucket.s3-website.us-east-1.amazonaws.com/haddock/git-hash/a8811868647ccc54e5c716e1a2781fe1c8b2ef9b/index.html\" URL in the browser . ",
    "url": "https://docs.napkin.run/user-manual/cli-reference/#haddock",
    "relUrl": "/user-manual/cli-reference/#haddock"
  },"75": {
    "doc": "CLI reference",
    "title": "Docs",
    "content": "shell . napkin docs --help . Usage: napkin docs Opens web page with Napkin tutorial Available options: -h,--help Show this help text Global options: -v,--verbose Print debug log -l,--log-level LOG_LEVEL Log severity level. Possible values: Debug, Info, Notice, Warning, Error, Critical, Alert, Emergency. (default: Info) --log-format LOG_FORMAT Log line format. Possible values: Simple, Server, Json. (default: Simple) . As well as for Haddock API reference, Napkin CLI can be asked for a link to this documentation site. shell . napkin docs . Opening \"https://docs.napkin.run/fundamentals\" URL in the browser . ",
    "url": "https://docs.napkin.run/user-manual/cli-reference/#docs",
    "relUrl": "/user-manual/cli-reference/#docs"
  },"76": {
    "doc": "CLI reference",
    "title": "REPL",
    "content": "napkin repl can be useful when using advanced meta-programming features. When this command is used Napkin will spawn Haskell GHCI session. Spec file has to be provided, so the REPL session will be configured appropriately. shell . napkin run --help . Usage: napkin repl [-s|--spec-file SPEC_YAML] Drops into Napkin repl Available options: -s,--spec-file SPEC_YAML Path to the spec yaml file (default: &quot;specs/spec.yaml&quot;) -h,--help Show this help text Global options: -v,--verbose Print debug log -l,--log-level LOG_LEVEL Log severity level. Possible values: Debug, Info, Notice, Warning, Error, Critical, Alert, Emergency. (default: Info) --log-format LOG_FORMAT Log line format. Possible values: Simple, Server, Json. (default: Simple) . ",
    "url": "https://docs.napkin.run/user-manual/cli-reference/#repl",
    "relUrl": "/user-manual/cli-reference/#repl"
  },"77": {
    "doc": "CLI reference",
    "title": "Yaml-Schema",
    "content": "shell . napkin yaml-schema --help . Usage: napkin yaml-schema OUTPUT Stores YAML schema in a file Available options: OUTPUT Name of the file to put generated yaml schema -h,--help Show this help text Global options: -v,--verbose Print debug log -l,--log-level LOG_LEVEL Log severity level. Possible values: Debug, Info, Notice, Warning, Error, Critical, Alert, Emergency. (default: Info) --log-format LOG_FORMAT Log line format. Possible values: Simple, Server, Json. (default: Simple) . Many modern text editors and IDEs have (either native or through plugins) support for YAML Schema to enhance the process of editing complex YAML files. Napkin provides a way to export YAML schema to an external file - so it is possible to configure your editor properly. In addition, if your editor or plugin supports that feature, you can point it to the web link with a latest YAML schema. YAML schemas for previous releases can be found here. | OUTPUT parameter is a path to a file to put generate yaml schema for the spec.yaml config. | . For example: . shell . napkin yaml-schema $HOME/napkin.schema . Saving YAML schema to a file: /Users/user/napkin.schema . ",
    "url": "https://docs.napkin.run/user-manual/cli-reference/#yaml-schema",
    "relUrl": "/user-manual/cli-reference/#yaml-schema"
  },"78": {
    "doc": "CLI reference",
    "title": "Hie-Bios",
    "content": "shell . napkin hie-bios --help . Usage: napkin hie-bios [-s|--spec-file SPEC_YAML] [GHC_OPTIONS] Used by Haskell Language Server Available options: -s,--spec-file SPEC_YAML Path to the spec yaml file (default: \"specs/spec.yaml\") GHC_OPTIONS List of extra GHC arguments to pass -h,--help Show this help text Global options: -v,--verbose Print debug log -l,--log-level LOG_LEVEL Log severity level. Possible values: Debug, Info, Notice, Warning, Error, Critical, Alert, Emergency. (default: Info) --log-format LOG_FORMAT Log line format. Possible values: Simple, Server, Json. (default: Simple) . napkin hie-bios provides support for advanced meta-programming with Napkin. This command is not supposed to be used directly by the user. Haskell Language Server sets HIE_BIOS_OUTPUT environment variable, which is used by Napkin as a file name to write all inferred and user-supplied options to. For debug purposes, Napkin defaults file location to hie-bios.txt in the current working directory. shell . HIE_BIOS_OUTPUT=/dev/tty napkin hie-bios -s specs/spec.yaml ... -package containers -package napkin -package ordered-containers -package unordered-containers -package text -package containers -package aeson ... -XBangPatterns -XBinaryLiterals -XConstrainedClassMethods -XConstraintKinds -XDeriveDataTypeable -XDeriveFoldable ... Usually, it is enough to add the hie.yaml to the root of the Napkin project for the HLS plugin from your editor to load. hie.yaml . cradle: bios: shell: napkin hie-bios --spec-file specs/spec.yaml . | GHC_OPTIONS - List of additional GHC options, which Napkin will pass to the GHC through HLS. | . ",
    "url": "https://docs.napkin.run/user-manual/cli-reference/#hie-bios",
    "relUrl": "/user-manual/cli-reference/#hie-bios"
  },"79": {
    "doc": "CLI reference",
    "title": "Version",
    "content": "shell . napkin version --help . Usage: napkin version Prints Git SHA and version of the build Available options: -h,--help Show this help text Global options: -v,--verbose Print debug log -l,--log-level LOG_LEVEL Log severity level. Possible values: Debug, Info, Notice, Warning, Error, Critical, Alert, Emergency. (default: Info) --log-format LOG_FORMAT Log line format. Possible values: Simple, Server, Json. (default: Simple) . Napkin can print current version, release data and exact git commit on it was built. For example: . shell . napkin version . Napkin version: 0.5.11 Git commit hash: a8811868647ccc54e5c716e1a2781fe1c8b2ef9b Built at: 2022-01-04 10:24:21.398355 UTC . If you want to install a newer version of Napkin – please, follow installation instructions. ",
    "url": "https://docs.napkin.run/user-manual/cli-reference/#version",
    "relUrl": "/user-manual/cli-reference/#version"
  },"80": {
    "doc": "CLI reference",
    "title": "CLI reference",
    "content": ". | Napkin . | Init | Run . | CLI UI | Dry runs | Partial spec runs . | Table selectors | Example use cases | . | Meta arguments | Other options | . | History . | List | Show | Gantt | . | Validate | Dump | Optimize . | Unused CTE column | Duplicated CTE | . | Haddock | Docs | REPL | Yaml-Schema | Hie-Bios | Version | . | . ",
    "url": "https://docs.napkin.run/user-manual/cli-reference/",
    "relUrl": "/user-manual/cli-reference/"
  },"81": {
    "doc": "Connecting to the database",
    "title": "Connecting to the database",
    "content": "Napkin works with a single DB backend and connection string a time. Database connection string has to be configured in YAML spec and it has to match the selected backend. Connection string can be later overridden with --uri (-u) flag when spec is executed with napkin run command. ",
    "url": "https://docs.napkin.run/user-manual/db-connection/",
    "relUrl": "/user-manual/db-connection/"
  },"82": {
    "doc": "Connecting to the database",
    "title": "BigQuery",
    "content": "backend: BigQuery db_url: bigquery://bigquery.googleapis.com/PROJECT-NAME?dataset=DATASET-NAME . Database connection string (db_url in YAML or --uri) should be formatted as follows: bigquery://bigquery.googleapis.com/PROJECT-NAME. Furthermore, one can set default dataset by appending ?dataset=DATASET-NAME. Note that authentication credentials are cached for particular Spec YAML path, database connection string and app. If any of them changes you will need to authenticate again. The credentials are cached in ~/.napkin/oauthdb by default, this path can be changed with --credentials-db argument. Alternatively, one may pass credentials JSON using --credentials-file (-C) option. This option can be useful when running napkin in unattended configuration. Authenticating locally . Run napkin auth create command and follow the instructions in the web browser. Authenticating for unattended runs . First, authenticate locally. Then run napkin auth show and save value of OAuth2-JsonToken in a JSON file. The credentials JSON can be later passed to napkin run using --credentials-file (-C) option. It may be then stored in a secret store of CI/CD platform of choice. Please refer to GitHub Actions example for further reference. ",
    "url": "https://docs.napkin.run/user-manual/db-connection/#bigquery",
    "relUrl": "/user-manual/db-connection/#bigquery"
  },"83": {
    "doc": "Connecting to the database",
    "title": "Postgresql and Redshift",
    "content": "backend: Postgres db_url: postgresql://user@db/napkin_db . Connection for Postgres and Redshift, which uses Postgres protocol, can be configured with connection string and/or environment variables. Connection strings use standard syntax from libpq and can be summarized with the following examples: . postgresql:// postgresql://localhost postgresql://localhost:5433 postgresql://localhost/mydb postgresql://user@localhost postgresql://user:secret@localhost . Values that are not provided in the connection string will be sourced from environment variables if available. If postgresql:// connection string is used whole database connection configuration will be sourced from environment variables. This can be useful in unattended environments. For more examples and further reference on connection string and environment variables please visit libpq manual. We do not recommend storing any passwords in the YAML spec. Instead, one can: . | store the password in a text file, then pass the file location with --credentials-file (-C) option, | store the password in a text file, then pass the file location with PGPASSFILE environment variable, | provide the password with the PGPASSWORD environment variable. | . ",
    "url": "https://docs.napkin.run/user-manual/db-connection/#postgresql-and-redshift",
    "relUrl": "/user-manual/db-connection/#postgresql-and-redshift"
  },"84": {
    "doc": "Connecting to the database",
    "title": "Sqlite",
    "content": "backend: Sqlite db_url: sqlite:some-folder/dbfile.db . Connection string is a path to the database file (with sqlite: prefix). ",
    "url": "https://docs.napkin.run/user-manual/db-connection/#sqlite",
    "relUrl": "/user-manual/db-connection/#sqlite"
  },"85": {
    "doc": "Devcontainer",
    "title": "Devcontainer",
    "content": "Devcontainer is a Visual Studio Code feature that enables to do development in Docker containers. Since IDE has native support for Docker-based development environments the usual drawbacks of such environments have been eliminated. The IDE will launch the necessary docker image that contains a comprehensive development environment, as well as will mount all the necessary volumes. Since the devcontainer configuration can be checked into the Git repo, it can be easily maintained in a team environment. The Docker host can be either the local machine (including Docker Desktop on macOS) or remote docker host (e.g. powerful cloud VM, or a server running in a private network). ",
    "url": "https://docs.napkin.run/user-manual/devcontainer/",
    "relUrl": "/user-manual/devcontainer/"
  },"86": {
    "doc": "Devcontainer",
    "title": "Using devcontainer enabled projects",
    "content": "The Napkin Docker image uses VSCode devcontainer as a base. It is based on Ubuntu Hirsute and includes Napkin, Git, and other common utilities. If other tools are necessary, one can further extend by adding extra layers with Dockerfile as described in this document. First, clone your Git repo as usual. Next, select “Reopen folder in container” when prompted. IDE will reload and open your project in a Docker-based environment with Napkin and other development tools available. ",
    "url": "https://docs.napkin.run/user-manual/devcontainer/#using-devcontainer-enabled-projects",
    "relUrl": "/user-manual/devcontainer/#using-devcontainer-enabled-projects"
  },"87": {
    "doc": "Devcontainer",
    "title": "Configuring the devcontainer in a new project",
    "content": "If you have used napkin init to create your project all necessary files have already been created for you. Minimal setup . Add .devcontainer.json file to the project root, then select “Reopen folder in Container”.devcontainer.json . { \"name\": \"Napkin Project\", \"image\": \"soostone/napkin-exe\", \"settings\": { \"yaml.schemas\": {\"/usr/share/napkin/spec-schema.json\": \"spec*.yaml\"} }, \"extensions\": [ \"haskell.haskell\", \"redhat.vscode-yaml\", \"eamodio.gitlens\", \"ms-azuretools.vscode-docker\", \"shinichi-takii.sql-bigquery\", \"ms-ossdata.vscode-postgresql\" ], \"forwardPorts\": [9901], // optional, can rely on auto forward, needed only for BigQuery \"remoteUser\": \"napkin\" } . Advanced setup with extra development tools . Should extra development tools be necessary, you can extend your development environment by extending the Napkin image with extra layers. Remember to select “Rebuild container” whenever you update .devcontainer.json or Dockerfile. Please refer to the manual for more information.devcontainer.json . { \"name\": \"Napkin Project\", \"build\": { \"dockerfile\": \"Dockerfile\", \"context\": \"../\", // Update 'NAPKIN_DOCKER_TAG' to upgrade napkin \"args\": { \"NAPKIN_DOCKER_TAG\": \"v1.2.3\" } }, // Set *default* container specific settings.json values on container create. \"settings\": { \"yaml.schemas\": {\"/usr/share/napkin/spec-schema.json\": \"spec*.yaml\"} }, // Add the IDs of extensions you want to be installed when the container is created. \"extensions\": [ \"haskell.haskell\", \"redhat.vscode-yaml\", \"eamodio.gitlens\", \"ms-azuretools.vscode-docker\", \"shinichi-takii.sql-bigquery\", \"ms-ossdata.vscode-postgresql\" ], // Use 'forwardPorts' to make a list of ports inside the container available locally. \"forwardPorts\": [9901], // Use 'postCreateCommand' to run commands after the container is created. // \"postCreateCommand\": \"uname -a\", // Comment out connect as root instead. More info: https://aka.ms/vscode-remote/containers/non-root. \"remoteUser\": \"vscode\" } . Dockerfile . ARG NAPKIN_DOCKER_TAG FROM soostone/napkin-exe:$NAPKIN_DOCKER_TAG ## You can extend your development environment by installing extra packages from ubuntu # RUN apt-get update &amp;&amp; apt-get install -y extra-package ## Or from nixos # RUN nix-env -f \"&lt;nixpkgs&gt;\" -iA extra-package . ",
    "url": "https://docs.napkin.run/user-manual/devcontainer/#configuring-the-devcontainer-in-a-new-project",
    "relUrl": "/user-manual/devcontainer/#configuring-the-devcontainer-in-a-new-project"
  },"88": {
    "doc": "Docker",
    "title": "Docker",
    "content": " ",
    "url": "https://docs.napkin.run/user-manual/docker/#docker",
    "relUrl": "/user-manual/docker/#docker"
  },"89": {
    "doc": "Docker",
    "title": "Basic usage",
    "content": "The idea behind the docker wrapper script is simple: usage patterns should be indistinguishable from regular Napkin usage. It is possible to invoke any Napkin command, for example: . sh &lt;(docker run --rm soostone/napkin-exe cat /bin/napkin-docker) init --project-name example sh &lt;(docker run --rm soostone/napkin-exe cat /bin/napkin-docker) validate --spec-file example/specs/spec.yaml --interactive sh &lt;(docker run --rm soostone/napkin-exe cat /bin/napkin-docker) haddock sh &lt;(docker run --rm soostone/napkin-exe cat /bin/napkin-docker) version . ",
    "url": "https://docs.napkin.run/user-manual/docker/#basic-usage",
    "relUrl": "/user-manual/docker/#basic-usage"
  },"90": {
    "doc": "Docker",
    "title": "Advanced usage",
    "content": "If your Napkin project requires some extra haskell packages, which is usually specified in spec.yaml file: . spec.yaml . haskell_packages: - cassava - cassava-embed . It is still possible to use docker wrapper script like so: . shell . NAPKIN_EXTRA_PACKAGES=\"cassava cassava-embed\" sh &lt;(docker run --rm soostone/napkin-exe cat /bin/napkin-docker) validate . Building custom napkin image with extra cassava cassava-embed packages, it can take a minute... sha256:707aab0cdce3d14929134cad2d8102bb463ef4377cdad3e1c59afa62d5342689 OK . Under the hood, wrapper script will build an extra layer to the base image with extra packages installed, so Napkin can see and use them with the following Nix expression: . nixpkgs.nix . (import https://github.com/NixOS/nixpkgs/archive/$NAPKIN_NIX_REVISION.tar.gz {}) .haskell.packages.$NAPKIN_GHC_VERSION_COMPACT .ghcWithPackages (p: with p; [$NAPKIN_EXTRA_PACKAGES]) . Note, that packages, specified with NAPKIN_EXTRA_PACKAGES variable, are pulled from Nix, so every package should exist in Nix, for example: cassava). ",
    "url": "https://docs.napkin.run/user-manual/docker/#advanced-usage",
    "relUrl": "/user-manual/docker/#advanced-usage"
  },"91": {
    "doc": "Docker",
    "title": "Technical details",
    "content": "Assumptions and internals . Docker wrapper script provides a way to run Napkin inside a container. Being executed inside a container, it needs access to the project files you are working on. Wrapper script makes an assumption, that it will be executed from the project root folder and mounts current folder from the host machine to the /project folder inside a container: --volume $PWD:/project. Since Napkin uses user’s $HOME folder to cache BigQuery credentials in SQLite database, wrapper script also needs to mount .napkin folder from the host to the container: --volume $HOME/.napkin:/home/napkin/.napkin. Occasionally, Napkin wants to open some links in the user’s browser (docs, BigQuery oAuth flow, etc.). Since there is no access to the host’s browser from the running container, wrapper scripts mounts a host-container pipe (temp folder based), sets up own tiny browser wrapper inside a container --volume $open:/tmp/open --volume $inbox:/tmp/inbox, listens to any invocations on the host, and opens host’s browser normally with help of standard open (on Mac) or xdg-open (on Linux) utilities. It is also possible to specify preferred browser with BROWSER environment variable. BigQuery oAuth flow requires program to have a valid HTTP callback (has to be on localhost in case of standalone desktop program). But since Napkin in being run inside a container, wrapper script publishes the port from a container to host machine --publish $oauth_port:9901, which possible to override with corresponding environment variable: oauth_port=\"${NAPKIN_OAUTH_PORT:=9901}\". Commit policy . Wrapper script, extracted out of docker container to the host filesystem, contains exact versions of Napkin, nixpkgs and GHC compiler. napkin-docker . image=\"soostone/napkin-exe:v0.5.7-9db950fa\" NAPKIN_GHC_VERSION_COMPACT=\"ghc8107\" NAPKIN_GHC_VERSION=\"ghc-8.10.7\" . That is precisely why on first use you see such output from docker in your terminal (but on on subsequent invocations): . shell . sh &lt;(docker run --rm soostone/napkin-exe cat /bin/napkin-docker) version . Unable to find image 'soostone/napkin-exe:v0.5.7-9db950fa' locally v0.5.7-9db950fa: Pulling from soostone/napkin-exe Digest: sha256:cccb09930b22216a7ab97f0eef0d4299d4a62a3e44928d23e88085eb25d98159 Status: Downloaded newer image for soostone/napkin-exe:v0.5.7-9db950fa Napkin version: 0.5.7 Git commit hash: 9db950fad2469ed88522976f700c3f7446eb5176 Built at: 2021-12-02 17:35:21.03233944 UTC . Since wrapper script has docker image tag as soostone/napkin-exe:v0.5.7-9db950fa and not just soostone/napkin-exe:latest, it does an extra pull, but quickly realize then images are the same. It is recommended to commit the wrapper script to the project git repository, so that other team members will use it. ",
    "url": "https://docs.napkin.run/user-manual/docker/#technical-details",
    "relUrl": "/user-manual/docker/#technical-details"
  },"92": {
    "doc": "Docker",
    "title": "Docker",
    "content": ". | Docker . | Basic usage | Advanced usage | Technical details . | Assumptions and internals | Commit policy | . | . | . ",
    "url": "https://docs.napkin.run/user-manual/docker/",
    "relUrl": "/user-manual/docker/"
  },"93": {
    "doc": "Running as a scheduled job on GitHub Actions",
    "title": "Running as a scheduled job on GitHub Actions",
    "content": "Docker distribution of Napkin makes it easy to deploy it in unattended environments, such as CI/CD platform. In the examples below, we will execute Napkin Spec on daily basis using GitHub Actions. This approach can be easily adapted by the user for a CI/CD platform of choice. Some Napkin features require the metadata store to be persisted between Spec runs. We recommend configuring a Postgres database as a metadata store. Alternatively, one needs to persist Sqlite metadata store between job runs. ",
    "url": "https://docs.napkin.run/user-manual/github-actions/",
    "relUrl": "/user-manual/github-actions/"
  },"94": {
    "doc": "Running as a scheduled job on GitHub Actions",
    "title": "BigQuery",
    "content": "In order to pass DB credentials to unattended job, you need to generated credentials file as described in Connecting to the database document. Next, you need to add it as NAPKIN_CREDS secret in GitHub repository settings. name: Napkin on: push: branches: - master schedule: # trigger the workflow daily at 3 am # * is a special character in YAML so you have to quote this string - cron: '0 3 * * *' jobs: napkin: runs-on: ubuntu-20.04 container: image: soostone/napkin-exe:v0.5.10 steps: - uses: actions/checkout@v2 - name: Get branch name id: branch-name uses: tj-actions/branch-names@v5 - shell: bash env: NAPKIN_CREDS: ${{ secrets.NAPKIN_CREDS }} run: echo \"$NAPKIN_CREDS\" &gt; creds.json - run: napkin run --log-format Server -C creds.json --arg branch=${{ steps.branch-name.outputs.current_branch }} . ",
    "url": "https://docs.napkin.run/user-manual/github-actions/#bigquery",
    "relUrl": "/user-manual/github-actions/#bigquery"
  },"95": {
    "doc": "Running as a scheduled job on GitHub Actions",
    "title": "Postgres/Redshift",
    "content": "Credentials to the database have to be provided, so Napkin can connect to the database. In this example, we assume that the full connection string is present in the YAML Spec except for the DB password. When you add it as PGPASSWORD secret in GitHub repository settings it will be reexported as PGPASSWORD variable that will be consumed by the Postgres client. name: Napkin on: push: branches: - master schedule: # trigger the workflow daily at 3 am # * is a special character in YAML so you have to quote this string - cron: '0 3 * * *' jobs: napkin: runs-on: ubuntu-20.04 container: image: soostone/napkin-exe:v0.5.10 steps: - uses: actions/checkout@v2 - name: Get branch name id: branch-name uses: tj-actions/branch-names@v5 - shell: bash env: PGPASSWORD: ${{ secrets.PGPASSWORD }} - run: napkin run --log-format Server --arg branch=${{ steps.branch-name.outputs.current_branch }} . ",
    "url": "https://docs.napkin.run/user-manual/github-actions/#postgresredshift",
    "relUrl": "/user-manual/github-actions/#postgresredshift"
  },"96": {
    "doc": "IDE Support",
    "title": "IDE Support",
    "content": "Visual Studio Code is the recommended IDE to develop data processing pipelines with Napkin. Visual Studio Code . However, some configuration is required to get the best experience. In this document, we describe what features can increase productivity and how they can be configured. We recommend installing the following extentions: . | YAML | Advanced meta programming support . | Haskell | . | SQL editing . | Postgres | SQL (BigQuery) | . | Devcontainer support . | Remote - Containers | Docker | . | . YAML Schema . Editing Specs in YAML format can be assisted by IDE with instant validation and autocomplete. Note that schema validation does not replace the napkin validate command which performs a dry-run check and evaluates all referenced programs and queries. Once YAML extension is installed you need to configure Napkin Spec schema. If you have used napkin init the appropriate line has been already added to spec.yaml. Otherwise, you need to add the following line to your spec.yaml. spec.yaml . # yaml-language-server: $schema=https://soostone-napkin-public.s3.us-east-1.amazonaws.com/schema/schema.json . Alternatively, one may configure the schema path in the VSCode configuration file (.vscode/settings.json or in global settings): .vscode/settings.json . \"yaml.schemas\": { \"https://soostone-napkin-public.s3.us-east-1.amazonaws.com/schema/schema.json\": \"spec*.yaml\", } . The above snippets will use Spec schema for the last release of Napkin. In some cases, the use of the latest and greatest schema may not be desired. We also publish tagged schema versions at https://soostone-napkin-public.s3.us-east-1.amazonaws.com/schema/app-version/X.Y.Z/schema.json. Alternatively, one can refer local schema file and check it in the Git repository. The schema file can be either downloaded or generated by the napkin yaml-schema spec-schema.json command. The schema file is also already in Docker image at \"/usr/share/napkin/spec-schema.json. Tasks . We provide a .vscode/tasks.json for the most frequent tasks: validate, validate (interactive), dump, and run commands.vscode/tasks.json . { // See https://go.microsoft.com/fwlink/?LinkId=733558 // for the documentation about the tasks.json format \"version\": \"2.0.0\", \"tasks\": [ { \"label\": \"dump\", \"type\": \"shell\", \"command\": \"napkin dump -o dump\" }, { \"label\": \"validate\", \"type\": \"shell\", \"command\": \"napkin validate\" }, { \"label\": \"validate interactive\", \"type\": \"shell\", \"command\": \"napkin validate --interactive\", \"isBackground\": true }, { \"label\": \"run\", \"type\": \"shell\", \"command\": \"napkin run\" }, ] } . Metaprogramming: Haskell Language Server . Note: this feature is not available if you are using standalone Napkin binary, either Docker or Nix setup is required . Metaprogramming in Haskell can be supported by Haskell Language Server and Haskell extension. Haskell Language Server will make the feedback loop shorter by displaying errors and warnings, as well as presenting information about types in tooltips. It will also speed up Spec development by providing autocompletion. All necessary binaries are included in Docker and Nix distributions of Napkin. However, the hie.yaml file has to be created in the project root directory. If you have used the napkin init command to create your project the necessary file is already there. hie.yaml . cradle: bios: shell: napkin hie-bios --spec-file specs/spec.yaml . ",
    "url": "https://docs.napkin.run/user-manual/ide/",
    "relUrl": "/user-manual/ide/"
  },"97": {
    "doc": "IDE Support",
    "title": "Other Editors",
    "content": "Both YAML and Haskell language servers provide support for numerous editors. YAML schema support can be enabled according to YAML Language Server documentation. Alternatively, Spec YAMLs can be validated with yajsv. For metaprogramming assistance, please refer to Haskell Language Server docs for further information. ",
    "url": "https://docs.napkin.run/user-manual/ide/#other-editors",
    "relUrl": "/user-manual/ide/#other-editors"
  },"98": {
    "doc": "User Manual",
    "title": "User Manual",
    "content": " ",
    "url": "https://docs.napkin.run/user-manual/",
    "relUrl": "/user-manual/"
  },"99": {
    "doc": "Multi-environment pipelines in a team setting",
    "title": "Multi-environment pipelines in a team setting",
    "content": "Napkin provides a variety of ways environments can be segregated. Typically, a production-grade project will have at least two environments: production and development. It may be also desired to separate development environments of multiple developers. Recommended data organization is as follows: . | There should be at least two datasets in a project (e.g. BigQuery) or schemas in a database (e.g. Postgres): production and development. | Raw dataset input tables used by the pipeline can either be shared between environments or separated by the environment as well. In either case, we recommend placing raw inputs into their own dataset/schema for clarity. For example: raw_data or raw_data_development and raw_data_production for cases where they are separated. | . If desirable, tables in the development environment should be prefixed by developer user name to avoid clashes as team members do work simultaneously. Below we present an example Spec snippet: . preprocessors: - table_namespace: value: development override_with_arg: environment - table_prefix: override_with_arg: developer separator: _ . Defaults can be changed by providing --arg environment=production and --arg developer=kate. Production runs should be running with --arg developer= to disable the developer prefix. Note that Napkin will fail if table_prefix arg is not provided. By default table_prefix and table_namespace are applied to all tables that are managed by Napkin. In projects that need to use different input datasets for production and development environments, input namespace can be also specified with renamers by changing scope from managed (default) to unmanaged: . preprocessors: # ... - table_namespace: value: development override_with_arg: input_dataset scope: unmanaged . ",
    "url": "https://docs.napkin.run/user-manual/multi-environment/",
    "relUrl": "/user-manual/multi-environment/"
  },"100": {
    "doc": "Mustache Interpolation",
    "title": "Mustache Interpolation",
    "content": "Mustache templates interpolation is the core feature of Napkin. It is used by Napkin internally (for incremental queries) and can be utilized by users to achieve some variability in the queries, which Napkin will execute on the database. Every SQL file or query Napkin tries to parse is treated as a mustache template, where things like {{{variable}}} will be replaced with the value of the variable. The typical use cases for variable interpolation are: . | query deduplication – reusable query can be stored in a SQL file and used with different variable values to create multiple tables, | customizing queries by passing arguments via CLI (--arg) options. | . Let’s demonstrate the concept with a single example: we want to define a query, which will create artist_of_the_month view. The table should contain only artists with names starting from the first letter of the current month’s name. ",
    "url": "https://docs.napkin.run/user-manual/mustache/#mustache-interpolation",
    "relUrl": "/user-manual/mustache/#mustache-interpolation"
  },"101": {
    "doc": "Mustache Interpolation",
    "title": "Variable substitution",
    "content": "Here we assume, that chinook database is already loaded into data/chinook-sql.db sqlite file. Follow the “Get example dataset” part of the tutorial for the details. We start from defining the idea in the spec.yaml file: . specs/spec.yaml . db_url: sqlite:data/chinook-sql.db backend: Sqlite tables: artist_of_the_month: create_action: sql_query: query: SELECT * FROM Artist WHERE Name LIKE upper('{{{letter}}}') || '%' vars: letter: j # (stands for January) . Running such Napkin spec creates a desired view: . shell . napkin run --spec-file specs/spec.yaml --verbose . output . [2022-01-12 08:10:27][Info] Determining tables for update... [2022-01-12 08:10:27][Debug] Tables in execution plan: artist_of_the_month: Execute / UpdateStrategy UpdateAlways [2022-01-12 08:10:27][Info] Forced tables: none [2022-01-12 08:10:27][Info] Skipped tables: none [2022-01-12 08:10:27][Info] Unmanaged tables that will be used as an input in this run: - Artist [2022-01-12 08:10:27][Info] Managed tables that will be used as an input in this run, but will not be updated: none [2022-01-12 08:10:27][Info] Managed tables that will be updated in this run: - artist_of_the_month [2022-01-12 08:10:27][Info] Estimated runtime: 0s [2022-01-12 08:10:27][Info] Running table hooks (Pre) [2022-01-12 08:10:27][Info] Executing table action. [2022-01-12 08:10:27][Debug] Executing command: DROP VIEW IF EXISTS \"artist_of_the_month\" [2022-01-12 08:10:27][Debug] Command performed in 0.00. [2022-01-12 08:10:27][Debug] Executing command: CREATE VIEW \"artist_of_the_month\" AS SELECT * FROM \"Artist\" AS \"Artist\" WHERE (\"Name\" like (upper('j') || '%')) [2022-01-12 08:10:27][Debug] Command performed in 0.00. [2022-01-12 08:10:27][Info] Table action complete. [2022-01-12 08:10:27][Info] Running table hooks (Post) [2022-01-12 08:10:27][Info] Table processing complete. [2022-01-12 08:10:27][Info] TableSpec \"artist_of_the_month\" server stats: unknown [2022-01-12 08:10:27][Info] Execution completed. Please see below for a summary. [2022-01-12 08:10:27][Info] ---------------------------------------------------------- [2022-01-12 08:10:27][Info] Table \"artist_of_the_month\" ran in 0.00 Server stats: unknown [2022-01-12 08:10:27][Info] Run complete. Total cost: unknown . This feature is useful for cases when you have to define multiple very similar tables. Let’s imagine we need a single view per month. First, we move query to the single sql/artist_per_month.sql file: . sql/artist_per_month.sql . SELECT * FROM Artist WHERE Name LIKE upper('{{{letter}}}') || '%' . And the spec.yaml spec would look like that: . specs/spec.yaml . db_url: sqlite:data/chinook-sql.db sql_folder: ../sql backend: Sqlite tables: artist_for_january: create_action: sql_file: source: artist_per_month.sql vars: { letter: j } artist_for_february: create_action: sql_file: source: artist_per_month.sql vars: { letter: f } artist_for_march: create_action: sql_file: source: artist_per_month.sql vars: { letter: m } . Yes, this is still repetitive table definitions (see Haskell API section to know how to avoid that), but at least the query itself is only defined once. But this is not very interesting, hard-coding a letter whenever next month coming isn’t fun. Lets try to provide a variable name from the outside of the spec.yaml file. First, lets try to remove the variable definition from the spec, leaving it undefined: . specs/spec.yaml . db_url: sqlite:data/chinook-sql.db backend: Sqlite tables: artist_of_the_month: create_action: sql_query: query: SELECT * FROM Artist WHERE Name LIKE upper('{{{letter}}}') || '%' . Napkin tries to protect from users from accidental mistakes, which are possible from following the mustache interpolation semantics (which treats non defined variables as empty). Running validate command with additional --strict-mustache flag does detects a problem with undefined variable: . shell . napkin validate --spec-file specs/spec.yaml --strict-mustache . output . Spec error artist_of_the_month: Could not load inline sql: Failed to process mustache template: Template Engine Error: in `SELECT * FROM Artist WHERE Name LIKE upper('{{{letter}}}'...` with a message: Interpolation warning: Referenced value was not provided, key: letter . Fortunately, there is a way out! Napkin propagates spec arguments into mustache variables under the \"args\". So we can override values by providing command line arguments. Before doing that, we should change spec.yaml definition slightly, pre-pending {{{letter}}} variable with args prefix. specs/spec.yaml . db_url: sqlite:data/chinook-sql.db backend: Sqlite tables: artist_of_the_month: create_action: sql_query: query: SELECT * FROM Artist WHERE Name LIKE upper('{{{args.letter}}}') || '%' . Now, after providing an override from CLI, Napkin creates a correct view: . shell . napkin run --spec-file specs/spec.yaml --verbose --arg letter=f # stands for February . output . [2022-01-12 09:10:23][Info] Determining tables for update... [2022-01-12 09:10:23][Debug] Tables in execution plan: artist_of_the_month: Execute / UpdateStrategy UpdateAlways [2022-01-12 09:10:23][Info] Forced tables: none [2022-01-12 09:10:23][Info] Skipped tables: none [2022-01-12 09:10:23][Info] Unmanaged tables that will be used as an input in this run: - Artist [2022-01-12 09:10:23][Info] Managed tables that will be used as an input in this run, but will not be updated: none [2022-01-12 09:10:23][Info] Managed tables that will be updated in this run: - artist_of_the_month [2022-01-12 09:10:23][Info] Estimated runtime: 0s [2022-01-12 09:10:23][Info] Running table hooks (Pre) [2022-01-12 09:10:23][Info] Executing table action. [2022-01-12 09:10:23][Debug] Executing command: DROP VIEW IF EXISTS \"artist_of_the_month\" [2022-01-12 09:10:23][Debug] Command performed in 0.00. [2022-01-12 09:10:23][Debug] Executing command: CREATE VIEW \"artist_of_the_month\" AS SELECT * FROM \"Artist\" AS \"Artist\" WHERE (\"Name\" like (upper('f') || '%')) [2022-01-12 09:10:23][Debug] Command performed in 0.00. [2022-01-12 09:10:23][Info] Table action complete. [2022-01-12 09:10:23][Info] Running table hooks (Post) [2022-01-12 09:10:23][Info] Table processing complete. [2022-01-12 09:10:23][Info] TableSpec \"artist_of_the_month\" server stats: unknown [2022-01-12 09:10:23][Info] Execution completed. Please see below for a summary. [2022-01-12 09:10:23][Info] ---------------------------------------------------------- [2022-01-12 09:10:23][Info] Table \"artist_of_the_month\" ran in 0.00 Server stats: unknown [2022-01-12 09:10:23][Info] Run complete. Total cost: unknown . ",
    "url": "https://docs.napkin.run/user-manual/mustache/#variable-substitution",
    "relUrl": "/user-manual/mustache/#variable-substitution"
  },"102": {
    "doc": "Mustache Interpolation",
    "title": "Mustache sections",
    "content": "But what if the context of the task has suddenly changed and now we need to filter artists with names, starting with several different letters? Napkin to the rescue :sparkles: . Mustache supports named sections syntax, which behaves differently, depending on the variable value: . | Section is not rendered if condition variable is not defined or ‘false’ or ‘empty list’. | Section is rendered multiple times for ‘non-empty list’, binding list element as a variable set. | Section is rendered once for a non-empty value, binding it as a variable set. | . Lets define a query in spec.yaml, which utilized a section syntax and performs our changed task: . specs/spec.yaml . db_url: sqlite:data/chinook-sql.db backend: Sqlite tables: artist_of_the_month: create_action: sql_query: target_type: view query: | SELECT * FROM Artist WHERE False {{#args.letters}} OR Name LIKE upper('{{{letter}}}') || '%' {{/args.letters}} . With strict mustache evaluation mode, Napkin produces an error when variable mentioned in section name is not defined: . shell . napkin validate --spec-file specs/spec.yaml --strict-mustache . output . Spec error artist_of_the_month: Could not load inline sql: Failed to process mustache template: Template Substitution Error: in `SELECT * FROM Artist WHERE False {{^args.letters}} OR Nam...` with a message: Variable used in section declaration is not defined: letters . Lets specify the values with --arg-json CLI flag: . shell . napkin run --spec-file specs/spec.yaml --arg-json '{\"letters\": [{\"letter\": \"a\"}, {\"letter\": \"b\"}]}' --verbose . output . [2022-01-12 18:16:51][Info] Determining tables for update... [2022-01-12 18:16:51][Debug] Tables in execution plan: artist_of_the_month: Execute / UpdateStrategy UpdateAlways [2022-01-12 18:16:51][Info] Forced tables: none [2022-01-12 18:16:51][Info] Skipped tables: none [2022-01-12 18:16:51][Info] Unmanaged tables that will be used as an input in this run: - Artist [2022-01-12 18:16:51][Info] Managed tables that will be used as an input in this run, but will not be updated: none [2022-01-12 18:16:51][Info] Managed tables that will be updated in this run: - artist_of_the_month [2022-01-12 18:16:51][Info] Estimated runtime: 0s [2022-01-12 18:16:51][Info] Running table hooks (Pre) [2022-01-12 18:16:51][Info] Executing table action. [2022-01-12 18:16:51][Debug] Executing command: DROP VIEW IF EXISTS \"artist_of_the_month\" [2022-01-12 18:16:51][Debug] Command performed in 0.00. [2022-01-12 18:16:51][Debug] Executing command: CREATE VIEW \"artist_of_the_month\" AS SELECT * FROM \"Artist\" AS \"Artist\" WHERE ((false) or ((\"Name\" like (upper('a') || '%')))) or ((\"Name\" like (upper('b') || '%'))) [2022-01-12 18:16:51][Debug] Command performed in 0.00. [2022-01-12 18:16:51][Info] Table action complete. [2022-01-12 18:16:51][Info] Running table hooks (Post) [2022-01-12 18:16:51][Info] Table processing complete. [2022-01-12 18:16:51][Info] TableSpec \"artist_of_the_month\" server stats: unknown [2022-01-12 18:16:51][Info] Execution completed. Please see below for a summary. [2022-01-12 18:16:51][Info] ---------------------------------------------------------- [2022-01-12 18:16:51][Info] Table \"artist_of_the_month\" ran in 0.01 Server stats: unknown [2022-01-12 18:16:51][Info] Run complete. Total cost: unknown . This time Napkin replaces both letters and executed the correct resulting query. But there is a better way to inspect the query without asking Napkin to execute it – the dump command. Let’s change out query one more time, to insert a default behavior (when no letter is specified). We will use the mustache inverse section, which is executed when the expression inside it is ‘false’ or ‘empty’. specs/spec.yaml . db_url: sqlite:data/chinook-sql.db backend: Sqlite tables: artist_of_the_month: create_action: sql_query: target_type: view query: | SELECT * FROM Artist WHERE False {{^args.letters}} OR Name = 'Metallica' {{/args.letters}} {{#args.letters}} OR Name LIKE upper('{{{letter}}}') || '%' {{/args.letters}} . Dumping the query gives us different results depending on provided arguments: . shell . napkin dump --spec-file specs/spec.yaml . output . | Without --arg-json argument . | With --arg-json '{\"letters\": {\"letter\": \"b\"}}' . | . | SELECT * FROM \"Artist\" AS \"Artist\" WHERE (false) or ((\"Name\" = 'Metallica')) . | SELECT * FROM \"Artist\" AS \"Artist\" WHERE (false) or ((\"Name\" like (upper('b') || '%'))) . | . ",
    "url": "https://docs.napkin.run/user-manual/mustache/#mustache-sections",
    "relUrl": "/user-manual/mustache/#mustache-sections"
  },"103": {
    "doc": "Mustache Interpolation",
    "title": "Haskell splicing",
    "content": "Napkin has number of special mustache sections, which are evaluated in non-standard mustache way. In scope of this tutorial, we will cover only two: strExp and selExp, whereas others can be found in haddock documentation. String interpolation . strExp is the simplest one of all – inner contents of the section gets evaluated as Haskell code. The resulting string will be used as a substitution for a section body. Lets say you have a table with such structure: . | Column | Type | . | year | integer | . | q1m1w1 | double precision | . | q1m1w2 | double precision | . | q1m1w3 | double precision | . | q1m1w4 | double precision | . | … | … | . | q4m3w1 | double precision | . | q4m3w2 | double precision | . | q4m3w3 | double precision | . | q4m3w4 | double precision | . The task is to calculate minimum and maximum week sales amount for each quarter. Obvious solution would be to such SQL query: . query.sql . SELECT year, GREATEST(q1m1w1, q1m1w2, q1m1w3, q1m1w4, q1m2w1, q1m2w2, q1m2w3, q1m2w4, q1m3w1, q1m3w2, q1m3w3, q1m3w4) AS q1_greatest, LEAST(q1m1w1, q1m1w2, q1m1w3, q1m1w4, q1m2w1, q1m2w2, q1m2w3, q1m2w4, q1m3w1, q1m3w2, q1m3w3, q1m3w4) AS q1_least, GREATEST(q2m1w1, q2m1w2, q2m1w3, q2m1w4, q2m2w1, q2m2w2, q2m2w3, q2m2w4, q2m3w1, q2m3w2, q2m3w3, q2m3w4) AS q2_greatest, LEAST(q2m1w1, q2m1w2, q2m1w3, q2m1w4, q2m2w1, q2m2w2, q2m2w3, q2m2w4, q2m3w1, q2m3w2, q2m3w3, q2m3w4) AS q2_least, GREATEST(q3m1w1, q3m1w2, q3m1w3, q3m1w4, q3m2w1, q3m2w2, q3m2w3, q3m2w4, q3m3w1, q3m3w2, q3m3w3, q3m3w4) AS q3_greatest, LEAST(q3m1w1, q3m1w2, q3m1w3, q3m1w4, q3m2w1, q3m2w2, q3m2w3, q3m2w4, q3m3w1, q3m3w2, q3m3w3, q3m3w4) AS q3_least, GREATEST(q4m1w1, q4m1w2, q4m1w3, q4m1w4, q4m2w1, q4m2w2, q4m2w3, q4m2w4, q4m3w1, q4m3w2, q4m3w3, q4m3w4) AS q4_greatest, LEAST(q4m1w1, q4m1w2, q4m1w3, q4m1w4, q4m2w1, q4m2w2, q4m2w3, q4m2w4, q4m3w1, q4m3w2, q4m3w3, q4m3w4) AS q4_least FROM sales ORDER BY year ASC . But this is very tedious and error prone to write and modify “by hands”. Instead, we are going to generate part of the query with mustache interpolation: . specs/spec.yaml . tables: result: create_action: sql_query: query: | SELECT {{#quarters}} {{#functions}} {{{function}}}(q{{{q}}}m1w1, q{{{q}}}m1w2, q{{{q}}}m1w3, q{{{q}}}m1w4, q{{{q}}}m2w1, q{{{q}}}m2w2, q{{{q}}}m2w3, q{{{q}}}m2w4, q{{{q}}}m3w1, q{{{q}}}m3w2, q{{{q}}}m3w3, q{{{q}}}m3w4) as q{{{q}}}_{{{function}}}, {{/functions}} {{/quarters}} year FROM sales ORDER BY year ASC vars: functions: - function: greatest - function: least quarters: - q: 1 - q: 2 - q: 3 - q: 4 post_hooks: - assert_expression: expression: (q1_greatest &gt;= q1_least) - assert_expression: expression: (q2_greatest &gt;= q2_least) - assert_expression: expression: (q3_greatest &gt;= q3_least) - assert_expression: expression: (q4_greatest &gt;= q4_least) . As expected, this query, being rendered, looks exactly like we need: . output . SELECT greatest(q1m1w1, q1m1w2, q1m1w3, q1m1w4, q1m2w1, q1m2w2, q1m2w3, q1m2w4, q1m3w1, q1m3w2, q1m3w3, q1m3w4) AS q1_greatest, least(q1m1w1, q1m1w2, q1m1w3, q1m1w4, q1m2w1, q1m2w2, q1m2w3, q1m2w4, q1m3w1, q1m3w2, q1m3w3, q1m3w4) AS q1_least, greatest(q2m1w1, q2m1w2, q2m1w3, q2m1w4, q2m2w1, q2m2w2, q2m2w3, q2m2w4, q2m3w1, q2m3w2, q2m3w3, q2m3w4) AS q2_greatest, least(q2m1w1, q2m1w2, q2m1w3, q2m1w4, q2m2w1, q2m2w2, q2m2w3, q2m2w4, q2m3w1, q2m3w2, q2m3w3, q2m3w4) AS q2_least, greatest(q3m1w1, q3m1w2, q3m1w3, q3m1w4, q3m2w1, q3m2w2, q3m2w3, q3m2w4, q3m3w1, q3m3w2, q3m3w3, q3m3w4) AS q3_greatest, least(q3m1w1, q3m1w2, q3m1w3, q3m1w4, q3m2w1, q3m2w2, q3m2w3, q3m2w4, q3m3w1, q3m3w2, q3m3w3, q3m3w4) AS q3_least, greatest(q4m1w1, q4m1w2, q4m1w3, q4m1w4, q4m2w1, q4m2w2, q4m2w3, q4m2w4, q4m3w1, q4m3w2, q4m3w3, q4m3w4) AS q4_greatest, least(q4m1w1, q4m1w2, q4m1w3, q4m1w4, q4m2w1, q4m2w2, q4m2w3, q4m2w4, q4m3w1, q4m3w2, q4m3w3, q4m3w4) AS q4_least, year FROM sales AS sales ORDER BY year ASC . But, that is still not very pleasant to maintain. It is too easy to make an accidental mistake in the long expression crafted by hands. Haskell to the rescue :sparkles: . We’ll write a Haskell expression, which generates exactly everything for us: . GHCI . :{ intercalate \", \\n\" $ join $ for [1..4] $ \\quarter -&gt; for [\"greatest\", \"least\"] $ \\fun -&gt; let expr = intercalate \", \" $ join $ for [1..3] $ \\month -&gt; for [1..4] $ \\week -&gt; \"q\" &lt;&gt; show quarter &lt;&gt; \"m\" &lt;&gt; show month &lt;&gt; \"w\" &lt;&gt; show week in fun &lt;&gt; \"(\" &lt;&gt; expr &lt;&gt; \") as q\" &lt;&gt; show quarter &lt;&gt; \"_\" &lt;&gt; fun :} . output . greatest(q1m1w1, q1m1w2, q1m1w3, q1m1w4, q1m2w1, q1m2w2, q1m2w3, q1m2w4, q1m3w1, q1m3w2, q1m3w3, q1m3w4) as q1_greatest, least(q1m1w1, q1m1w2, q1m1w3, q1m1w4, q1m2w1, q1m2w2, q1m2w3, q1m2w4, q1m3w1, q1m3w2, q1m3w3, q1m3w4) as q1_least, greatest(q2m1w1, q2m1w2, q2m1w3, q2m1w4, q2m2w1, q2m2w2, q2m2w3, q2m2w4, q2m3w1, q2m3w2, q2m3w3, q2m3w4) as q2_greatest, least(q2m1w1, q2m1w2, q2m1w3, q2m1w4, q2m2w1, q2m2w2, q2m2w3, q2m2w4, q2m3w1, q2m3w2, q2m3w3, q2m3w4) as q2_least, greatest(q3m1w1, q3m1w2, q3m1w3, q3m1w4, q3m2w1, q3m2w2, q3m2w3, q3m2w4, q3m3w1, q3m3w2, q3m3w3, q3m3w4) as q3_greatest, least(q3m1w1, q3m1w2, q3m1w3, q3m1w4, q3m2w1, q3m2w2, q3m2w3, q3m2w4, q3m3w1, q3m3w2, q3m3w3, q3m3w4) as q3_least, greatest(q4m1w1, q4m1w2, q4m1w3, q4m1w4, q4m2w1, q4m2w2, q4m2w3, q4m2w4, q4m3w1, q4m3w2, q4m3w3, q4m3w4) as q4_greatest, least(q4m1w1, q4m1w2, q4m1w3, q4m1w4, q4m2w1, q4m2w2, q4m2w3, q4m2w4, q4m3w1, q4m3w2, q4m3w3, q4m3w4) as q4_least . In order to embed the evaluation result into the SQL query, we would need to use special {{#strExp}}...{{/strExp}} section: . specs/spec.yaml . tables: result: create_action: sql_query: query: | SELECT year, {{#strExp}} intercalate \", \\n\" $ join $ for [1..4] $ \\quarter -&gt; for [\"greatest\", \"least\"] $ \\fun -&gt; let expr = intercalate \", \" $ join $ for [1..3] $ \\month -&gt; for [1..4] $ \\week -&gt; \"q\" &lt;&gt; show quarter &lt;&gt; \"m\" &lt;&gt; show month &lt;&gt; \"w\" &lt;&gt; show week in fun &lt;&gt; \"(\" &lt;&gt; expr &lt;&gt; \") as q\" &lt;&gt; show quarter &lt;&gt; \"_\" &lt;&gt; fun {{/strExp}} FROM sales ORDER BY year ASC post_hooks: - assert_expression: expression: (q1_greatest &gt;= q1_least) - assert_expression: expression: (q2_greatest &gt;= q2_least) - assert_expression: expression: (q3_greatest &gt;= q3_least) - assert_expression: expression: (q4_greatest &gt;= q4_least) . post_hooks is here just to demonstrate the idea of a posteriori data verification: column with greatest value cannot be less that a column with least value. It is very hard to make an accidental mistake in the SQL statement you generate. If, in future, we would need to also calculate the SUM of the sales in each quarter, it would be easy to refactor the Haskell expression as such: . GHCI . :{ intercalate \", \\n\" $ join $ for [1..4] $ \\quarter -&gt; let expr = join $ for [1..3] $ \\month -&gt; for [1..4] $ \\week -&gt; \"q\" &lt;&gt; show quarter &lt;&gt; \"m\" &lt;&gt; show month &lt;&gt; \"w\" &lt;&gt; show week vals = for [\"greatest\", \"least\"] $ \\fun -&gt; fun &lt;&gt; \"(\" &lt;&gt; intercalate \", \" expr &lt;&gt; \") as q\" &lt;&gt; show quarter &lt;&gt; \"_\" &lt;&gt; fun in [intercalate \" + \" expr &lt;&gt; \" as q\" &lt;&gt; show quarter &lt;&gt; \"_sum\"] &lt;&gt; vals :} . output . q1m1w1 + q1m1w2 + q1m1w3 + q1m1w4 + q1m2w1 + q1m2w2 + q1m2w3 + q1m2w4 + q1m3w1 + q1m3w2 + q1m3w3 + q1m3w4 as q1_sum, greatest(q1m1w1, q1m1w2, q1m1w3, q1m1w4, q1m2w1, q1m2w2, q1m2w3, q1m2w4, q1m3w1, q1m3w2, q1m3w3, q1m3w4) as q1_greatest, least(q1m1w1, q1m1w2, q1m1w3, q1m1w4, q1m2w1, q1m2w2, q1m2w3, q1m2w4, q1m3w1, q1m3w2, q1m3w3, q1m3w4) as q1_least, q2m1w1 + q2m1w2 + q2m1w3 + q2m1w4 + q2m2w1 + q2m2w2 + q2m2w3 + q2m2w4 + q2m3w1 + q2m3w2 + q2m3w3 + q2m3w4 as q2_sum, greatest(q2m1w1, q2m1w2, q2m1w3, q2m1w4, q2m2w1, q2m2w2, q2m2w3, q2m2w4, q2m3w1, q2m3w2, q2m3w3, q2m3w4) as q2_greatest, least(q2m1w1, q2m1w2, q2m1w3, q2m1w4, q2m2w1, q2m2w2, q2m2w3, q2m2w4, q2m3w1, q2m3w2, q2m3w3, q2m3w4) as q2_least, q3m1w1 + q3m1w2 + q3m1w3 + q3m1w4 + q3m2w1 + q3m2w2 + q3m2w3 + q3m2w4 + q3m3w1 + q3m3w2 + q3m3w3 + q3m3w4 as q3_sum, greatest(q3m1w1, q3m1w2, q3m1w3, q3m1w4, q3m2w1, q3m2w2, q3m2w3, q3m2w4, q3m3w1, q3m3w2, q3m3w3, q3m3w4) as q3_greatest, least(q3m1w1, q3m1w2, q3m1w3, q3m1w4, q3m2w1, q3m2w2, q3m2w3, q3m2w4, q3m3w1, q3m3w2, q3m3w3, q3m3w4) as q3_least, q4m1w1 + q4m1w2 + q4m1w3 + q4m1w4 + q4m2w1 + q4m2w2 + q4m2w3 + q4m2w4 + q4m3w1 + q4m3w2 + q4m3w3 + q4m3w4 as q4_sum, greatest(q4m1w1, q4m1w2, q4m1w3, q4m1w4, q4m2w1, q4m2w2, q4m2w3, q4m2w4, q4m3w1, q4m3w2, q4m3w3, q4m3w4) as q4_greatest, least(q4m1w1, q4m1w2, q4m1w3, q4m1w4, q4m2w1, q4m2w2, q4m2w3, q4m2w4, q4m3w1, q4m3w2, q4m3w3, q4m3w4) as q4_least . And voila, we have a result we wanted: . | Column | Type | . | **year | integer | . | q1_sum | double precision | . | q1_greatest | double precision | . | q1_least | double precision | . | q2_sum | double precision | . | q2_greatest | double precision | . | q2_least | double precision | . | q3_sum | double precision | . | q3_greatest | double precision | . | q3_least | double precision | . | q4_sum | double precision | . | q4_greatest | double precision | . | q4_least | double precision | . Select clause interpolation . The same result can be achieved with selExp special mustache section, but in more type safe way. Instead of generating a string, we will generate part of the SQL statement in AST form (string conversion is implicit). Resulting query will be slightly different, but semantically remains the same: . specs/spec.yaml . tables: result: create_action: sql_query: query: | SELECT year, {{#selExp}} join $ for [1..4] $ \\quarterNumber -&gt; let quarter = \"q\" &lt;&gt; show quarterNumber lst = [varString $ quarter &lt;&gt; \"m\" &lt;&gt; show month &lt;&gt; \"w\" &lt;&gt; show week | month &lt;- [1..3], week &lt;- [1..4]] in [ greatest lst `as` ref (quarter &lt;&gt; \"_greatest\"), least lst `as` ref (quarter &lt;&gt; \"_least\"), foldl1 (+) lst `as` ref (quarter &lt;&gt; \"_sum\") ] {{/selExp}} FROM sales ORDER BY year ASC post_hooks: - assert_expression: expression: (q1_greatest &gt;= q1_least) - assert_expression: expression: (q2_greatest &gt;= q2_least) - assert_expression: expression: (q3_greatest &gt;= q3_least) - assert_expression: expression: (q4_greatest &gt;= q4_least) . output . SELECT year, greatest(q1m1w1, q1m1w2, q1m1w3, q1m1w4, q1m2w1, q1m2w2, q1m2w3, q1m2w4, q1m3w1, q1m3w2, q1m3w3, q1m3w4) AS q1_greatest, least(q1m1w1, q1m1w2, q1m1w3, q1m1w4, q1m2w1, q1m2w2, q1m2w3, q1m2w4, q1m3w1, q1m3w2, q1m3w3, q1m3w4) AS q1_least, (((((((((((q1m1w1 + q1m1w2) + q1m1w3) + q1m1w4) + q1m2w1) + q1m2w2) + q1m2w3) + q1m2w4) + q1m3w1) + q1m3w2) + q1m3w3) + q1m3w4) AS q1_sum, greatest(q2m1w1, q2m1w2, q2m1w3, q2m1w4, q2m2w1, q2m2w2, q2m2w3, q2m2w4, q2m3w1, q2m3w2, q2m3w3, q2m3w4) AS q2_greatest, least(q2m1w1, q2m1w2, q2m1w3, q2m1w4, q2m2w1, q2m2w2, q2m2w3, q2m2w4, q2m3w1, q2m3w2, q2m3w3, q2m3w4) AS q2_least, (((((((((((q2m1w1 + q2m1w2) + q2m1w3) + q2m1w4) + q2m2w1) + q2m2w2) + q2m2w3) + q2m2w4) + q2m3w1) + q2m3w2) + q2m3w3) + q2m3w4) AS q2_sum, greatest(q3m1w1, q3m1w2, q3m1w3, q3m1w4, q3m2w1, q3m2w2, q3m2w3, q3m2w4, q3m3w1, q3m3w2, q3m3w3, q3m3w4) AS q3_greatest, least(q3m1w1, q3m1w2, q3m1w3, q3m1w4, q3m2w1, q3m2w2, q3m2w3, q3m2w4, q3m3w1, q3m3w2, q3m3w3, q3m3w4) AS q3_least, (((((((((((q3m1w1 + q3m1w2) + q3m1w3) + q3m1w4) + q3m2w1) + q3m2w2) + q3m2w3) + q3m2w4) + q3m3w1) + q3m3w2) + q3m3w3) + q3m3w4) AS q3_sum, greatest(q4m1w1, q4m1w2, q4m1w3, q4m1w4, q4m2w1, q4m2w2, q4m2w3, q4m2w4, q4m3w1, q4m3w2, q4m3w3, q4m3w4) AS q4_greatest, least(q4m1w1, q4m1w2, q4m1w3, q4m1w4, q4m2w1, q4m2w2, q4m2w3, q4m2w4, q4m3w1, q4m3w2, q4m3w3, q4m3w4) AS q4_least, (((((((((((q4m1w1 + q4m1w2) + q4m1w3) + q4m1w4) + q4m2w1) + q4m2w2) + q4m2w3) + q4m2w4) + q4m3w1) + q4m3w2) + q4m3w3) + q4m3w4) AS q4_sum FROM sales AS sales ORDER BY year ASC . ",
    "url": "https://docs.napkin.run/user-manual/mustache/#haskell-splicing",
    "relUrl": "/user-manual/mustache/#haskell-splicing"
  },"104": {
    "doc": "Mustache Interpolation",
    "title": "Mustache Interpolation",
    "content": ". | Mustache Interpolation . | Variable substitution | Mustache sections | Haskell splicing . | String interpolation | Select clause interpolation | . | . | . ",
    "url": "https://docs.napkin.run/user-manual/mustache/",
    "relUrl": "/user-manual/mustache/"
  },"105": {
    "doc": "Preprocessors",
    "title": "Preprocessors",
    "content": "Preprocessors allow to systematically apply certain modifications to the Spec. Napkin has two preprocessors that facilitate table renaming built-in. Additionally, the user may implement custom preprocessors when needed. Preprocessors are configured in the preprocessors section of the YAML file. Multiple preprocessors can be used in a single spec, they will be applied in sequence. The syntax can be summarized as follows: . spec.yaml . preprocessors: - builtin_a: # built-in preprocessor with arguments param_foo: foo param_bar: bar - builtin_b # built-in preprocessor without arguments - MyPreprocessors.custom_a: # custom preprocessor with arguments param_foo: foo param_bar: bar - MyPreprocessors.custom_b # custom preprocessor without arguments . ",
    "url": "https://docs.napkin.run/user-manual/preprocessors/#preprocessors",
    "relUrl": "/user-manual/preprocessors/#preprocessors"
  },"106": {
    "doc": "Preprocessors",
    "title": "Built-in preprocessors",
    "content": "table_prefix . This preprocessor adds a prefix to table references. The tables are renamed when created by Napkin and all the queries are updated accordingly. By default, it affects only managed tables but can be also applied to unmanaged tables as well. Arguments: . | value – prefix, optional. | override_with_arg – indicates the name of spec argument that allows overriding prefix with --arg CLI option, optional. | separator – the preprocessor will insert an optional separator between prefix and table name if the prefix is not empty. | scope – indicates the tables that should be prefixed, optional, can be one of: . | managed (default) | unmanaged | all | . | only – allows applying the renamer to selected tables only, optional. | except – allows applying the renamer to all but selected tables, optional. | . Example: Segregating environments . Please refer to our Multi-environment pipelines in a team setting tutorial for recommended setup. Prefix managed tables with environment . preprocessors: - table_prefix: value: development override_with_arg: environment separator: _ scope: managed . Napkin will rename all managed tables and prefix them with the environment name. The default environment is development. For staging and production runs, one needs to provide --arg environment=staging or --arg environment=production to adjust the prefix accordingly. Example: Isolating development environments . Prefix managed tables with developer name . preprocessors: - table_prefix: override_with_arg: developer # the preprocessor will fail if the developer name has not been provided explicitly when executing the spec separator: _ scope: managed . Napkin will rename all managed tables and prefix them with the developer name. There is no default developer name, so the preprocessor will fail and prevent Spec from running. One needs to explicitly pass --arg developer=john for each Spec run. Note, that if an empty developer argument will be provided (--arg developer=) the table names will have the original name with no extra _. table_namespace . This preprocessor sets the namespace (BigQuery dataset or Postgres schema) to all table references. The table namespaces are renamed when created by Napkin and all the queries are updated accordingly. By default, it affects only managed tables but can be also applied to unmanaged tables as well. Arguments: . | value – prefix, optional. | override_with_arg – indicates the name of spec argument that allows overriding prefix with --arg CLI option, optional. | scope – indicates the tables that should be prefixed, optional, can be one of: . | managed (default) | unmanaged | all | . | only – allows applying the renamer to selected tables only, optional. | except – allows applying the renamer to all but selected tables, optional. | on_exists – by default, this preprocessor will overwrite any schema that was explicitly provided in the Spec or any SQL query. This behavior can be changed to keep existing schema if any and set it only when missing, can be one of: . | overwrite (default) | keep_original | . | . Example: Segregating environments . Isolate environments in namespaces . preprocessors: - table_namespace: value: development override_with_arg: environment . Napkin will move all managed tables to the environment namespace. The default environment is development. For staging and production runs, one needs to provide --arg environment=staging or --arg environment=production to adjust the prefix accordingly. Note that the BigQuery dataset or Postgres schema has to exist. Example: Segregating output tables by the audience . Isolate environments in namespaces . preprocessors: - table_namespace: value: data_science_internal # will be overwritten if necessary - table_namespace: value: marketing only: - conversion - customer_churn - table_namespace: value: warehouse only: - overstock . In some cases, we need to segregate tables by the expected audience and possibly limit access to the data. In the above example, we put all managed tables to data_science_internal. We only store selected tables in separate namespaces. Database engine ACL can be used to configure access rules as needed. ",
    "url": "https://docs.napkin.run/user-manual/preprocessors/#built-in-preprocessors",
    "relUrl": "/user-manual/preprocessors/#built-in-preprocessors"
  },"107": {
    "doc": "Preprocessors",
    "title": "Custom preprocessors",
    "content": "Coming soon :zzz: . ",
    "url": "https://docs.napkin.run/user-manual/preprocessors/#custom-preprocessors",
    "relUrl": "/user-manual/preprocessors/#custom-preprocessors"
  },"108": {
    "doc": "Preprocessors",
    "title": "Preprocessors",
    "content": ". | Preprocessors . | Built-in preprocessors . | table_prefix . | Example: Segregating environments | Example: Isolating development environments | . | table_namespace . | Example: Segregating environments | Example: Segregating output tables by the audience | . | . | Custom preprocessors | . | . ",
    "url": "https://docs.napkin.run/user-manual/preprocessors/",
    "relUrl": "/user-manual/preprocessors/"
  },"109": {
    "doc": "Spec arguments",
    "title": "Spec arguments",
    "content": "Coming soon :zzz: . ",
    "url": "https://docs.napkin.run/user-manual/spec-arguments/",
    "relUrl": "/user-manual/spec-arguments/"
  },"110": {
    "doc": "Spec YAML reference",
    "title": "Spec YAML reference",
    "content": "# Enable YAML schema in IDE # yaml-language-server: $schema=http://napkin-public-storage-bucket.s3-website.us-east-1.amazonaws.com/schema/schema.json # We don't allow extra fields except for top-level fields that start with _. # They may be useful when YAML anchors are used for code reuse. _commonOptions: &amp;common some_common: options ## Application name app_name: foo bar ## Metadata configuration # Napkin maintains a database with the history of runs. It's used to estimate pipeline runtime as well as to enforce update strategies. metadata_url: sqlite:metadata.db # optional, by default metadata will be stored in the Sqlite file in $PWD/data directory. # metadata_url: postgresql://db@host/napkin_metadata # metadata can be also stored in Postgres #### Database connection configuration ## BigQuery backend: BigQuery db_url: bigquery://bigquery.googleapis.com/project-acme?dataset=acme-analytics backend_options: # backend specific options labels: # can be used to segregate costs in Google Cloud billing dashboard client: acme repo: client-acme concurrent_queries: 100 timeout: 300 ## Postgres # backend: Postgres # db_url: postgresql://user@db/napkin_db # backend_options: # backend specific options # connection_pool: 100 ## Sqlite # backend: Sqlite # db_url: sqlite:acme.db # backend_options: {} # no backend options for Sqlite ## Location of SQL files when referenced in Spec. sql_folder: ../sql # optional, this is a default value ## SQL dialect to use when parsing queries parser_dialect: napkin.bigquery # napkin.* family of dialects will parse SQL then convert to Napkin intermediate representation. # This enables Napkin optimization features, however some SQL features are not available. # Queries parsed that way will be rendered accordingly to db backend syntax – this can be used to convert queries (e.g. parse as BigQuery, run with Postgres). # parser_dialect: napkin.bigquery # parser_dialect: napkin.postgres # parser_dialect: napkin.sqlite # parser_dialect: napkin.ansi2011 # generic.* family of dialects will parse SQL, but will not convert into Napkin intermediate representation. # This provides is provides more compatibility with SQL features. # Queries parsed that way will be rendered accordingly to db backend dialect – this can be used to convert queries (e.g. parse as BigQuery, run with Postgres). # parser_dialect: generic.bigquery # parser_dialect: generic.postgres # parser_dialect: generic.sqlite # parser_dialect: generic.ansi2011 # Use parser based on Postgres SQL grammar. # Postgres-specific features (such as -&gt;&gt; JSON operator) can be used with in this mode. # Queries parsed that way will be always rendered according to Postgres dialect. # parser_dialect: postgres # Consider queries as a plain text. Use this only if other fail. # WARNING: dependency discovery as well as table renaming will not work with raw queries # parser_dialect: raw ## Location of Haskell files when referenced in Spec. haskell_folder: ../haskell # optional, this is a default value ## Meta-programming &amp; scripting configuration ## List of Haskell modules that should be imported (in a qualified way) into the Mustache templating environment. # You don't need to list modules used for table specs, hooks, preprocessors, etc. here. haskell_modules: # optional, defaults to empty - ACME.Mustache # Haskell module ## Haskell function that provides function macros that can extend SQL. function_macros: ACME.macros # optional, Haskell symbol, defaults to none ## Custom validator validator: ACME.validator # optional, Haskell symbol, defaults to none ## Haskell spec # In addition to specifying tables in YAML, tables can be also specified in the Haskell program with Napkin DSL. haskell_spec: ACME.spec # optional, Haskell symbol, defaults to none ## Default extensions for Haskell interpreter haskell_default_language_extensions: - OverlappingInstances - NoOverloadedStrings # extensions enabled by default can be disabled as well ## List of extra packages to be made available in the Haskell scripting # Note: these extensions need to be present in a package DB. Please consult the user manual for directions on how to add extra packages. haskell_packages: # optional, defaults to empty - fmt - yaml ## Spec preprocessors preprocessors: # defaults to empty # Prefix all tables (according to configuration) without hardcoding the prefix directly in the Spec and queries - table_prefix: # one or both of value and override_with_arg need to be specified value: development # prefix override_with_arg: environment # allow to override with CLI, e.g. --arg environment=production separator: _ # defaults to empty, # scope: unmanaged # scope: all # only: [foo, bar, baz] # apply renamer only to selected tables # except: [foo, bar, baz] # apply renamer to all tables except selected tables # Move all tables (according to configuration) to dataset/schema without specifying it directly in the Spec and queries - table_namespace: # one or both of value and override_with_arg need to be specified value: development # namespace override_with_arg: environment # allow to override with CLI, e.g. --arg environment=production scope: managed # scope: unmanaged # scope: all only: [foo, bar, baz] # apply renamer only to selected tables except: [foo, bar, baz] # apply renamer to all tables except selected tables on_existing: overwrite # default, replace namespace even if it was specified explicitly # on_existing: keep_original # keep original namespace if it has been specified explicitly in the Spec # Custom preprocessor with some arguments (Haskell symbol) - ACME.Preprocessors.example: some: arguments # Custom preprocessor without arguments (Haskell symbol) - ACME.Preprocessors.example ## Tables that are managed by Napkin tables: some_table: # Table name # Defines a method the table will be created with create_action: # This table should be created with a SQL query stored in a file sql_file: source: some_query.sql # SQL file location relative to sql_dir target_type: table: # default # Backend specific table options, optional partitioning: # Big Query options column: partitioning_column range: start: 1 end: 100 step: 2 write_disposition: append clustering: [clustering_column] # target_type: view # create view instead of table vars: # Variables for Mustache templating, defaults to none foo: bar baz: boo # What should trigger table being updated or recreated, optional, defaults to: always update_strategy: - type: always # Update table always - type: periodically # Update table periodically (every hour = 3600 seconds). Requires metadata to be persisted between Napkin runs period: 3600 - type: with_dependency # Update table when it's older than its dependency or the dependency will update as well during this run - type: if_missing # Update table when it does not exist # List of tables that are not referenced in the query, but should be considered as a dependency when scheduling, optional, defaults to empty # Typically, Napkin will discover all dependencies automatically, so it does not have to be used. You can double-check dependencies discovered with dump command deps: - some_dependency # List of tables that are referenced in the query and should not be considered as a dependency when scheduling, optional, defaults to empty # Typically, Napkin will discover all dependencies automatically, so it does not have to be used. You can double-check dependencies discovered with dump command hidden_deps: - ignored_dependency # Parser dialect to use for parsing queries for that table. See comments on the global attribute. parser_dialect: napkin.bigquery # Table tags, can be used to selectively enable or disable table execution with CLI flags during development tags: [foo, bar, baz] ## Pre-hooks can be used to validate input data # See examples below what assertions are built-in pre_hooks: # optional, defaults to empty - assert_unique: table: source_table columns: [some_column] ## Post-hooks can be used to validate query results # The same set of assertions can be used both in pre and post hooks # See examples below what assertions are built-in post_hooks: # optional, defaults to empty - assert_unique: table: some_table columns: [result_column] # on_failure controls how the hook failures are handled, this attribute applies to all hooks on_failure: fail_now # default, fail the whole spec as soon hook fails # on_failure: fail_later # fail the whole spec, but allows remaining hooks for this table to be executed # on_failure: warn_only # don't fail the whole spec, failure will be logged in a report inline_table: create_action: # This table should be created with a SQL query inlined in the YAML file sql_query: query: SELECT * FROM some_table incremental_by_time_table: create_action: # This table should be updated with incremental by time strategy. Please refer to the user manual for details incremental_by_time: source: incremental_query.sql # SQL file location relative to sql_dir timestamp_column: column_foo # Column that contains timestamp start_day: 2021-01-01 # Starting date, used when table is created from scratch lookback_days: 14 # Last N days slice to be updated during subsequent runts vars: # Variables for Mustache templating, this program will implicitly provide \"cutoff\" variable foo: bar baz: boo incremental_pk_table: create_action: # This table should be updated with incremental by time strategy. Please refer to the user manual for details incremental_by_pk: source: incremental_pk.sql # SQL file location relative to sql_dir pk: [column_foo] start_day: 2021-01-01 lookback_days: 14 custom_prog: # This table should be created by a custom program # since we don't provide any arguments to the program, we can use simplified syntax create_action: ACME.custom_prog custom_prog_with_args: create_action: # This table should be created by a custom program # In this case we pass some arguments to the program ACME.custom_prog: some_args: true all_hooks: create_action: sql_file: source: some.sql # Example of all hooks available, can be used as pre_hooks as well post_hooks: - assert_all_values_within: table: Invoice column: InvoiceDate type: date from: 2009-01-01 to: 2013-12-31 # on_failure controls how the hook failures are handled, this attribute applies to all hooks on_failure: fail_now # default, fail the whole spec as soon hook fails # on_failure: fail_later # fail the whole spec, but allows remaining hooks for this table to be executed # on_failure: warn_only # don't fail the whole spec, failure will be logged in a report - assert_any_values_within: table: Artist column: Count type: int from: 1 to: 25 - assert_no_values_within: table: popular_tracks_for_pink_floyd column: Name values: - Sweet Home Alabama - Brain Damage - assert_all_values_within: table: popular_tracks_for_pink_floyd column: Name values: - Time - The Great Gig In The Sky - Any Colour You Like - Brain Damage - Sweet Home Alabama - assert_unique: table: popular_tracks_for_pink_floyd columns: [Name] - assert_count: table: popular_tracks_for_pink_floyd equal: 123 - assert_count: table: popular_tracks_for_pink_floyd greater_than_or_equal: popular_tracks_for_pink_floyd - assert_count: table: popular_tracks_for_pink_floyd approximately_equal: 123 type: absolute # type: relative tolerance: 10 # we can be off by 10 and it's still ok - assert_cardinalities: table: popular_tracks_for_pink_floyd column: name equal: table: Artist column: name - assert_expression: table: popular_tracks_for_pink_floyd expression: name - assert_not_null: table: popular_tracks_for_pink_floyd columns: - some_column - expression: NULLIF (some_column, '') # custom hook without arguments - ACME.custom_hook # custom hook with arguments - ACME.custom_hook: target: artists_hashes . ",
    "url": "https://docs.napkin.run/user-manual/yaml-reference/",
    "relUrl": "/user-manual/yaml-reference/"
  }
}
