{"0": {
    "doc": "Getting Started",
    "title": "Getting Started",
    "content": "# Getting Started ## Installation The **Napkin** app is available in 2 forms: a [native installer](#native-installer) and a [dockerized version](#docker-version). ### Native installer The Napkin native installer is a self extracting and self contained SHELL archive that does not require a docker or nix environment. It is located in this [repository](https://soostone-napkin-public.s3.us-east-1.amazonaws.com/index.html){:target=\"_blank\" rel=\"noopener\"} and has distributions for the Mac OS Big Sur and Linux operating systems. To use the native installer: 1) Find the distribution corresponding to your operating system 2) Copy the installation snippet 3) Execute it locally As an example, this is the command to install Napkin on MacOS Big Sur: ```sh curl -s https://soostone-napkin-public.s3.us-east-1.amazonaws.com/x86_64-MacOS-20.5.0/last-build/napkin-native-installer.sh \\ | bash -s -- ``` {: .copiable } The installation bundle is large as it is written in Haskell, a compiled coding language and contains all compiled dependencies. If you need access to the compiler (Glasgow Haskell Compiler `ghci`), simply supply the file derivations for the app you are interested in: ```sh curl -s https://soostone-napkin-public.s3.us-east-1.amazonaws.com/x86_64-MacOS-20.5.0/last-build/napkin-native-installer.sh \\ | bash -s -- -e '*-ghc-*' ``` {: .copiable } The ghc derivation contains version **ghci-8.10.4**. Paths to Napkin are imported through the `$PATH` variable. When installing Napkin, remember to use a new shell session so you get the updated `$PATH` variable. You can also use the `exec $SHELL` command. #### Uninstallation The Napkin native installer contains an uninstaller as well. Type `uninstall-` and you can use the 'tab' key to auto-complete the uninstallation script. ```sh $ uninstall- $ uninstall-b82gj7f0igpw23sapafxifsrr0f52771-napkin-0.3.8.sh ``` ### Docker version The native installer is not available on [Windows](https://en.wikipedia.org/wiki/Microsoft_Windows) yet. There are tutorials on the Internet for setting up [Nix](https://nixos.wiki/wiki/Nix) with [WSL](https://en.wikipedia.org/wiki/Windows_Subsystem_for_Linux) on Windows. ## Key Features Napkin has a robust set of capabilities, with more to come! See our roadmap for more information and to let us know which feature(s) you would like to see added to Napkin the most! One key feature to highlight here is Napkin's capability to detect unused [CTE](https://en.wikipedia.org/wiki/Hierarchical_and_recursive_queries_in_SQL#Common_table_expression) columns. While some table columns are introduced in the SQL pipeline for debugging and development purposes, these can get stale or be forgotten about over time. And more importantly, unused fields are still contributing to pipeline cost and time-to-run. This feature is available via Napkin's Command Line Interface (CLI). For reference, to see the list of available Napkin commands, type `--help`. ```sh $ napkin --help ``` {: .copiable } ``` Usage: napkin [-v|--verbose] COMMAND Cli tool to work with Napkin ... find-unused-cte-columns Parse SQL file and find unused CTE columns ... ``` Detailed help for `find-unused-cte-columns` command: ```sh napkin find-unused-cte-columns --help ``` {: .copiable } ``` Usage: napkin find-unused-cte-columns (-b|--backend ARG) (-f|--sql-file ARG) Parse SQL file and find unused CTE columns Available options: -b,--backend ARG choose kind of SQL - BigQueryBackend or PostgresqlBackend -f,--sql-file ARG path to sql file ``` Let's work through an example. First, let's check the following query for unused columns in an intermediate CTE table within a .sql file: {% include codeCaption.html label=\"query.sql\" %} ```sql WITH CTE AS (SELECT f, g FROM DbTable) SELECT f FROM CTE ``` {: .copiable } {% include codeCaption.html label=\"shell\" %} ```sh napkin find-unused-cte-columns -b PostgresqlBackend -f query.sql {\"CTE\":[\"g\"]} ``` {: .copiable } To deal with a set of SQL files - generate a spec file, sort of a project file: {% include codeCaption.html label=\"shell\" %} ```sh cp query.sql query2.sql mkdir sql mv query.sql query2.sql sql napkin generate-spec -d sql # Generated spec file at: specs/spec.yaml cat specs/spec.yaml | grep source # source: query.sql source: query2.sql mv sql specs # Tiny workaround - might not be needed in the future napkin optimize ``` {: .copiable } ``` Table: [query] query, CTE \"CTE\" has unused columns [\"g\"] Table: [query2] query2, CTE \"CTE\" has unused columns [\"g\"] ``` ## [Continue to the Tutorial section...](/tutorial) ",
    "url": "https://soostone.github.io/getting-started/",
    "relUrl": "/getting-started/"
  },"1": {
    "doc": "Google Site Search",
    "title": "Google Site Search",
    "content": "# Google Search {% include googleSearch.html %} ",
    "url": "https://soostone.github.io/google-search/",
    "relUrl": "/google-search/"
  },"2": {
    "doc": "Haddock",
    "title": "Haddock",
    "content": "# Haddock ## Docs for versions * [Version 0.3.8]({{site.haddock_url}}/app-version/index.html) ## Docs for branches * [last build]({{site.haddock_url}}/last-build/index.html) * [master]({{site.haddock_url}}/branch/master/index.html) * [Full list of branches with haddoc]({{site.haddock_url}}/branch/index.html) ",
    "url": "https://soostone.github.io/haddock/",
    "relUrl": "/haddock/"
  },"3": {
    "doc": "About",
    "title": "About",
    "content": "# Napkin ## Intro Napkin Wiki is a collection of HOWTOs and tutorials about the tool assisting data scientists when they crunch data through their relational and NoSQL DBs. ## About Napkin app Napkin is an command line interface application and also a library Haskell library. Napkin goal is to make data scientist daily routine easier, so there are many parallel features bundled together: 1. A SQL wrapper in Haskell geared towards Redshift and therefore Postgres. * The general idea is to be able to express any hand-written SQL, no matter how complex, directly in Haskell via types and a structure that mirror SQL directly. * The Untyped folder coupled with Napkin.Types has all the types and operations to represent SQL expresions - i.e. the columns in a SQL query. * The Render folder is all about (pretty)printing queries into their final SQL form. * The Untyped.Monad module provides a simple monadic interface to constructing queries. 2. A SQL runner with some basic support for running queries on Postgres and Redshift. 3. A Spec orchestrator that allows for expression of chains of table/view creations, updates and re-creations with interdependencies that are automatically managed. * Entire SQL based computation pipelines can be expressed this way and have their periodic updates managed automatically by the framework. * Specs are necessarily stateful; we currently persist metadata in a local sqlite3 database, which can be migrated to a more serious database solution. 4. A command line interface that can take Specs and provide convenient flags for forcing certain table updates, skipping others, etc. ## [Continue with **Getting Started**](/getting-started) ",
    "url": "https://soostone.github.io/",
    "relUrl": "/"
  },"4": {
    "doc": "Tips and Tricks",
    "title": "Tips and Tricks",
    "content": "1. TOC {:toc} # Introduction The purpose of this page is to give capture a selection of napkin usage examples that occur very frequently in the day-to-day development and maintenance of a data pipeline. # napkin run ## Running only a single table in a spec It's very common when iteratively working on a given table to update it repeatedly and in isolation. Disable all tables, force-update any table with word \"uplift\" in it: ``` napkin run -s specs/myspec1.yaml -D -f '.*(uplift).*' ``` Notice the pattern is a proper Regular Expression. # napkin validate ## Continuosly validating codebase on every change Keeping a validating screen open is invaluable in rapid iteration. Napkin will notice every change on every file that's touched by a given spec and automatically re-validate the entire project. This will catch obvious structural errors in SQL files, mistakes in templates and any compilation errors in custom Haskell code. ``` napkin validate -s specs/myspec1.yaml --live ``` ",
    "url": "https://soostone.github.io/tips-and-tricks/",
    "relUrl": "/tips-and-tricks/"
  },"5": {
    "doc": "Tutorial",
    "title": "Tutorial",
    "content": "1. TOC {:toc} # Tutorial ## Interacting with Napkin - The Spec Napkin is a data pipeline automation tool, designed to execute a series of .sql queries where the resultant data is utilized to create downstream tables. The interface to specify what .sql files to execute is called a Napkin **Spec**. A Spec is defined via a configuration language called [YAML](https://en.wikipedia.org/wiki/YAML). This follows a common paradigm where source tables are never mutated. Instead, Napkin can be repeatably run to create a series of dependent tables as new data comes in to one or many source tables. The sql files / tables in the Napkin Spec comprise an implicit [DAG](https://en.wikipedia.org/wiki/Directed_acyclic_graph), where edges are references to fields from other tables. A common use case is as follows - Source tables are created or updated manually via a data extract or automatically by a piece of software - .SQL files are written to mutate the data into the desired shape that is fit for use in new, downstream tables (never modifying the source data) - A Napkin Spec is created to execute these .SQL files automatically, repeatedly and in the correct dependency order - As new data comes in, the Napkin Spec can be manually or automatically run to recreate the dependent tables with the new data for ongoing use ## Sales DB demo ### Starting out - Creating your Napkin Spec file While you can write out a Napkin Spec file in .yaml yourself, Napkin also has built-in functionality to set this up for you and can help automatically generate the Spec file. All Napkin needs to do this is at least one SQL file. Let's consider a sales DB with the following tables: ### Schema and Input Data First, we'll need to create a source dataset. As a reminder, our SQL pipeline will read from these tables to create the downstream tables but will never mutate the data directly in the source tables. {% include codeCaption.html label=\"input-schema.sql\" %} ```sql CREATE TABLE product ( id INT PRIMARY KEY, price INT NOT NULL, name TEXT NOT NULL); CREATE TABLE sale( id INT PRIMARY KEY, quantity INT NOT NULL, product_id INT NOT NULL); ``` {: .copiable } {% include codeCaption.html label=\"shell\" %} ```sh $ psql -f input-schema.sql salesDb ``` Then we'll put some data into them. {% include codeCaption.html label=\"input-data.sql\" %} ```sql INSERT INTO product VALUES (1, 2, 'chocolate bar'), (2, 3, 'coke'), (3, 50, 'kale'); INSERT INTO sale VALUES (1, 1000, 1), (2, 2, 3), (3, 300, 2), (4, 10, 3); ``` {: .copiable } {% include codeCaption.html label=\"shell\" %} ```sh psql -f input-data.sql salesDb ``` {: .copiable } ### Setting up The Pipeline Place the .sql files that will create your dependent tables in the /sql sub-folder so the Napkin Spec can see and execute them. Here are a couple examples of queries that can run against the source tables we just created. {% include codeCaption.html label=\"sql/best-seller.sql\" %} ```sql SELECT product_id, sum(quantity) FROM sale GROUP BY product_id ORDER BY sum(quantity) DESC; ``` {: .copiable } {% include codeCaption.html label=\"sql/best-revenue.sql\" %} ```sql SELECT product_id, sum(p.price * s.quantity) FROM sale s INNER JOIN product p ON (s.product_id = p.id) GROUP BY product_id ORDER BY 2 DESC; ``` {: .copiable } Next, let's have Napkin automatically generate the Spec file to execute the queries above. Note that Napkin can figure out the dependency relationship of the SQL files and execute them in the correct order. {% include codeCaption.html label=\"shell\" %} ```sql napkin generate-spec -d sql -o spec.yaml mkdir specs mv sql spec.yaml specs ``` {: .copiable } This is what the head of the spec.yaml might look like: {% include codeCaption.html label=\"spec.yaml\" %} ```yaml sql_folder: sql db_url: postgres:/// haskell_packages: [] backend: Postgres haskell: null tables: ``` For `db_url`, this is an example for configuring Napkin to run against a local PostgreSQL instance: ```yaml db_url: postgresql:/user:123@127.0.0.1/salesDb ``` ``` To avoid keepking passwords in a common configuration file, you can provide the database password through Napkin's `--connectionURL` command line option. See `--help` for more details. ``` {: .info} Once you have the Spec, your source tables and the .sql files to execute, you're all set! The next step is to tell Napkin to execute the pipieline you've created. This is done through the 'run' command as shown below. {% include codeCaption.html label=\"shell\" %} ```sh $ napkin run --spec-file specs/spec.yaml ``` {: .copiable } ``` NOTICE: table \"best-revenue\" does not exist, skipping NOTICE: table \"best-seller\" does not exist, skipping [2021-08-20 17:25:14] Table \"best-revenue\" ran in 0.03s: 3 rows affected [2021-08-20 17:25:14] Table \"best-seller\" ran in 0.04s: 3 rows affected [2021-08-20 17:25:14] Run complete. Total cost: 6 rows affected ``` Once the pipeline has run, you can check the database for the new tables (`best-seller`and `best-revenue`): {% include codeCaption.html label=\"psql salesDb\" %} ```sql SELECT * FROM \"best-seller\"; ``` {: .copiable } ``` product_id | sum ------------+------ 1 | 1000 2 | 300 3 | 12 ``` Now you're all set! You have a working SQL pipeline and as new data comes in to your source tables you can repeatably give napkin the 'run' command to execute the pipeline and recreate the target tables. As an example, let's modify our input data here and rerun the pipeline. {% include codeCaption.html label=\"psql salesDb\" %} ```sql INSERT INTO sale VALUES (5, 999, 3); ``` {: .copiable } {% include codeCaption.html label=\"shell\" %} ```sh napkin run --spec-file specs/spec.yaml ``` {: .copiable } {% include codeCaption.html label=\"psql salesDb\" %} ```sql SELECT * FROM \"best-seller\"; ``` {: .copiable } ``` product_id | sum ------------+------ 3 | 1011 1 | 1000 2 | 300 ``` As expected, the updated target tables reflect the change in source data and the `best-seller` table is updated with the new row with the value 'kale' in it on top. ## Using Templates and Variable Interpolation The next step in mastering Napkin is to utilize templates to parameterize your queries. This is an extremely important feature because it can be used for everything from handling different table or variable names in development vs production datasets or simply reusing the same query for multiple purposes. Template variables can be set in a Spec file or can be overridden globally with command line options. Template variables hold text to be used in substitution for the variable name in a template query. The final query should always be a valid SQL expression. Let's work through an example. There are 2 versions of STDDEV functions in Postgres. Let's compare them by computing the results in dedicated tables, but reusing a single parameterized query. {% include codeCaption.html label=\"sql/stdev.mtpl\" %} {% raw %} ```sql SELECT p.name, {{stddev}}(s.{{column_name}}) FROM {{table_name}} s INNER JOIN product p ON (p.id = s.product_id) GROUP BY p.name; ``` {% endraw %} {: .copiable } Append the following config to spec.yaml: {% include codeCaption.html label=\"spec.yaml\" %} ```yaml - post_hooks: [] pre_hooks: [] update_strategy: - type: always create_action: strategy: default hidden_deps: [] deps: [] source: stddev.sql type: sql_file vars: {\"column_name\": \"quantity\", \"table_name\": \"sale\", \"stddev\": \"stddev_pop\"} target_type: table target: sale_stddev_pop_quantity tags: [] - post_hooks: [] pre_hooks: [] update_strategy: - type: always create_action: strategy: default hidden_deps: [] deps: [] source: stddev.sql type: sql_file vars: {\"column_name\": \"quantity\", \"table_name\": \"sale\", \"stddev\": \"stddev_samp\"} target_type: table target: sale_stddev_sam_quantity tags: [] ``` {: .copiable } {% include codeCaption.html label=\"shell\" %} ```sh napkin run --spec-file specs/spec.yaml ``` {: .copiable } Compare the results: {% include codeCaption.html label=\"psql salesDb\" %} ```sql SELECT s.name, stddev_pop, stddev_samp FROM sale_stddev_pop_quantity p, sale_stddev_sam_quantity s WHERE s.name = p.name; ``` {: .copiable } ``` name | stddev_pop | stddev_samp ---------------+------------------+------------------ coke | 0 | kale | 468.116082469580 | 573.322771220540 chocolate bar | 0 | ``` Here we can see that Napkin was able to run the same sql file but in taking the configuration from the spec.yaml file, actually executed two different valid queries with different results placed into different target tables. ## Chain of queries In the previous example, we ran our join query manually. Let's update our Spec so that it's automatically executed. Append the following config to your spec.yaml: {% include codeCaption.html label=\"spec.yaml\" %} ```yaml - post_hooks: [] pre_hooks: [] update_strategy: - type: always create_action: strategy: default hidden_deps: [] deps: [] source: compare-sale-stddev-quantity.sql type: sql_file vars: {} target_type: table target: compare_sale_stddev_quantity tags: [] ``` Let's truncate our intermediate tables so we're 100% confident that the resultant dataset in the target tables was created with the pipelien run we're about to do. {% include codeCaption.html label=\"psql salesDb\" %} ```sql truncate sale_stddev_pop_quantity; truncate sale_stddev_sam_quantity; ``` {: .copiable } {% include codeCaption.html label=\"shell\" %} ```sh napkin run --spec-file specs/spec.yaml ``` {: .copiable } {% include codeCaption.html label=\"psql salesDb\" %} ```sql SELECT * FROM compare_sale_stddev_quantity; ``` As we can see, the target table correctly shows the desired result from our parameterized queries: ``` name | stddev_pop | stddev_samp ---------------+------------------+------------------ coke | 0 | kale | 468.116082469580 | 573.322771220540 chocolate bar | 0 | ``` ## SQL macro functions ## Backends Napkin supports following backends: * [Postgres](https://www.postgresql.org/) * [BigQuery](https://cloud.google.com/bigquery) * [Redshift](https://aws.amazon.com/redshift) These are the databases we currently support. Let us know if your database of choice is not here and maybe we can add it to a future release! ## BigQuery OAuth authentication. ## Embedded Haskell Lot's of programs have embedded scripting langauge for better flexibility and automating complex business logic, recall - [VBA](https://en.wikipedia.org/wiki/Visual_Basic_for_Applications), [Emacs Lisp](https://en.wikipedia.org/wiki/Emacs_Lisp), [MaxScript](https://en.wikipedia.org/wiki/Autodesk_3ds_Max), etc and Napkin is on the same line with them and Napkin Api is available through [Haskell](https://en.wikipedia.org/wiki/Haskell_(programming_language)). ### Evaluation Haskell through **eval** Napkin eval command can interpret a set of free form Haskell modules - just specify top function you are intereseted in. Module with a lazy string function: ```haskell module Hello where import Prelude (String, (++), Char) pureStr :: String pureStr = \"Hello World!!!\" ``` ```sh $ napkin eval -f pureStr -m Hello -r . -i Pure str Hello World!!! ``` See help for calling IO action. Access to plain Haskell interpeter is a test environment for code to be used in spec for describing tables. ## Misc ### Unused column detection Some table columns are introduced for debugging and development purpose, but as time goes an engineer can stop using them and completely forget, meanwhile such columns contribute into cost of running queries. Let's check following query for unused columns in an intermediate CTE table: {% include codeCaption.html label=\"sql/query.sql\" %} ```sql with CTE as (select f, g from DbTable) select f from CTE ``` {: .copiable } {% include codeCaption.html label=\"shell\" %} ```sh napkin optimize ``` {: .copiable } ``` Table: [query] query, CTE \"CTE\" has unused columns [\"g\"] ``` ",
    "url": "https://soostone.github.io/tutorial/",
    "relUrl": "/tutorial/"
  },"6": {
    "doc": "User Manual",
    "title": "User Manual",
    "content": "# User Manual ",
    "url": "https://soostone.github.io/user-manual/",
    "relUrl": "/user-manual/"
  }
}
