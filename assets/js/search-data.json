{"0": {
    "doc": "Docker",
    "title": "Basic usage",
    "content": "The idea behind the docker wrapper script is simple: usage patterns should be indistinguishable from regular Napkin usage. It is possible to invoke any Napkin command, for example: . sh &lt;(docker run --rm soostone/napkin-exe cat /bin/napkin-docker) init -p USER:example -c sh &lt;(docker run --rm soostone/napkin-exe cat /bin/napkin-docker) validate --spec-file specs/spec.yaml --live sh &lt;(docker run --rm soostone/napkin-exe cat /bin/napkin-docker) haddock sh &lt;(docker run --rm soostone/napkin-exe cat /bin/napkin-docker) version . ",
    "url": "https://soostone.github.io/docker/#basic-usage",
    "relUrl": "/docker/#basic-usage"
  },"1": {
    "doc": "Docker",
    "title": "Advanced usage",
    "content": "If your Napkin project requires some extra haskell packages, which is usually specified in spec.yaml file: . haskell_packages: - cassava - cassava-embed . It is still possible to use docker wrapper script like so: . NAPKIN_EXTRA_PACKAGES=\"cassava cassava-embed\" sh &lt;(docker run --rm soostone/napkin-exe cat /bin/napkin-docker) validate . Building custom napkin image with extra cassava cassava-embed packages, it can take a minute... sha256:707aab0cdce3d14929134cad2d8102bb463ef4377cdad3e1c59afa62d5342689 OK . Under the hood, wrapper script will build an extra layer to the base image with extra packages installed, so Napkin can see and use them with the following Nix expression: . import ( builtins.fetchTarball { url = \"https://github.com/NixOS/nixpkgs/archive/$NAPKIN_NIX_REVISION.tar.gz\"; sha256 = \"$NAPKIN_NIX_SHA256\"; }) {} ).haskell.packages.$NAPKIN_GHC_VERSION_COMPACT.ghcWithPackages (p: with p; [$NAPKIN_EXTRA_PACKAGES] ) . Note, that packages, specified with NAPKIN_EXTRA_PACKAGES variable, are pulled from Nix, so every package should exist in Nix, for example: cassava). ",
    "url": "https://soostone.github.io/docker/#advanced-usage",
    "relUrl": "/docker/#advanced-usage"
  },"2": {
    "doc": "Docker",
    "title": "Technical details",
    "content": " ",
    "url": "https://soostone.github.io/docker/#technical-details",
    "relUrl": "/docker/#technical-details"
  },"3": {
    "doc": "Docker",
    "title": "Assumptions and internals",
    "content": "Docker wrapper script provides a way to run Napkin inside a container. Being executed inside a container, it needs access to the project files you are working on. Wrapper script makes an assumption, that it will be executed from the project root folder and mounts current folder from the host machine to the /project folder inside a container: --volume $PWD:/project. Since Napkin uses user’s $HOME folder to cache BigQuery credentials in SQLite database, wrapper script also needs to mount .napkin folder from the host to the container: --volume $HOME/.napkin:/home/napkin/.napkin. Occasionally, Napkin wants to open some links in the user’s browser (docs, BigQuery oAuth flow, etc.). Since there is no access to the host’s browser from the running container, wrapper scripts mounts a host-container pipe (temp folder based), sets up own tiny browser wrapper inside a container --volume $open:/tmp/open --volume $inbox:/tmp/inbox, listens to any invocations on the host, and opens host’s browser normally with help of standard open (on Mac) or xdg-open (on Linux) utilities. It is also possible to specify preferred browser with BROWSER environment variable. BigQuery oAuth flow requires program to have a valid HTTP callback (has to be on localhost in case of standalone desktop program). But since Napkin in being run inside a container, wrapper script publishes the port from a container to host machine --publish $oauth_port:9901, which possible to override with corresponding environment variable: oauth_port=\"${NAPKIN_OAUTH_PORT:=9901}\". ",
    "url": "https://soostone.github.io/docker/#assumptions-and-internals",
    "relUrl": "/docker/#assumptions-and-internals"
  },"4": {
    "doc": "Docker",
    "title": "Commit policy",
    "content": "Wrapper script, extracted out of docker container to the host filesystem, contains exact versions of Napkin, nixpkgs and GHC compiler. image=\"soostone/napkin-exe:0.5.7-9db950fa\" NAPKIN_NIX_REVISION=\"85ec7e87563663c66fefc17bd76bc8b9036bd99c\" NAPKIN_NIX_SHA256=\"0w11dfym7s3blj3ygrv4fqf7rz292agknm5dmrl8nvdw4qxgbm9r\" NAPKIN_GHC_VERSION_COMPACT=\"ghc8107\" NAPKIN_GHC_VERSION=\"ghc-8.10.7\" . That is precisely why on first use you see such output from docker in your terminal (but on on subsequent invocations): . sh &lt;(docker run --rm soostone/napkin-exe cat /bin/napkin-docker) version . nable to find image 'soostone/napkin-exe:0.5.7-9db950fa' locally 0.5.7-9db950fa: Pulling from soostone/napkin-exe Digest: sha256:cccb09930b22216a7ab97f0eef0d4299d4a62a3e44928d23e88085eb25d98159 Status: Downloaded newer image for soostone/napkin-exe:0.5.7-9db950fa Napkin version: 0.5.7 Git commit hash: 9db950fad2469ed88522976f700c3f7446eb5176 Built at: 2021-12-02 17:35:21.03233944 UTC . Since wrapper script has docker image tag as soostone/napkin-exe:0.5.7-9db950fa and not just soostone/napkin-exe:latest, it does an extra pull, but quickly realize then images are the same. Is is recommended to commit the wrapper script to the project git repository, so that other team members will use it. ",
    "url": "https://soostone.github.io/docker/#commit-policy",
    "relUrl": "/docker/#commit-policy"
  },"5": {
    "doc": "Docker",
    "title": "Docker",
    "content": " ",
    "url": "https://soostone.github.io/docker/",
    "relUrl": "/docker/"
  },"6": {
    "doc": "Getting Started",
    "title": "Getting Started",
    "content": " ",
    "url": "https://soostone.github.io/getting-started/",
    "relUrl": "/getting-started/"
  },"7": {
    "doc": "Getting Started",
    "title": "Installation",
    "content": "The Napkin app is available in 3 variants: a native installer, docker version and a NIX derivation. Native installer . The Napkin native installer is a self extracting and self contained SHELL archive that does not require a docker or nix environment. It is located in this repository and has distributions for the Mac OS Big Sur and Linux operating systems. To use the native installer: 1) Find the distribution corresponding to your operating system 2) Copy the installation snippet 3) Execute it locally As an example, this is the command to install Napkin on MacOS Big Sur: . curl -s https://soostone-napkin-public.s3.us-east-1.amazonaws.com/x86_64-MacOS-20.5.0/last-build/napkin-native-installer.sh \\ | bash -s -- . The installation bundle is large as it is written in Haskell, a compiled coding language and contains all compiled dependencies. If you need access to the compiler (Glasgow Haskell Compiler ghci), simply supply the file derivations for the app you are interested in: . curl -s https://soostone-napkin-public.s3.us-east-1.amazonaws.com/x86_64-MacOS-20.5.0/last-build/napkin-native-installer.sh \\ | bash -s -- -e '*-ghc-*' . The ghc derivation contains version ghci-8.10.4. Paths to Napkin are imported through the $PATH variable. When installing Napkin, remember to use a new shell session so you get the updated $PATH variable. You can also use the exec $SHELL command. Uninstallation . The Napkin native installer contains an uninstaller as well. Type uninstall- and you can use the ‘tab’ key to auto-complete the uninstallation script. $ uninstall- $ uninstall-b82gj7f0igpw23sapafxifsrr0f52771-napkin-0.3.8.sh . ",
    "url": "https://soostone.github.io/getting-started/#installation",
    "relUrl": "/getting-started/#installation"
  },"8": {
    "doc": "Getting Started",
    "title": "Docker version",
    "content": "Extract a tiny bash script: . docker run --rm soostone/napkin-exe cat /bin/napkin-docker &gt; ./napkin-docker chmod +x ./napkin-docker . Extracted ./napkin-docker script can be uses as a Napkin binary replacement: ./napkin-docker version . Napkin version: 0.5.7 Git commit hash: 7fc76cfd3664be7cfacfb61546f6ae39f86e4420 Built at: 2021-12-01 21:25:37.218782171 UTC . Alternatively, you can skip the extraction step and use Napkin directly: . sh &lt;(docker run --rm soostone/napkin-exe cat /bin/napkin-docker) version . Napkin version: 0.5.7 Git commit hash: 7fc76cfd3664be7cfacfb61546f6ae39f86e4420 Built at: 2021-12-01 21:25:37.218782171 UTC . If you need to use different version of Napkin, docker has the following naming theme: . | soostone/napkin-exe:latest - Released latest version | soostone/napkin-exe:$VERSION-latest - Napkin from $VERSION version (for example, 0.5.6) | . So this command: . sh &lt;(docker run --rm soostone/napkin-exe:0.5.6-latest cat /bin/napkin-docker) version . Napkin version: 0.5.6 Git commit hash: 6406eeb5fed9dee2e2bd475647fa44283ae67551 Built at: 2021-12-01 13:20:04.213727885 UTC . Since docker image contains the wrapper script itself, update procedure is trivial, just pull the newer image and extract the script: . docker run --pull=always --rm soostone/napkin-exe cat /bin/napkin-docker &gt; ./napkin-docker chmod +x ./napkin-docker . Docker manual contains all technical details about using Napkin from docker container. ",
    "url": "https://soostone.github.io/getting-started/#docker-version",
    "relUrl": "/getting-started/#docker-version"
  },"9": {
    "doc": "Getting Started",
    "title": "NIX version",
    "content": "Coming soon! . ",
    "url": "https://soostone.github.io/getting-started/#nix-version",
    "relUrl": "/getting-started/#nix-version"
  },"10": {
    "doc": "Getting Started",
    "title": "Key Features",
    "content": "Napkin has a robust set of capabilities, with more to come! See our roadmap for more information and to let us know which feature(s) you would like to see added to Napkin the most! . One key feature to highlight here is Napkin’s capability to detect unused CTE columns. While some table columns are introduced in the SQL pipeline for debugging and development purposes, these can get stale or be forgotten about over time. And more importantly, unused fields are still contributing to pipeline cost and time-to-run. This feature is available via Napkin’s Command Line Interface (CLI). For reference, to see the list of available Napkin commands, type --help. $ napkin --help . Usage: napkin [-v|--verbose] COMMAND Cli tool to work with Napkin ... find-unused-cte-columns Parse SQL file and find unused CTE columns ... Detailed help for find-unused-cte-columns command: . napkin find-unused-cte-columns --help . Usage: napkin find-unused-cte-columns (-b|--backend ARG) (-f|--sql-file ARG) Parse SQL file and find unused CTE columns Available options: -b,--backend ARG choose kind of SQL - BigQueryBackend or PostgresqlBackend -f,--sql-file ARG path to sql file . Let’s work through an example. First, let’s check the following query for unused columns in an intermediate CTE table within a .sql file: . query.sql . WITH CTE AS (SELECT f, g FROM DbTable) SELECT f FROM CTE . shell . napkin find-unused-cte-columns -b PostgresqlBackend -f query.sql {\"CTE\":[\"g\"]} . To deal with a set of SQL files - generate a spec file, sort of a project file: . shell . cp query.sql query2.sql mkdir sql mv query.sql query2.sql sql napkin generate-spec -d sql # Generated spec file at: specs/spec.yaml cat specs/spec.yaml | grep source # source: query.sql source: query2.sql mv sql specs # Tiny workaround - might not be needed in the future napkin optimize . Table: [query] query, CTE \"CTE\" has unused columns [\"g\"] Table: [query2] query2, CTE \"CTE\" has unused columns [\"g\"] . ",
    "url": "https://soostone.github.io/getting-started/#key-features",
    "relUrl": "/getting-started/#key-features"
  },"11": {
    "doc": "Getting Started",
    "title": "Continue to the Tutorial section…",
    "content": " ",
    "url": "https://soostone.github.io/getting-started/#continue-to-the-tutorial-section",
    "relUrl": "/getting-started/#continue-to-the-tutorial-section"
  },"12": {
    "doc": "Haddock",
    "title": "Haddock",
    "content": " ",
    "url": "https://soostone.github.io/haddock/",
    "relUrl": "/haddock/"
  },"13": {
    "doc": "Haddock",
    "title": "Docs for versions",
    "content": ". | Version 0.3.8 | . ",
    "url": "https://soostone.github.io/haddock/#docs-for-versions",
    "relUrl": "/haddock/#docs-for-versions"
  },"14": {
    "doc": "Haddock",
    "title": "Docs for branches",
    "content": ". | last build | master | Full list of branches with haddock | . ",
    "url": "https://soostone.github.io/haddock/#docs-for-branches",
    "relUrl": "/haddock/#docs-for-branches"
  },"15": {
    "doc": "About",
    "title": "What is Napkin",
    "content": "Napkin is a command line application that executes data pipelines of all sizes, backed by a feature-rich Haskell library offering programmatic freedom. It’s lightweight, offers a quick start for new projects and yet scales to massive data pipelines with powerful meta-programming possibilities. Napkin has a broad vision in making life easier for data scientists and engineers, encapsulating a large portion of the data engineering landscape. It therefore bundles several key features together: . | A consumer-grade Command Line Interface (CLI) that acts as the single point of entry for all typical workflows of data engineering and pipeline curation. The napkin app can refresh entire data pipelines, re-create individual tables, validate/typecheck pipelines in seconds, export dependency graphs and more. | A multi-backend (w.g. BigQuery and Redshift) database runtime environment that provides for all key capabilities in executing a modern data pipeline, including interacting the database (see what’s there, query tables, create/recreate/update tables, etc.), performing runtime unit-tests/assertions, logging, timing and interacting with the outside world. | A built-in DAG orchestrator that can automatically detect all the dependency relationships in a data pipeline (e.g. 30+ tables) and perform the pipeline updates in the correct order. Data pipelines are called “Spec”s in napkin and ship with all batteries included: Ability to rewrite table destinations into different schemas/datasets for different environments (e.g. devel vs. prod), mass-prefixing/renaming tables, setting different “Refresh Strategies” for each table (e.g. update daily vs. only update when missing), a wide range of data unit-tests (e.g. table must be unique by columns X+Y) that are automatically performed each time the table is updated. | For the power user, a SQL wrapper DSL in Haskell that stays as close as possible to SQL, without any intermediary object or relational mappings. This DSL looks almost like regular SQL, but allows sophisticated programmatic manipulation and composition of SQL queries and statements. Napkin can parse regular SQL into this internal DSL, perform any desired manipulations and render it back out as regular SQL. | A sophisticated SQL meta-programming environment that accelerates modern data engineering efforts. Napkin users can interweave several options for crafting SQL as they see fit, even in the same file. These options include: . | . | Writing plain SQL files without any low-grade templating noise. Napkin will still auto-detect all dependencies and make the pipeline “just work”. | Using lightweight variable substitutions in .sql files via Mustache templates. | Using sophisticated #{sexp} ... #{/sexp} splices directly in .sql files to write Haskell code that dynamically generates SQL fragments on the fly. | Expressing entire queries directly using napkin’s Haskell DSL, often used for dynamic generation of SQL code based on complex logic. For example, prediction trees can be rendered into SQL this way, sometimes generated 100K LOC SQL files from a single model. | . ",
    "url": "https://soostone.github.io/#what-is-napkin",
    "relUrl": "/#what-is-napkin"
  },"16": {
    "doc": "About",
    "title": "Napkin’s Philosophy",
    "content": "Napkin was created to capitalize on an opportunity we noticed back in 2015 to (massively) accelerate our team’s data engineering capabilities and yet make the resulting code-bases way more sustainable/maintainable. At the time, we were drowning in the complexity of custom Hadoop MapReduce programs, Spark programs and repositories of ad-hoc SQL scripts targeted on Redshift/Hive/etc at the time. We created napkin because we sorely needed something more practical and reliable for our own work. Over time, the opportunities we saw got crystallized into a set of philosophies we can articulate about what napkin is trying to achieve and whether it may be the huge catalyst for your team that it has been for us: . ",
    "url": "https://soostone.github.io/#napkins-philosophy",
    "relUrl": "/#napkins-philosophy"
  },"17": {
    "doc": "About",
    "title": "Base as much data compute as possible on SQL",
    "content": "Despite its age and missed opportunities, SQL code is declarative, functional and highly expressive. It’s easy to construct even for non-engineer data scientists/analysts and tends to offer good “equational reasoning”. It’s constrained just the right amount that business logic does not go “off the hook” like it can in typical programming languages like Python, R, Scala, etc. Once written and tested, SQL tends to produce reliable results. Over the years, we have found almost all data engineering efforts outside of SQL to be error-prone, hard to grow and expensive (e.g. needs data engineers) to maintain over time. If you can imagine how a table should be structured and express that table as a query in SQL, you can use napkin to engineer a pipeline. ",
    "url": "https://soostone.github.io/#base-as-much-data-compute-as-possible-on-sql",
    "relUrl": "/#base-as-much-data-compute-as-possible-on-sql"
  },"18": {
    "doc": "About",
    "title": "Do as much compute as possible on modern analytics DBs like BigQuery/Redshift/Snowflake",
    "content": "Napkin aims to be a data engineering superpower even for very small teams. This is accomplished in large part by leaning on the amazing compute capabilities of modern analytics databases like BigQuery. Napkin’s creation goes back to our realization that if we could express even a very complex computation in SQL on these databases, no matter how convoluted, they would get the work done in astonishingly little time for minimal cost. In our work, we have produced numerous 200,000+ LOC SQL queries using napkin’s meta-programming capabilities that run within minutes on databases like Amazon Redshift and Google’s BigQuery. Fun fact: BigQuery has a ~1M character limit on queries, which we sometimes bypass by breaking complex queries into parts and joining them up / unioning them later. Even this transformation can be done automatically for you by napkin in certain cases! . ",
    "url": "https://soostone.github.io/#do-as-much-compute-as-possible-on-modern-analytics-dbs-like-bigqueryredshiftsnowflake",
    "relUrl": "/#do-as-much-compute-as-possible-on-modern-analytics-dbs-like-bigqueryredshiftsnowflake"
  },"19": {
    "doc": "About",
    "title": "Abstract and reuse complex transformations where possible",
    "content": " ",
    "url": "https://soostone.github.io/#abstract-and-reuse-complex-transformations-where-possible",
    "relUrl": "/#abstract-and-reuse-complex-transformations-where-possible"
  },"20": {
    "doc": "About",
    "title": "Data pipelines should be declarative and managed on Git",
    "content": " ",
    "url": "https://soostone.github.io/#data-pipelines-should-be-declarative-and-managed-on-git",
    "relUrl": "/#data-pipelines-should-be-declarative-and-managed-on-git"
  },"21": {
    "doc": "About",
    "title": "Data pipelines should be regenerative",
    "content": " ",
    "url": "https://soostone.github.io/#data-pipelines-should-be-regenerative",
    "relUrl": "/#data-pipelines-should-be-regenerative"
  },"22": {
    "doc": "About",
    "title": "Data pipeline dev should be lightweight on bare laptops",
    "content": " ",
    "url": "https://soostone.github.io/#data-pipeline-dev-should-be-lightweight-on-bare-laptops",
    "relUrl": "/#data-pipeline-dev-should-be-lightweight-on-bare-laptops"
  },"23": {
    "doc": "About",
    "title": "Doctrine of extreme convenience",
    "content": "With napkin, we aim to make various data engineering and data science workflows so easy to perform that practitioners change their behavior to lean on them more frequently. We believe that speed and convenience without sacrificing correctness and reliability makes a huge difference in sustaining data ecosystem effectiveness. ",
    "url": "https://soostone.github.io/#doctrine-of-extreme-convenience",
    "relUrl": "/#doctrine-of-extreme-convenience"
  },"24": {
    "doc": "About",
    "title": "Napkin’s Benefits",
    "content": "Here’s our best description of benefits you can expect after you’ve gotten a hang of napkin: . | You’ll be able to see and manage your entire data pipeline in a simple codebase, in declarative fashion and in source control - just like any modern software project. | You’ll always be able to “blow away and fully refresh” your entire pipeline from raw data at the push of a button - recovering from mistakes will be a breeze. | Your data pipeline will entirely rely on the power of your backend database, whatever it may be. The likes of BigQuery for large datasets or Postgres (or even Sqlite) when you can get away with it on small data. You won’t rely on error prone Python pandas code, your own custom data processing application and similar constructs that are hard to grow/maintain and ensure correctness over time. | Your data will have actual unit tests that will confirm correctness with each update. (Example: Making sure you don’t double count sales) . | You’ll benefit from tens of combinators we ship with napkin, such as incrementally updating large tables, column-to-row transformations, union-in same-structured tables into one, etc. As we improve napkin, you’ll get all that for free. | You’ll be able to implement your own clever SQL meta-programming to express logic that’d be too tedious to do in plain SQL. Yet the result will still have all the benefits of declarative SQL running on modern analytics databases, instead of your custom Python/R/Scala scripting machine. You’ll be able to create your own mini programs that produce 10-table “purchasing funnel” computations that connect just the right way based on configuration parameters supplied. | . ",
    "url": "https://soostone.github.io/#napkins-benefits",
    "relUrl": "/#napkins-benefits"
  },"25": {
    "doc": "About",
    "title": "Napkin’s Future",
    "content": "Napkin is utilized heavily in commercial projects both at Soostone and at our clients. We improve napkin all the time and have a long backlog of major features we will realize in the future. We would like to be transparent with our roadmap and are looking for ways to best communicate our plans. We’re currently maintaining a Trello board with our roadmap where we would love to hear your reactions and feedback. You can access our roadmap board at Napkin Roadmap . ",
    "url": "https://soostone.github.io/#napkins-future",
    "relUrl": "/#napkins-future"
  },"26": {
    "doc": "About",
    "title": "Next Steps",
    "content": "Continue with Getting Started . ",
    "url": "https://soostone.github.io/#next-steps",
    "relUrl": "/#next-steps"
  },"27": {
    "doc": "About",
    "title": "About",
    "content": " ",
    "url": "https://soostone.github.io/",
    "relUrl": "/"
  },"28": {
    "doc": "Tips and Tricks",
    "title": "Introduction",
    "content": "The purpose of this page is to give capture a selection of napkin usage examples that occur very frequently in the day-to-day development and maintenance of a data pipeline. ",
    "url": "https://soostone.github.io/tips-and-tricks/#introduction",
    "relUrl": "/tips-and-tricks/#introduction"
  },"29": {
    "doc": "Tips and Tricks",
    "title": "napkin run",
    "content": " ",
    "url": "https://soostone.github.io/tips-and-tricks/#napkin-run",
    "relUrl": "/tips-and-tricks/#napkin-run"
  },"30": {
    "doc": "Tips and Tricks",
    "title": "Running only a single table in a spec",
    "content": "It’s very common when iteratively working on a given table to update it repeatedly and in isolation. Disable all tables, force-update any table with word “uplift” in it: . napkin run -s specs/myspec1.yaml -D -f '.*(uplift).*' . Notice the pattern is a proper Regular Expression. ",
    "url": "https://soostone.github.io/tips-and-tricks/#running-only-a-single-table-in-a-spec",
    "relUrl": "/tips-and-tricks/#running-only-a-single-table-in-a-spec"
  },"31": {
    "doc": "Tips and Tricks",
    "title": "napkin validate",
    "content": " ",
    "url": "https://soostone.github.io/tips-and-tricks/#napkin-validate",
    "relUrl": "/tips-and-tricks/#napkin-validate"
  },"32": {
    "doc": "Tips and Tricks",
    "title": "Continuously validating codebase on every change",
    "content": "Keeping a validating screen open is invaluable in rapid iteration. Napkin will notice every change on every file that’s touched by a given spec and automatically re-validate the entire project. This will catch obvious structural errors in SQL files, mistakes in templates and any compilation errors in custom Haskell code. napkin validate -s specs/myspec1.yaml --live . ",
    "url": "https://soostone.github.io/tips-and-tricks/#continuously-validating-codebase-on-every-change",
    "relUrl": "/tips-and-tricks/#continuously-validating-codebase-on-every-change"
  },"33": {
    "doc": "Tips and Tricks",
    "title": "Tips and Tricks",
    "content": ". | Introduction | napkin run . | Running only a single table in a spec | . | napkin validate . | Continuously validating codebase on every change | . | . ",
    "url": "https://soostone.github.io/tips-and-tricks/",
    "relUrl": "/tips-and-tricks/"
  },"34": {
    "doc": "Tutorial",
    "title": "Tutorial",
    "content": " ",
    "url": "https://soostone.github.io/tutorial/#tutorial",
    "relUrl": "/tutorial/#tutorial"
  },"35": {
    "doc": "Tutorial",
    "title": "Interacting with Napkin - The Spec",
    "content": "Napkin is a data pipeline automation tool, designed to execute a series of .sql queries where the resultant data is utilized to create downstream tables. The interface to specify what .sql files to execute is called a Napkin Spec. A Spec is defined via a configuration language called YAML. This follows a common paradigm where source tables are never mutated. Instead, Napkin can be repeatably run to create a series of dependent tables as new data comes in to one or many source tables. The sql files / tables in the Napkin Spec comprise an implicit DAG, where edges are references to fields from other tables. A common use case is as follows . | Source tables are created or updated manually via a data extract or automatically by a piece of software | .SQL files are written to mutate the data into the desired shape that is fit for use in new, downstream tables (never modifying the source data) | A Napkin Spec is created to execute these .SQL files automatically, repeatedly and in the correct dependency order | As new data comes in, the Napkin Spec can be manually or automatically run to recreate the dependent tables with the new data for ongoing use | . ",
    "url": "https://soostone.github.io/tutorial/#interacting-with-napkin---the-spec",
    "relUrl": "/tutorial/#interacting-with-napkin---the-spec"
  },"36": {
    "doc": "Tutorial",
    "title": "Sales DB demo",
    "content": "Starting out - Creating your Napkin Spec file . While you can write out a Napkin Spec file in .yaml yourself, Napkin also has built-in functionality to set this up for you and can help automatically generate the Spec file. All Napkin needs to do this is at least one SQL file. Let’s consider a sales DB with the following tables: . Schema and Input Data . First, we’ll need to create a source dataset. As a reminder, our SQL pipeline will read from these tables to create the downstream tables but will never mutate the data directly in the source tables. input-schema.sql . CREATE TABLE product ( id INT PRIMARY KEY, price INT NOT NULL, name TEXT NOT NULL); CREATE TABLE sale( id INT PRIMARY KEY, quantity INT NOT NULL, product_id INT NOT NULL); . shell . $ psql -f input-schema.sql salesDb . Then we’ll put some data into them. input-data.sql . INSERT INTO product VALUES (1, 2, 'chocolate bar'), (2, 3, 'coke'), (3, 50, 'kale'); INSERT INTO sale VALUES (1, 1000, 1), (2, 2, 3), (3, 300, 2), (4, 10, 3); . shell . psql -f input-data.sql salesDb . Setting up The Pipeline . Place the .sql files that will create your dependent tables in the /sql sub-folder so the Napkin Spec can see and execute them. Here are a couple examples of queries that can run against the source tables we just created. sql/best-seller.sql . SELECT product_id, sum(quantity) FROM sale GROUP BY product_id ORDER BY sum(quantity) DESC; . sql/best-revenue.sql . SELECT product_id, sum(p.price * s.quantity) FROM sale s INNER JOIN product p ON (s.product_id = p.id) GROUP BY product_id ORDER BY 2 DESC; . Next, let’s have Napkin automatically generate the Spec file to execute the queries above. Note that Napkin can figure out the dependency relationship of the SQL files and execute them in the correct order. shell . napkin generate-spec -d sql -o spec.yaml mkdir specs mv sql spec.yaml specs . This is what the head of the spec.yaml might look like: . spec.yaml . sql_folder: sql db_url: postgres:/// haskell_packages: [] backend: Postgres haskell: null tables: . For db_url, this is an example for configuring Napkin to run against a local PostgreSQL instance: . db_url: postgresql:/user:123@127.0.0.1/salesDb . To avoid keepking passwords in a common configuration file, you can provide the database password through Napkin's `--connectionURL` command line option. See `--help` for more details. Once you have the Spec, your source tables and the .sql files to execute, you’re all set! The next step is to tell Napkin to execute the pipeline you’ve created. This is done through the ‘run’ command as shown below. shell . $ napkin run --spec-file specs/spec.yaml . NOTICE: table \"best-revenue\" does not exist, skipping NOTICE: table \"best-seller\" does not exist, skipping [2021-08-20 17:25:14] Table \"best-revenue\" ran in 0.03s: 3 rows affected [2021-08-20 17:25:14] Table \"best-seller\" ran in 0.04s: 3 rows affected [2021-08-20 17:25:14] Run complete. Total cost: 6 rows affected . Once the pipeline has run, you can check the database for the new tables (best-sellerand best-revenue): . psql salesDb . SELECT * FROM \"best-seller\"; . product_id | sum ------------+------ 1 | 1000 2 | 300 3 | 12 . Now you’re all set! You have a working SQL pipeline and as new data comes in to your source tables you can repeatably give napkin the ‘run’ command to execute the pipeline and recreate the target tables. As an example, let’s modify our input data here and rerun the pipeline. psql salesDb . INSERT INTO sale VALUES (5, 999, 3); . shell . napkin run --spec-file specs/spec.yaml . psql salesDb . SELECT * FROM \"best-seller\"; . product_id | sum ------------+------ 3 | 1011 1 | 1000 2 | 300 . As expected, the updated target tables reflect the change in source data and the best-seller table is updated with the new row with the value ‘kale’ in it on top. ",
    "url": "https://soostone.github.io/tutorial/#sales-db-demo",
    "relUrl": "/tutorial/#sales-db-demo"
  },"37": {
    "doc": "Tutorial",
    "title": "Using Templates and Variable Interpolation",
    "content": "The next step in mastering Napkin is to utilize templates to parameterize your queries. This is an extremely important feature because it can be used for everything from handling different table or variable names in development vs production datasets or simply reusing the same query for multiple purposes. Template variables can be set in a Spec file or can be overridden globally with command line options. Template variables hold text to be used in substitution for the variable name in a template query. The final query should always be a valid SQL expression. Let’s work through an example. There are 2 versions of STDDEV functions in Postgres. Let’s compare them by computing the results in dedicated tables, but reusing a single parameterized query. sql/stdev.mtpl . SELECT p.name, {{stddev}}(s.{{column_name}}) FROM {{table_name}} s INNER JOIN product p ON (p.id = s.product_id) GROUP BY p.name; . Append the following config to spec.yaml: . spec.yaml . - post_hooks: [] pre_hooks: [] update_strategy: - type: always create_action: strategy: default hidden_deps: [] deps: [] source: stddev.sql type: sql_file vars: {\"column_name\": \"quantity\", \"table_name\": \"sale\", \"stddev\": \"stddev_pop\"} target_type: table target: sale_stddev_pop_quantity tags: [] - post_hooks: [] pre_hooks: [] update_strategy: - type: always create_action: strategy: default hidden_deps: [] deps: [] source: stddev.sql type: sql_file vars: {\"column_name\": \"quantity\", \"table_name\": \"sale\", \"stddev\": \"stddev_samp\"} target_type: table target: sale_stddev_sam_quantity tags: [] . shell . napkin run --spec-file specs/spec.yaml . Compare the results: . psql salesDb . SELECT s.name, stddev_pop, stddev_samp FROM sale_stddev_pop_quantity p, sale_stddev_sam_quantity s WHERE s.name = p.name; . name | stddev_pop | stddev_samp ---------------+------------------+------------------ coke | 0 | kale | 468.116082469580 | 573.322771220540 chocolate bar | 0 | . Here we can see that Napkin was able to run the same sql file but in taking the configuration from the spec.yaml file, actually executed two different valid queries with different results placed into different target tables. ",
    "url": "https://soostone.github.io/tutorial/#using-templates-and-variable-interpolation",
    "relUrl": "/tutorial/#using-templates-and-variable-interpolation"
  },"38": {
    "doc": "Tutorial",
    "title": "Chain of queries",
    "content": "In the previous example, we ran our join query manually. Let’s update our Spec so that it’s automatically executed. Append the following config to your spec.yaml: . spec.yaml . - post_hooks: [] pre_hooks: [] update_strategy: - type: always create_action: strategy: default hidden_deps: [] deps: [] source: compare-sale-stddev-quantity.sql type: sql_file vars: {} target_type: table target: compare_sale_stddev_quantity tags: [] . Let’s truncate our intermediate tables so we’re 100% confident that the resultant dataset in the target tables was created with the pipelien run we’re about to do. psql salesDb . truncate sale_stddev_pop_quantity; truncate sale_stddev_sam_quantity; . shell . napkin run --spec-file specs/spec.yaml . psql salesDb . SELECT * FROM compare_sale_stddev_quantity; . As we can see, the target table correctly shows the desired result from our parameterized queries: . name | stddev_pop | stddev_samp ---------------+------------------+------------------ coke | 0 | kale | 468.116082469580 | 573.322771220540 chocolate bar | 0 | . ",
    "url": "https://soostone.github.io/tutorial/#chain-of-queries",
    "relUrl": "/tutorial/#chain-of-queries"
  },"39": {
    "doc": "Tutorial",
    "title": "SQL macro functions",
    "content": " ",
    "url": "https://soostone.github.io/tutorial/#sql-macro-functions",
    "relUrl": "/tutorial/#sql-macro-functions"
  },"40": {
    "doc": "Tutorial",
    "title": "Backends",
    "content": "Napkin supports following backends: . | Postgres | BigQuery | Redshift | . These are the databases we currently support. Let us know if your database of choice is not here and maybe we can add it to a future release! . ",
    "url": "https://soostone.github.io/tutorial/#backends",
    "relUrl": "/tutorial/#backends"
  },"41": {
    "doc": "Tutorial",
    "title": "BigQuery",
    "content": "OAuth authentication. ",
    "url": "https://soostone.github.io/tutorial/#bigquery",
    "relUrl": "/tutorial/#bigquery"
  },"42": {
    "doc": "Tutorial",
    "title": "Embedded Haskell",
    "content": "Lot’s of programs have embedded scripting langauge for better flexibility and automating complex business logic, recall - VBA, Emacs Lisp, MaxScript, etc and Napkin is on the same line with them and Napkin Api is available through Haskell. Evaluation Haskell through eval . Napkin eval command can interpret a set of free form Haskell modules - just specify top function you are interested in. Module with a lazy string function: . module Hello where import Prelude (String, (++), Char) pureStr :: String pureStr = \"Hello World!!!\" . $ napkin eval -f pureStr -m Hello -r . -i Pure str Hello World!!! . See help for calling IO action. Access to plain Haskell interpreter is a test environment for code to be used in spec for describing tables. ",
    "url": "https://soostone.github.io/tutorial/#embedded-haskell",
    "relUrl": "/tutorial/#embedded-haskell"
  },"43": {
    "doc": "Tutorial",
    "title": "Misc",
    "content": "Unused column detection . Some table columns are introduced for debugging and development purpose, but as time goes an engineer can stop using them and completely forget, meanwhile such columns contribute into cost of running queries. Let’s check following query for unused columns in an intermediate CTE table: . sql/query.sql . with CTE as (select f, g from DbTable) select f from CTE . shell . napkin optimize . Table: [query] query, CTE \"CTE\" has unused columns [\"g\"] . ",
    "url": "https://soostone.github.io/tutorial/#misc",
    "relUrl": "/tutorial/#misc"
  },"44": {
    "doc": "Tutorial",
    "title": "Tutorial",
    "content": ". | Tutorial . | Interacting with Napkin - The Spec | Sales DB demo . | Starting out - Creating your Napkin Spec file | Schema and Input Data | Setting up The Pipeline | . | Using Templates and Variable Interpolation | Chain of queries | SQL macro functions | Backends | BigQuery | Embedded Haskell . | Evaluation Haskell through eval | . | Misc . | Unused column detection | . | . | . ",
    "url": "https://soostone.github.io/tutorial/",
    "relUrl": "/tutorial/"
  },"45": {
    "doc": "User Manual",
    "title": "User Manual",
    "content": " ",
    "url": "https://soostone.github.io/user-manual/",
    "relUrl": "/user-manual/"
  }
}
